<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Gabe&#39;s Gulch</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Gabe&#39;s Gulch</description>
    <generator>Hugo -- 0.140.0</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[WIP] Database Systems - Query Execution and Processing</title>
      <link>http://localhost:1313/posts/db-systems-query-processing/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/db-systems-query-processing/</guid>
      <description>&lt;h2 id=&#34;operator-execution&#34;&gt;Operator execution&lt;/h2&gt;
&lt;p&gt;In OLAP systems, sequential scans are the primary method for query execution. The goal is two-fold: (1) minimize the amount of data fetched from the disk or a remote object store, and (2) maximize the use of hardware resources for efficient query execution.&lt;/p&gt;
&lt;p&gt;Andy’s (unscientific) top three execution optimization techniques:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data parallelization (vectorization)&lt;/strong&gt;. Breaking down a query into smaller tasks and running them in parallel on different cores, threads, or nodes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Task parallelization (multi-threading)&lt;/strong&gt;. Breaking down a query into smaller independent tasks and executing them concurrently. This allows the DBMS to take full advantage of hardware capabilities and or multiple machines to improve query execution time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Code specialization (pre-compiled / JIT)&lt;/strong&gt;. Code generation for specific queries, e.g. JIT or pre-compiled parameters.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;which fall into three primary ways for speeding up queries:&lt;/p&gt;</description>
    </item>
    <item>
      <title>[WIP] Database Systems - Storage</title>
      <link>http://localhost:1313/posts/db-systems-file-formats/</link>
      <pubDate>Sun, 01 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/db-systems-file-formats/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;As the business landscape embraces data-driven approaches for analysis and decision-making, there is a rapid surge in the volume of data requiring storage and processing. This surge has led to the growing popularity of OLAP database systems.&lt;/p&gt;
&lt;p&gt;An OLAP system workload is characterized by complex queries that require scanning over large portions of the database. In OLAP workloads, the database system is often analyzing and deriving new data from existing data collected on the OLTP side. In contrast, OLTP workloads are characterized by fast, relatively simple and repetitive queries that operate on a single entity at a time (usually involving an update or insert).&lt;/p&gt;</description>
    </item>
    <item>
      <title>Creating C Callbacks with Numba and Calling Them From Rust</title>
      <link>http://localhost:1313/posts/2024-11-30-c-callbacks/</link>
      <pubDate>Sat, 30 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2024-11-30-c-callbacks/</guid>
      <description>&lt;p&gt;When interfacing with libraries written in C/C++ from Rust, it may require writing native callbacks to provide functionality or logic to the library. A C Callback is a function pointer that is passed as an argument to another function, allowing that function to &amp;ldquo;call back&amp;rdquo; and execute the passed function at runtime.&lt;/p&gt;
&lt;p&gt;When interfacing with Python from Rust, there may be scenarios where the Rust code also needs to be able to call a Python function. Rust&amp;rsquo;s foreign function interface (FFI) and &lt;code&gt;pyo3&lt;/code&gt; crate in fact lets you do this. However, calling Python from Rust involves invoking the Python interpreter, which can reduce performance. If one of the goals for using Rust is to improve the performance of your application or library, this overhead might be undesirable. To avoid invoking the Python interpreter, you can use Numba. Numba allows you to create a C callback, pass this function pointer to Rust, and perform the callback without incurring the overhead associated with Python.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Database Systems - Series Overview</title>
      <link>http://localhost:1313/posts/db-systems-blog-series-overview/</link>
      <pubDate>Tue, 22 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/db-systems-blog-series-overview/</guid>
      <description>&lt;p&gt;A blog series consisting of my notes on the Carnegie Mellon University (CMU) Introduction and Advanced Database Systems Lectures by Andy Pavlo and Jignesh Patel. The primary goal of this series is to: (1) consolidate my notes, and (2) act as a reference guide for my future self. Perhaps some readers may extract some value, but I would highly recommend watching the lectures for yourself.&lt;/p&gt;
&lt;p&gt;The series will cover:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Database storage&lt;/li&gt;
&lt;li&gt;Indexes&lt;/li&gt;
&lt;li&gt;Join algorithms&lt;/li&gt;
&lt;li&gt;Query execution and processing&lt;/li&gt;
&lt;li&gt;Query optimization&lt;/li&gt;
&lt;li&gt;Query scheduling and coordination&lt;/li&gt;
&lt;li&gt;Concurrency control&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;olap-database-management-system-components&#34;&gt;OLAP database management system components&lt;/h2&gt;
&lt;p&gt;The series will primarily focus on the components of OLAP database management systems (DBMS). A recent trend of the last decade is the breakout of OLAP DBMS components into standalone services and libraries for:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Hierarchical Regression With Missing Data</title>
      <link>http://localhost:1313/posts/hierarchical-regression-missing-data/</link>
      <pubDate>Tue, 10 Sep 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/hierarchical-regression-missing-data/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Hierarchical regression, also known as multilevel modeling, is a powerful modeling technique that allows one to analyze data with a nested structure. This approach is particularly useful when dealing with data that has natural groupings, such as students within schools, patients within hospitals, or in the example below, product configurations within manufacturing processes. One of the key advantages of hierarchical regression lies in its ability to handle missing data in groups, i.e., when one group may not share the same covariates as another group or some groups may contain missong observations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Stateful Joins in SQL</title>
      <link>http://localhost:1313/posts/sql-stateful-joins/</link>
      <pubDate>Thu, 22 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/sql-stateful-joins/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In some scenarios, one needs to enrich an event stream with data from another source that holds &amp;ldquo;state&amp;rdquo;. This state provides additional context to the event stream.&lt;/p&gt;
&lt;p&gt;For example, in manufacturing, a machine may use a set of machine process parameters (pressure, speed, force, etc.) when producing an item. The process parameters represent the &amp;ldquo;state&amp;rdquo; of the machine at production time $t$. However, the software services that publishes messages on what is being produced and the machine process parameters currently used are separate. Furthermore, to avoid the duplication of data, the service that publishes process parameters only publishes a message when there is a change in state, e.g when an operator changes one of process parameters.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Alternative Samplers to NUTS in Bambi</title>
      <link>http://localhost:1313/posts/bambi-alternative-samplers/</link>
      <pubDate>Fri, 29 Mar 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-alternative-samplers/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;alternative-sampling-backends&#34;&gt;Alternative sampling backends&lt;/h1&gt;
&lt;p&gt;This blog post is a copy of the alternative samplers documentation I wrote for &lt;a href=&#34;https://bambinos.github.io/bambi/&#34;&gt;Bambi&lt;/a&gt;. The original post can be found &lt;a href=&#34;https://bambinos.github.io/bambi/notebooks/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Bambi, the sampler used is automatically selected given the type of variables used in the model. For inference, Bambi supports both MCMC and variational inference. By default, Bambi uses PyMC&amp;rsquo;s implementation of the adaptive Hamiltonian Monte Carlo (HMC) algorithm for sampling. Also known as the No-U-Turn Sampler (NUTS). This sampler is a good choice for many models. However, it is not the only sampling method, nor is PyMC the only library implementing NUTS.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Advanced Interpret Usage in Bambi</title>
      <link>http://localhost:1313/posts/bambi-advanced-marginal-effects/</link>
      <pubDate>Sat, 09 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-advanced-marginal-effects/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;interpret-advanced-usage&#34;&gt;Interpret Advanced Usage&lt;/h1&gt;
&lt;p&gt;The &lt;code&gt;interpret&lt;/code&gt; module is inspired by the R package &lt;a href=&#34;https://marginaleffects.com&#34;&gt;marginaleffects&lt;/a&gt; and ports the core functionality of {marginaleffects} to Bambi. To close the gap of non-supported functionality in Bambi, &lt;code&gt;interpret&lt;/code&gt; now provides a set of helper functions to aid the user in more advanced and complex analysis not covered within the &lt;code&gt;comparisons&lt;/code&gt;, &lt;code&gt;predictions&lt;/code&gt;, and &lt;code&gt;slopes&lt;/code&gt; functions.&lt;/p&gt;
&lt;p&gt;These helper functions are &lt;code&gt;data_grid&lt;/code&gt; and &lt;code&gt;select_draws&lt;/code&gt;. The &lt;code&gt;data_grid&lt;/code&gt; can be used to create a pairwise grid of data points for the user to pass to &lt;code&gt;model.predict&lt;/code&gt;. Subsequently, &lt;code&gt;select_draws&lt;/code&gt; is used to select the draws from the posterior (or posterior predictive) group of the InferenceData object returned by the predict method that correspond to the data points that &amp;ldquo;produced&amp;rdquo; that draw.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Outcome Constraints in Bayesian Optimization</title>
      <link>http://localhost:1313/posts/constrained-bayesian-optimization/</link>
      <pubDate>Tue, 28 Nov 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/constrained-bayesian-optimization/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#| code-fold: true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; torch
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; botorch.acquisition &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; qLogExpectedImprovement
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; botorch.fit &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fit_gpytorch_model
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; botorch.models &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; SingleTaskGP
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; botorch.optim &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; optimize_acqf
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; gpytorch.mlls &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ExactMarginalLogLikelihood
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; torch.distributions &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Normal
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;style&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;use(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://raw.githubusercontent.com/GStechschulte/filterjax/main/docs/styles.mplstyle&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;outcome-constraints&#34;&gt;Outcome constraints&lt;/h1&gt;
&lt;p&gt;In optimization, it is often the goal that we need to optimize an objective function while satisfying some constraints. For example, we may want to minimize the scrap rate by finding the optimal process parameters of an manufacturing machine. However, we know the scrap rate cannot be below 0. In another setting, we may want to maximize the throughput of a machine, but we know that the throughput cannot exceed the maximum belt speed of the machine. Thus, we need to find regions in the search space that both yield high objective values and satisfy these constraints. In this blog, we will focus on inequality &lt;em&gt;outcome constraints&lt;/em&gt;. That is, the domain of the objective function is&lt;/p&gt;</description>
    </item>
    <item>
      <title>Survival Models in Bambi</title>
      <link>http://localhost:1313/posts/bambi-survival-models/</link>
      <pubDate>Wed, 25 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-survival-models/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;survival-models&#34;&gt;Survival Models&lt;/h1&gt;
&lt;p&gt;Survival models, also known as time-to-event models, are specialized statistical methods designed to analyze the time until the occurrence of an event of interest. In this notebook, a review of survival analysis (using non-parametric and parametric methods) and censored data is provided, followed by a survival model implementation in Bambi.&lt;/p&gt;
&lt;p&gt;This blog post is a copy of the survival models documentation I wrote for &lt;a href=&#34;https://bambinos.github.io/bambi/&#34;&gt;Bambi&lt;/a&gt;. The original post can be found &lt;a href=&#34;https://bambinos.github.io/bambi/notebooks/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Predict New Groups with Hierarchical Models in Bambi</title>
      <link>http://localhost:1313/posts/bambi-predict-new-groups/</link>
      <pubDate>Tue, 10 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-predict-new-groups/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h1 id=&#34;predict-new-groups&#34;&gt;Predict New Groups&lt;/h1&gt;
&lt;p&gt;In Bambi, it is possible to perform predictions on new, unseen, groups of data that were not in the observed data used to fit the model with the argument &lt;code&gt;sample_new_groups&lt;/code&gt; in the &lt;code&gt;model.predict()&lt;/code&gt; method. This is useful in the context of hierarchical modeling, where groups are assumed to be a sample from a larger group.&lt;/p&gt;
&lt;p&gt;This blog post is a copy of the zero inflated models documentation I wrote for &lt;a href=&#34;https://bambinos.github.io/bambi/&#34;&gt;Bambi&lt;/a&gt;. The original post can be found &lt;a href=&#34;https://bambinos.github.io/bambi/notebooks/predict_new_groups.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ordinal Models in Bambi</title>
      <link>http://localhost:1313/posts/bambi-ordinal-models/</link>
      <pubDate>Fri, 29 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-ordinal-models/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#| code-fold: true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; arviz &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; az
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; matplotlib.lines &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Line2D
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; warnings
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; bambi &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; bmb
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;warnings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;filterwarnings(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ignore&amp;#34;&lt;/span&gt;, category&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;FutureWarning&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;ordinal-regression&#34;&gt;Ordinal Regression&lt;/h1&gt;
&lt;p&gt;This blog post is a copy of the ordinal models documentation I wrote for &lt;a href=&#34;https://bambinos.github.io/bambi/&#34;&gt;Bambi&lt;/a&gt;. The original post can be found &lt;a href=&#34;https://bambinos.github.io/bambi/notebooks/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In some scenarios, the response variable is discrete, like a count, and ordered. Common examples of such data come from questionnaires where the respondent is asked to rate a product, service, or experience on a scale. This scale is often referred to as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Likert_scale&#34;&gt;Likert scale&lt;/a&gt;. For example, a five-level Likert scale could be:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Zero Inflated Models in Bambi</title>
      <link>http://localhost:1313/posts/bambi-zip-models/</link>
      <pubDate>Fri, 29 Sep 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-zip-models/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#| code-fold: true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; arviz &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; az
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; plt
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; matplotlib.lines &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Line2D
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; numpy &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; np
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; pandas &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; pd
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; scipy.stats &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; stats
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; seaborn &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; sns
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; warnings
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; bambi &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; bmb
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;warnings&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;simplefilter(action&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ignore&amp;#39;&lt;/span&gt;, category&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;FutureWarning&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;zero-inflated-models&#34;&gt;Zero inflated models&lt;/h1&gt;
&lt;p&gt;This blog post is a copy of the zero inflated models documentation I wrote for &lt;a href=&#34;https://bambinos.github.io/bambi/&#34;&gt;Bambi&lt;/a&gt;. The original post can be found &lt;a href=&#34;https://bambinos.github.io/bambi/notebooks/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Summer of Code - Final Report</title>
      <link>http://localhost:1313/posts/gsoc-final-report/</link>
      <pubDate>Thu, 10 Aug 2023 18:43:46 +0200</pubDate>
      <guid>http://localhost:1313/posts/gsoc-final-report/</guid>
      <description>&lt;p&gt;&lt;img loading=&#34;lazy&#34; src=&#34;bambi-logo.png&#34; alt=&#34;alt&#34;  /&gt;
&lt;/p&gt;
&lt;p&gt;My project &amp;ldquo;Better tools to interpret complex Bambi regression models&amp;rdquo; was completed under the organization of NumFOCUS, and mentors Tomás Capretto and Osvaldo Martin. Before I describe the project, objectives, and work completed, I would like to thank my mentors Tomás and Osvaldo for their precious time and support throughout the summer. They were always available and timely in communicating over Slack and GitHub, and provided valuable feedback during code reviews. Additionally, I would like to thank NumFOCUS and the Google Summer of Code (GSoC) program for providing the opportunity to work on such an open source project over the summer. It has been an invaluable experience, and I look forward to contributing to open source projects in the future.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Summer of Code - Average Predictive Slopes</title>
      <link>http://localhost:1313/posts/bambi-slopes/</link>
      <pubDate>Tue, 01 Aug 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-slopes/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;It is currently the beginning of week ten of Google Summer of Code 2023. According to the original deliverables table outlined in my proposal, the goal was to have opened a draft PR for the basic functionality of the &lt;code&gt;plot_slopes&lt;/code&gt;. Subsequently, week 11 was reserved to further develop the &lt;code&gt;plot_slopes&lt;/code&gt; function, and to write tests and a notebook for the documentation, respectively.&lt;/p&gt;
&lt;p&gt;However, at the beginning of week ten, I have a &lt;a href=&#34;https://github.com/bambinos/bambi/pull/699&#34;&gt;PR&lt;/a&gt; open with the majority of the functionality that &lt;a href=&#34;https://vincentarelbundock.github.io/marginaleffects/&#34;&gt;marginaleffects&lt;/a&gt; has for &lt;code&gt;slopes&lt;/code&gt;. In addition, I also exposed the &lt;code&gt;slopes&lt;/code&gt; function, added tests, and have a &lt;a href=&#34;https://github.com/bambinos/bambi/pull/701&#34;&gt;PR&lt;/a&gt; open for the documentation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Google Summer of Code - Average Predictive Comparisons</title>
      <link>http://localhost:1313/posts/bambi-comparisons/</link>
      <pubDate>Fri, 30 Jun 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/bambi-comparisons/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;It is currently the end of week five of Google Summer of Code 2023. According to the original deliverables table outlined in my proposal, the goal was to have opened a draft PR for the core functionality of the &lt;code&gt;plot_comparisons&lt;/code&gt;. Subsequently, week six and seven were to be spent further developing the &lt;code&gt;plot_comparisons&lt;/code&gt; function, and writing tests and a demo notebook for the documentation, respectively. However, at the end of week five, I have a PR open with the majority of the functionality that &lt;a href=&#34;https://vincentarelbundock.github.io/marginaleffects/&#34;&gt;marginaleffects&lt;/a&gt; has. In addition, I also exposed the &lt;code&gt;comparisons&lt;/code&gt; function, added tests (which can and will be improved), and have started on documentation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Gibbs Sampler From Scratch</title>
      <link>http://localhost:1313/posts/gibbs-sampler/</link>
      <pubDate>Wed, 12 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gibbs-sampler/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;A variant of the Metropolis-Hastings (MH) algorithm that uses clever proposals and is therefore more efficient (you can get a good approximate of the posterior with far fewer samples) is Gibbs sampling. A problem with MH is the need to choose the proposal distribution, and the fact that the acceptance rate may be low.&lt;/p&gt;
&lt;p&gt;The improvement arises from adaptive proposals in which the distribution of proposed parameter values adjusts itself intelligently, &lt;strong&gt;depending upon the parameter values at the moment&lt;/strong&gt;. This dependence upon the parameters at that moment is an exploitation of conditional independence properties of a graphical model to automatically create a good proposal, with acceptance probability equal to one.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Metropolis Hastings Sampler From Scratch</title>
      <link>http://localhost:1313/posts/metropolis-hastings-sampler/</link>
      <pubDate>Sat, 08 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/metropolis-hastings-sampler/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h3 id=&#34;main-idea&#34;&gt;Main Idea&lt;/h3&gt;
&lt;p&gt;Metropolis-Hastings (MH) is one of the simplest kinds of MCMC algorithms. The idea with MH is that at each step, we propose to move from the current state $x$ to a new state $x&amp;rsquo;$ with probability $q(x&amp;rsquo;|x)$, where $q$ is the &lt;strong&gt;proposal distribution&lt;/strong&gt;. The user is free to choose the proposal distribution and the choice of the proposal is dependent on the form of the target distribution. Once a proposal has been made to move to $x&amp;rsquo;$, we then decide whether to &lt;strong&gt;accept&lt;/strong&gt; or &lt;strong&gt;reject&lt;/strong&gt; the proposal according to some rule. If the proposal is accepted, the new state is $x&amp;rsquo;$, else the new state is the same as the current state $x$.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Monte Carlo Approximation</title>
      <link>http://localhost:1313/posts/monte-carlo-approximation/</link>
      <pubDate>Fri, 07 Oct 2022 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/monte-carlo-approximation/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;
&lt;h2 id=&#34;inference&#34;&gt;Inference&lt;/h2&gt;
&lt;p&gt;In the probabilistic approach to machine learning, all unknown quantities—predictions about the future, hidden states of a system, or parameters of a model—are treated as random variables, and endowed with probability distributions. The process of inference corresponds to computing the posterior distribution over these quantities, conditioning on whatever data is available. Given that the posterior is a probability distribution, we can draw samples from it. The samples in this case are parameter values. The Bayesian formalism treats parameter distributions as the degrees of relative plausibility, i.e., if this parameter is chosen, how likely is the data to have arisen? We use Bayes&amp;rsquo; rule for this process of inference. Let $h$ represent the uknown variables and $D$ the known variables, i.e., the data. Given a likelihood $p(D|h)$ and a prior $p(h)$, we can compute the posterior $p(h|D)$ using Bayes&amp;rsquo; rule:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Variational Inference - Evidence Lower Bound</title>
      <link>http://localhost:1313/posts/variational-inference/</link>
      <pubDate>Fri, 03 Jun 2022 18:43:46 +0200</pubDate>
      <guid>http://localhost:1313/posts/variational-inference/</guid>
      <description>&lt;p&gt;We don&amp;rsquo;t know the real posterior so we are going to choose a distribution $Q(\theta)$ from a family of distributions $Q^*$ that are &lt;strong&gt;easy to work with&lt;/strong&gt; and parameterized by $\theta$. The approximate distribution should be &lt;em&gt;as close as possible&lt;/em&gt; to the true posterior. This closeness is measured using KL-Divergence. If we have the joint $p(x, z)$ where $x$ is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior.&lt;/p&gt;</description>
    </item>
    <item>
      <title>No Code, Dependency, and Building Technology</title>
      <link>http://localhost:1313/posts/no-code-building-technology/2021-08-10-no-code-dependency-and-building-technology/</link>
      <pubDate>Tue, 10 Aug 2021 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/no-code-building-technology/2021-08-10-no-code-dependency-and-building-technology/</guid>
      <description>&lt;h2 id=&#34;modernity-and-abstraction&#34;&gt;Modernity and Abstraction&lt;/h2&gt;
&lt;p&gt;&amp;lsquo;Programmers&amp;rsquo;, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his &lt;a href=&#34;https://karpathy.github.io/2019/04/25/recipe/&#34;&gt;blog&lt;/a&gt; on the matter, &amp;ldquo;A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect&amp;rdquo;.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
