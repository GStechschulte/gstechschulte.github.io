---
aliases:
- /probability/2022/06/30/Probabilistic-Prediction-Problems-Part-2
categories:
- probability
date: '2022-06-30'
layout: post
title: Probabilistic Prediction Problems - Part 2

---

Part two deals with model evaluation and selection using the metrics and scoring rules defined in part one.

## The Problem with Parameters

Three main monsters when it comes to "modeling":
1. Overfitting
2. Underfitting
3. Con-founders

The goal of the model should be stated before you choose your methods to tame these monsters:
- Is the goal _predictive power_?
- Is the goal to understand _causes_?

Regarding monster (1), adding variables and parameters to a model can help to reveal hidden effects and improve estimates. However, more parameters always results in a better model "fit". While more complex models fit the data better, they often predict new data worse. Models that have many parameters tend to overfit more than simpler models. Generally, fit is measured by how well the model can retrodict the data used to fit the model. A common metric for this is "variance explained", $R^2$. Monster (2) hurts, too as underfitting produces models that are inaccurate both within and out of sample. Underfit models have learned too little, whether that be from uninformative features or too simple a model.

So, how to navigate overfitting and underfitting? First, pick a criterion of model performance as the targetâ€”_what do you want the model to be good at?_ Methods based on information theory can provide a common and useful target. 

## Evaluating Generative Models

Generative models aim to model the _underlying generative process_  of the data, typically using Bayes theorem from which we can also generate new samples:

$$p(z | x) = \frac{p(x|z)p(z)}{p(x)}$$

Generally speaking, when evaluating generative models, we want the metrics to capture:
- sample quality - are the samples generated by the model part of the data distribution?
- sample diversity - are the samples from the model distribution capturing all modes of the data?
- generalization - is the model generalizing beyond the training data?

## Model Selection and Evaluation

To check the results of modeling and inference, we would like to know how well a model fits observed data $x$, which we can quantify with the evidence or marginal likelihood.