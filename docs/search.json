[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A home for sampled thoughts and learnings."
  },
  {
    "objectID": "posts/2022-07-25-mosquito-walk.html",
    "href": "posts/2022-07-25-mosquito-walk.html",
    "title": "A Fragment of the Sphinx",
    "section": "",
    "text": "This code complements the figures and methods described in Chapter 4: A Fragment of the Sphinx of the book Shape: The Hidden Geometry of Everything by Jordan Ellenberg. In 1897, Sir Ronald Ross had discovered that maleria was carried by the bite of the anopheles mosquito. In the late summer of 1904, he gave a lecture in Missouri, United States titled “The Logical Basis of Sanitary Policy of Mosquito Reduction”. This lecture contained the first glimmer into a new geometric theory that would explode into physics, finance, and many other areas of science and engineering: the random walk.\nRoss proposed you eliminate propogation of mosquitos in a circular region by draining the pools where they breed. That doesn’t eliminate all potentially malarial mosquitoes from the region, because others may be born outside the circle and fly in. But, the mosquitoes’ life is brief and lacks ambition; it won’t set a course straight for the center and stick to it. So, some region around the center of the circle would hopefully be malaria free, as long as the circle is large enough. How large is large enough? That depends on how far a mosquito is likely to travel.\n“Suppose that a mosquito is born at a given point, and that during its life it wanders about, to or fro, to left or to right, where it wills…After a time it will die. What are the probabilities that its dead body will be found at a given distance from its birthplace?”"
  },
  {
    "objectID": "posts/2022-07-25-mosquito-walk.html#one-dimensional",
    "href": "posts/2022-07-25-mosquito-walk.html#one-dimensional",
    "title": "A Fragment of the Sphinx",
    "section": "One-Dimensional",
    "text": "One-Dimensional\nRoss was only able to handle the simple case where the mosquito is fixed to a straight line, choosing merely whether to flit northeast or southwest. To deal with the one-dimensional case on pg.66, we need a notion of distance and space. The outcome, defined as a choice to fly either northeast or eouthwest over the course of 10 days, can be described using a Binomial distribution parameterized by \\(n =\\) trials and \\(p =\\) probability of success. Setting \\(n=1\\) and \\(p=0.5\\) indicates an equal probability of observing \\(1\\) or \\(0\\) and is equivalent to a Beroulli distribution. For the one-dimensional case, \\(1\\) is encoded as moving \\(1\\) unit northeast and \\(0\\) as moving \\(-1\\) units southwest.\n\n\nCode\ndef plot_walk_1d(direction):\n\n    direction_df = pd.DataFrame.from_dict(\n    direction, orient='index', columns=['move'])\n    direction_df.index = direction_df.index.set_names(['step'])\n\n    shift = direction_df['move'].shift(periods=1, fill_value=0)\n    direction_df['running_tally'] = direction_df['move'].cumsum()\n    direction_df = direction_df.reset_index()\n\n    x_min, x_max = direction_df['running_tally'].min(), \\\n    direction_df['running_tally'].max()\n\n    fig = px.scatter(\n            direction_df, \n            x='running_tally', y=np.zeros(len(direction_df)), \n            animation_frame='step',\n            range_x=[-10, 10],\n            labels={\n                'y': 'Northeast - Southwest'\n            },\n            title='Mosquito Distance Traveled'\n            )\n\n    fig.update_yaxes(showgrid=False, \n                    zeroline=True, zerolinecolor='grey', zerolinewidth=1,\n                    showticklabels=False)\n    fig.update_layout(\n        xaxis = dict(\n            tickmode='linear'\n        )\n    )\n    \n    fig.show()\n\n\n\noutcome = np.random.binomial(1, 0.5, 20)\n\ndirection = {}\ndirection[0] = 0\ncnt = 0\nfor step in outcome:\n    cnt += 1\n    if step == 0:\n        direction[cnt] = 1\n    else:\n        direction[cnt] = -1\n\nplot_walk_1d(direction)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/2022-07-25-mosquito-walk.html#two-dimensional",
    "href": "posts/2022-07-25-mosquito-walk.html#two-dimensional",
    "title": "A Fragment of the Sphinx",
    "section": "Two-Dimensional",
    "text": "Two-Dimensional\nOn pg.66, Ellenberg shows a two-dimensional representation of a hypothetical mosquito flitting about, to or fro, to left or to right, where it wills. The code below achieves a similar simulation. However, instead of explicitly using a Binomial distribution, the np.random.choice() function is used and is parameterized by ([-1, 1], n_steps). This is equivalent to using a Bernoulli distribution using the same parameters as in the 1d simulation, with the exception of either \\(-1\\) or \\(1\\) being returned instead of \\(1\\) or \\(0\\).\nFor the notion of space, the x-y plane is used. North and south are encoded as the y-axis whereas west and east are encoded as the x-axis. By calling the np.random.choice() function twice, one for each unit step on the 2d coordinate plane, it can take the following directions: - north and east: \\((+, +)\\) - north and west: \\((+, -)\\) - south and west: \\((-, -)\\) - south and east: \\((-, +)\\)\n\n\nCode\ndef plot_2d_walk(df):\n\n    fig = go.Figure(\n    layout=go.Layout(\n        xaxis=dict(range=[-80, 80], autorange=False),\n        yaxis=dict(range=[-80, 80], autorange=False),\n        width=1000, height=650,\n        xaxis_title='Units (East or West)',\n        yaxis_title='Units (North or South)',\n        title=\"Mosquito Random Walk (2d)\",\n        updatemenus=[dict(\n            type=\"buttons\",\n            buttons=[dict(label=\"Play\",\n                            method=\"animate\",\n                            args=[None, {\n                                'frame': {'duration': 0.5}, \n                                'transition': {'duration': 0}\n                                }])])]\n        )\n    )\n\n    fig.add_trace(\n        go.Scatter(\n            x=df.x[:1],\n            y=df.y[:1]\n        )\n    )\n\n    fig.update(frames=[\n        go.Frame(data=[go.Scatter(x=df.x[:k], y=df.y[:k])])\n            for k in range(1, len(df) + 1)\n        ]\n    )\n\n    fig.show()\n\n\n\ndef mosquito_random_walk_2d(n_steps):\n\n    x_steps = np.random.choice([-1, 1], n_steps)\n    y_steps = np.random.choice([-1, 1], n_steps)\n    x_pos, y_pos = np.cumsum(x_steps), np.cumsum(y_steps)\n\n\n    df = pd.DataFrame({\n        'step': np.arange(0, n_steps),\n        'x': x_pos,\n        'y': y_pos\n    })\n\n    plot_2d_walk(df)\n\n\nmosquito_random_walk_2d(n_steps=1500)\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html",
    "href": "posts/2022-07-22-bmcp-ch-4.html",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nimport arviz as az\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom torch.distributions import constraints, transforms\nfrom pyro.distributions import constraints\nfrom pyro.infer import Predictive, TracePredictive, NUTS, MCMC\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom pyro.infer.mcmc.util import summary\nimport os\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (7, 4)"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#transforming-covariates",
    "href": "posts/2022-07-22-bmcp-ch-4.html#transforming-covariates",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.1 Transforming Covariates",
    "text": "4.1 Transforming Covariates\n\nFigure 4.2\n\nX_ = torch.from_numpy(babies['Month'].values.reshape(-1, 1)).to(torch.float)\n#X_ = torch.tensor(X_).float()\n\ny = torch.from_numpy(babies['Length'].values).to(torch.float)\n#y = torch.tensor(y).float()\n\n\ndef linear_babies(month, length=None):\n\n    N, P = month.shape\n    \n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0, 10.).expand([P]))\n    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n    mu = beta_0 + torch.matmul(beta_1, month.T)\n    \n    with pyro.plate('plate', size=N):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=length)\n\n\npyro.render_model(\n    linear_babies, \n    model_args=(X_, y),\n    render_distributions=True\n    )\n\n\n\n\n\n\n\n\n\nkernel = NUTS(linear_babies, adapt_step_size=True)\nmcmc_linear_babies = MCMC(kernel, 500, 300)\nmcmc_linear_babies.run(X_, y)\n\nSample: 100%|██████████| 800/800 [00:05, 144.48it/s, step size=3.89e-01, acc. prob=0.917]\n\n\n\nmcmc_babie_samples = mcmc_linear_babies.get_samples(1000)\npredictive = Predictive(linear_babies, mcmc_babie_samples)(X_, None)\n\naz_linear_babies = az.from_pyro(\n    posterior=mcmc_linear_babies, posterior_predictive=predictive)\n\nposterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented.\n\n\n\ny_mu = predictive['y'].mean(axis=0)\n\n\nsns.scatterplot(x=babies['Month'], y=babies['Length'], color='grey', alpha=0.75)\nsns.lineplot(x=babies['Month'], y=y_mu, color='blue')\naz.plot_hdi(x=babies['Month'], y=predictive['y'].numpy());\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.3\n\ndef sqrt_babies(month, length=None):\n\n    N, P = month.shape\n    \n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0, 10.).expand([P]))\n    sigma = pyro.sample('sigma', dist.HalfNormal(10.))\n    mu = beta_0 + torch.matmul(beta_1, torch.sqrt(month.T))\n    \n    with pyro.plate('plate', size=N):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=length)\n\n\nkernel = NUTS(sqrt_babies, adapt_step_size=True)\nmcmc_sqrt = MCMC(kernel, 500, 300)\nmcmc_sqrt.run(X_, y)\n\nSample: 100%|██████████| 800/800 [00:06, 119.31it/s, step size=2.91e-01, acc. prob=0.943]\n\n\n\nmcmc_sqrt_babie_samples = mcmc_sqrt.get_samples(1000)\npredictive_sqrt = Predictive(sqrt_babies, mcmc_sqrt_babie_samples)(X_, None)\n\naz_sqrt_babies = az.from_pyro(\n    posterior=mcmc_sqrt, posterior_predictive=predictive_sqrt)\n\nposterior predictive shape not compatible with number of chains and draws.This can mean that some draws or even whole chains are not represented.\n\n\n\ny_mu = predictive_sqrt['y'].mean(axis=0)\n\n\nplt.scatter(babies['Month'], babies['Length'], color='grey', alpha=0.75)\nplt.plot(babies['Month'], y_mu, color='blue')\naz.plot_hdi(x=babies['Month'], y=az_sqrt_babies['posterior_predictive']['y'], hdi_prob=.50, color='grey')\naz.plot_hdi(x=babies['Month'], y=az_sqrt_babies['posterior_predictive']['y'], hdi_prob=.94, color='darkgrey')\nplt.title('Linear model with square root transformation on months');"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#varying-uncertainty",
    "href": "posts/2022-07-22-bmcp-ch-4.html#varying-uncertainty",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.2 - Varying Uncertainty",
    "text": "4.2 - Varying Uncertainty\n\ndef varying_uncertainty(month, length=None):\n\n    N, P = month.shape\n    \n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0, 10.).expand([P]))\n    delta = pyro.sample('delta', dist.HalfNormal(10.).expand([2]))\n    sigma = pyro.deterministic(\n        'sigma', \n        delta[0].unsqueeze(-1) + torch.matmul(delta[1].unsqueeze(-1), month.T)\n        )\n    mu = beta_0 + torch.matmul(beta_1, torch.sqrt(month.T))\n    \n    with pyro.plate('plate', size=N):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=length)\n\n\npyro.render_model(\n    varying_uncertainty,\n    (X_, y),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nvarying_sigma_mcmc = MCMC(NUTS(varying_uncertainty), num_samples=500, warmup_steps=300)\nvarying_sigma_mcmc.run(X_, y)\n\nSample: 100%|██████████| 800/800 [00:08, 95.74it/s, step size=3.06e-01, acc. prob=0.912] \n\n\n\nvarying_sigma_samples = varying_sigma_mcmc.get_samples(1000)\nvarying_sigma_post_pred = Predictive(\n    varying_uncertainty, varying_sigma_samples)(X_, None)\n\n\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(7, 5))\n\nax[0].scatter(babies['Month'], babies['Length'], color='grey', alpha=0.75)\nax[0].plot(babies['Month'], varying_sigma_post_pred['y'].mean(axis=0), color='blue')\naz.plot_hdi(x=babies['Month'], y=varying_sigma_post_pred['y'], hdi_prob=.50, color='grey', ax=ax[0])\naz.plot_hdi(x=babies['Month'], y=varying_sigma_post_pred['y'], hdi_prob=.94, color='darkgrey', ax=ax[0])\nax[0].set_ylabel('length')\nax[0].set_title('Linear model with square root transformation on months')\n\nax[1].plot(babies['Month'], varying_sigma_post_pred['sigma'].mean(axis=0)[0], color='blue', alpha=0.75)\nax[1].set_ylabel('$\\sigma$')\nax[1].set_title('Varying $\\sigma$');"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#interaction-effects",
    "href": "posts/2022-07-22-bmcp-ch-4.html#interaction-effects",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.3 - Interaction Effects",
    "text": "4.3 - Interaction Effects\n\ntips_df = pd.read_csv(os.path.abspath('.') + '/data/tips.csv')\ntips_df\n\n\n\n\n\n\n\n\ntotal_bill\ntip\nsex\nsmoker\nday\ntime\nsize\n\n\n\n\n0\n16.99\n1.01\nFemale\nNo\nSun\nDinner\n2\n\n\n1\n10.34\n1.66\nMale\nNo\nSun\nDinner\n3\n\n\n2\n21.01\n3.50\nMale\nNo\nSun\nDinner\n3\n\n\n3\n23.68\n3.31\nMale\nNo\nSun\nDinner\n2\n\n\n4\n24.59\n3.61\nFemale\nNo\nSun\nDinner\n4\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n239\n29.03\n5.92\nMale\nNo\nSat\nDinner\n3\n\n\n240\n27.18\n2.00\nFemale\nYes\nSat\nDinner\n2\n\n\n241\n22.67\n2.00\nMale\nYes\nSat\nDinner\n2\n\n\n242\n17.82\n1.75\nMale\nNo\nSat\nDinner\n2\n\n\n243\n18.78\n3.00\nFemale\nNo\nThur\nDinner\n2\n\n\n\n\n244 rows × 7 columns\n\n\n\n\ntotal_bill_centered = torch.tensor((tips_df[\"total_bill\"] - tips_df[\"total_bill\"].mean()).values, dtype=torch.float32)\ntips = torch.tensor(tips_df[\"tip\"].values, dtype=torch.float)\nsmoker = torch.tensor(pd.Categorical(tips_df[\"smoker\"]).codes, dtype=torch.float)\n\n\ndef interaction_model(bill, smoker, tips=None):\n\n    beta = pyro.sample('beta', dist.Normal(0., 1.).expand([4]))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n    mu = beta[0] + beta[1] * bill + beta[2] * smoker + beta[3] * smoker * bill\n\n    with pyro.plate('plate', len(bill)):\n        y = pyro.sample('y', dist.Normal(mu, sigma), obs=tips)\n\n\npyro.render_model(\n    interaction_model,\n    (total_bill_centered, smoker, tips), render_distributions=True)\n\n\n\n\n\n\n\n\n\ninteraction_mcmc = MCMC(NUTS(interaction_model), num_samples=500, warmup_steps=300)\ninteraction_mcmc.run(total_bill_centered, smoker, tips)\n\nSample: 100%|██████████| 800/800 [00:05, 144.46it/s, step size=4.95e-01, acc. prob=0.895]\n\n\n\nmcmc_interaction_samples = interaction_mcmc.get_samples(1000)\ninteraction_predictive = Predictive(interaction_model, mcmc_interaction_samples)\nposterior_predictive = interaction_predictive(total_bill_centered, smoker, None)\n\naz_inference_interaction = az.from_pyro(\n    posterior=interaction_mcmc, posterior_predictive=posterior_predictive)\n\n\ntip_mu = posterior_predictive['y'].mean(axis=0)\ntip_std = posterior_predictive['y'].std(axis=0)\n\npredictions = pd.DataFrame({\n    'bill': total_bill_centered,\n    'smoker': smoker,\n    'tip': tips, \n    'tip_mu': tip_mu,\n    'tip_std': tip_std,\n    'tip_high': tip_mu + tip_std,\n    'tip_low': tip_mu - tip_std\n})\n\npredictions = predictions.sort_values(by=['bill'])\n\n\nsmoker_df = predictions[predictions['smoker'] == 1]\nnonsmoker_df = predictions[predictions['smoker'] == 0]\n\n\n# colors are terrible - TO DO\nsns.lineplot(\n    x=smoker_df['bill'], y=smoker_df['tip_mu'], color='blue', label='smoker'\n    )\nplt.fill_between(\n    smoker_df['bill'], smoker_df['tip_low'], smoker_df['tip_high'], \n    color='lightblue', alpha=0.5\n    )\nsns.lineplot(\n    x=nonsmoker_df['bill'], y=nonsmoker_df['tip_mu'], color='red', label='non-smoker'\n    )\nplt.fill_between(\n    nonsmoker_df['bill'], nonsmoker_df['tip_low'], nonsmoker_df['tip_high'],\n    color='lightgrey', alpha=0.5\n    )\nsns.scatterplot(\n    x=predictions['bill'], y=predictions['tip'], hue=predictions['smoker']\n    )\nplt.legend()\nplt.title('Interaction Effect on Tip')\nplt.show()"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#robust-regression",
    "href": "posts/2022-07-22-bmcp-ch-4.html#robust-regression",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.4 - Robust Regression",
    "text": "4.4 - Robust Regression\n\ndef generate_sales(*, days, mean, std, label):\n    \"\"\"code taken from the authors / book\"\"\"\n    np.random.seed(0)\n    df = pd.DataFrame(index=range(1, days+1), columns=[\"customers\", \"sales\"])\n    for day in range(1, days+1):\n        num_customers = stats.randint(30, 100).rvs()+1\n        \n        # This is correct as there is an independent draw for each customers orders\n        dollar_sales = stats.norm(mean, std).rvs(num_customers).sum()\n        \n        df.loc[day, \"customers\"] = num_customers\n        df.loc[day, \"sales\"] = dollar_sales\n        \n    # Fix the types as not to cause Theano errors\n    df = df.astype({'customers': 'int32', 'sales': 'float32'})\n    \n    # Sorting will make plotting the posterior predictive easier later\n    df[\"Food_Category\"] = label\n    df = df.sort_values(\"customers\")\n    \n    return df\n\n\nempanadas = generate_sales(days=200, mean=180, std=30, label=\"Empanada\")\n\nempanadas.iloc[0] = [50, 92000, \"Empanada\"]\nempanadas.iloc[1] = [60, 90000, \"Empanada\"]\nempanadas.iloc[2] = [70, 96000, \"Empanada\"]\nempanadas.iloc[3] = [80, 91000, \"Empanada\"]\nempanadas.iloc[4] = [90, 99000, \"Empanada\"]\n\nempanadas = empanadas.sort_values(\"customers\")\n\nfig, ax = plt.subplots()\nempanadas.sort_values(\"sales\")[:-5].plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax);\nempanadas.sort_values(\"sales\")[-5:].plot(x=\"customers\", y=\"sales\", kind=\"scatter\", c=\"black\", ax=ax);\n\nax.set_ylabel(\"Argentine Peso\")\nax.set_xlabel(\"Customer Count\")\nax.set_title(\"Empanada Sales\");\n\n\n\n\n\n\n\n\n\ncustomer_count = torch.tensor(empanadas['customers'].values, dtype=torch.float)\nsales = torch.tensor(empanadas['sales'].values, dtype=torch.float)\n\n\ndef robust_regression(customers, peso=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(50.))\n    beta = pyro.sample('beta', dist.Normal(150., 20.))\n    v = pyro.sample('dof', dist.HalfNormal(20.))\n\n    mu = pyro.deterministic('mu', beta * customers)\n\n    with pyro.plate('plate', len(customers)):\n        sales = pyro.sample('sales', dist.StudentT(loc=mu, scale=sigma, df=v), obs=peso)\n\n\npyro.render_model(\n    robust_regression, (customer_count, sales), render_distributions=True)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(robust_regression)\nmcmc_robust = MCMC(kernel, 500, 300)\nmcmc_robust.run(customer_count, sales)\n\nSample: 100%|██████████| 800/800 [00:09, 88.07it/s, step size=6.43e-01, acc. prob=0.925] \n\n\n\nmcmc_robust.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      beta    179.60      0.25    179.61    179.18    179.99    333.32      1.00\n       dof      1.27      0.15      1.27      1.00      1.50    509.08      1.00\n     sigma    150.67     12.30    150.52    128.41    168.89    479.97      1.00\n\nNumber of divergences: 0\n\n\n\nmcmc_robust_samples = mcmc_robust.get_samples(1000)\nrobust_predictive = Predictive(robust_regression, mcmc_robust_samples)(customer_count, None)\naz_robust_inf = az.from_pyro(posterior=mcmc_robust, posterior_predictive=robust_predictive)\n\n\nmu = az_robust_inf['posterior_predictive']['mu'].values.reshape(-1, len(customer_count)).mean(axis=0)\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(7, 5), sharex=True)\n\nax[0].scatter(customer_count, sales)\nax[0].plot(customer_count, mu, c='blue')\naz.plot_hdi(\n    customer_count, az_robust_inf['posterior_predictive']['sales'], \n    color='grey', ax=ax[0])\nax[0].set_ylabel('sales')\n\n\nax[1].scatter(customer_count, sales)\nax[1].plot(customer_count, mu, c='blue')\naz.plot_hdi(\n    customer_count, az_robust_inf['posterior_predictive']['sales'], \n    color='grey', ax=ax[1]\n    )\nax[1].set_ylim(bottom=2500, top=21000)\nax[1].set_ylabel('sales')\n\nplt.suptitle('Robust Regression using Student-t likelihood');"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#pooling-multilevel-models-mixed-effects",
    "href": "posts/2022-07-22-bmcp-ch-4.html#pooling-multilevel-models-mixed-effects",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "4.5 - Pooling, Multilevel Models, Mixed Effects",
    "text": "4.5 - Pooling, Multilevel Models, Mixed Effects\n\ndef generate_sales(*, days, mean, std, label):\n    \"\"\"code taken from authors / book\"\"\"\n    np.random.seed(0)\n    df = pd.DataFrame(index=range(1, days+1), columns=[\"customers\", \"sales\"])\n    for day in range(1, days+1):\n        num_customers = stats.randint(30, 100).rvs()+1\n        \n        # This is correct as there is an independent draw for each customers orders\n        dollar_sales = stats.norm(mean, std).rvs(num_customers).sum()\n        \n        df.loc[day, \"customers\"] = num_customers\n        df.loc[day, \"sales\"] = dollar_sales\n        \n    # Fix the types as not to cause Theano errors\n    df = df.astype({'customers': 'int32', 'sales': 'float32'})\n    \n    # Sorting will make plotting the posterior predictive easier later\n    df[\"Food_Category\"] = label\n    df = df.sort_values(\"customers\")\n    \n    return df\n\n\npizza_df = generate_sales(days=365, mean=13, std=5, label=\"Pizza\")\nsandwich_df = generate_sales(days=100, mean=6, std=5, label=\"Sandwich\")\nsalad_days = 3\nsalad_df = generate_sales(days=salad_days, mean=8 ,std=3, label=\"Salad\")\n\n\nfig, ax = plt.subplots()\npizza_df.plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax, c=\"grey\", label=\"Pizza\", marker=\"^\", s=60);\nsandwich_df.plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax, c='black', label=\"Sandwich\", marker=\"s\");\nsalad_df.plot(x=\"customers\", y=\"sales\", kind=\"scatter\", ax=ax, label=\"Salad\", c=\"blue\");\n\nax.set_xlabel(\"Number of Customers\")\nax.set_ylabel(\"Daily Sales Dollars\")\nax.set_title(\"Aggregated Sales Dollars\")\nax.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nsales_df = pd.concat([pizza_df, sandwich_df, salad_df]).reset_index(drop=True)\nsales_df[\"Food_Category\"] = pd.Categorical(sales_df[\"Food_Category\"])\n\n\ncustomers = torch.tensor(sales_df['customers'].values, dtype=torch.float)\nsales = torch.tensor(sales_df['sales'].values, dtype=torch.float)\nfood_category = torch.tensor(sales_df['Food_Category'].cat.codes.values, dtype=torch.long)\n\nNotes:\n\nextend shape to 3 because of the 3 food categories\nuse dtype = torch.long when using a tensor as indices\nif you use the pyro.plate() primitive, it seems you do not need to specify the .expand() method on distributions, i.e., to make the batch size &gt; 1 in the case of a multidimensional design matrix\n\n\nUnpooled - MCMC\n\ndef unpooled_model(food_cat, customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n        beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\n# should be: beta -&gt; mu -&gt; y\npyro.render_model(\n    unpooled_model, (food_category, customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(unpooled_model)\nmcmc_unpooled = MCMC(kernel, 500, 300)\nmcmc_unpooled.run(food_category, customers, sales)\n\nSample: 100%|██████████| 800/800 [00:04, 171.15it/s, step size=5.88e-01, acc. prob=0.889]\n\n\n\nmcmc_unpooled.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   beta[0]     13.02      0.03     13.02     12.97     13.07    678.99      1.00\n   beta[1]      8.13      0.20      8.13      7.82      8.45    395.54      1.00\n   beta[2]      6.11      0.05      6.11      6.03      6.19    574.60      1.00\n  sigma[0]     40.09      1.39     39.99     37.95     42.38    778.36      1.00\n  sigma[1]     21.57      8.38     19.84     11.07     35.72    507.41      1.01\n  sigma[2]     36.13      2.58     36.10     32.26     40.33    560.15      1.00\n\nNumber of divergences: 0\n\n\n\nunpooled_posterior_samples = mcmc_unpooled.get_samples(1000)\nunpooled_predictive = \\\n    Predictive(unpooled_model, unpooled_posterior_samples)(food_category, customers, None)\n\naz_unpooled_inf = az.from_pyro(\n    posterior=mcmc_unpooled, posterior_predictive=unpooled_predictive)\n\n\naz.plot_trace(az_unpooled_inf, var_names=[\"beta\", \"sigma\"], compact=False)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\naz.plot_forest(az_unpooled_inf, var_names=['beta'])\nplt.show()\n\n\n\n\n\n\n\n\n\naz.plot_forest(az_unpooled_inf, var_names=['sigma'])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nUnpooled - SVI\nNOT FINISHED\n\ndef unpooled_model(food_cat, customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n        beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\ndef unpooled_guide(food_cat, customers, sales=None): \n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma_scale = pyro.param(\n            'sigma_scale', torch.tensor(1.), constraint=constraints.positive\n            )\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_scale))\n\n        beta_loc = pyro.param('beta_loc', torch.tensor(10.))\n        beta_scale = pyro.param(\n            'beta_scale', torch.tensor(1.), constraint=constraints.positive)\n        beta = pyro.sample('beta', dist.Normal(beta_loc, beta_scale))\n\n\npyro.render_model(\n    unpooled_guide, (food_category, customers, sales), render_params=True)\n\n\n\n\n\n\n\n\n\nauto_guide = AutoLaplaceApproximation(unpooled_model)\n\n\npyro.clear_param_store()\n\nadam_params = {'lr': 0.002}\noptim = Adam(adam_params)\nsvi = SVI(unpooled_model, auto_guide, optim, Trace_ELBO())\n\niter = 1000\nelbo_loss = []\nfor i in range(iter):\n    loss = svi.step(food_category, customers, sales)\n    elbo_loss.append(loss)\n\nplt.figure(figsize=(10, 3))\nplt.plot(np.arange(1, iter+1), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}')\nplt.show()\n\n\n\n\n\n\n\n\n\nfor name, value in pyro.get_param_store().items():\n    print(name, pyro.param(name).data.cpu().numpy())\n\nAutoLaplaceApproximation.loc [ 4.113231   3.4546025  3.8238153 12.153096   8.615855   6.703431 ]\n\n\n\npredictive = Predictive(unpooled_model, guide=auto_guide, num_samples=1000)\nposterior_svi_samples = predictive(food_category, customers, None)\n\n\n\nPooled - MCMC\n\ndef pooled_model(customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n    beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\npyro.render_model(\n    pooled_model, (customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(pooled_model)\nmcmc_pooled = MCMC(kernel, 500, 300)\nmcmc_pooled.run(customers, sales)\n\nSample: 100%|██████████| 800/800 [00:02, 289.74it/s, step size=7.09e-01, acc. prob=0.931]\n\n\n\nmcmc_pooled.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n      beta     11.50      0.12     11.50     11.32     11.70    530.37      1.00\n     sigma    186.28      4.92    186.04    178.49    194.12    262.65      1.00\n\nNumber of divergences: 0\n\n\n\npooled_samples = mcmc_pooled.get_samples(1000)\npooled_predictive = Predictive(pooled_model, pooled_samples)(customers, None)\naz_pooled_inf = az.from_pyro(\n    posterior=mcmc_pooled, posterior_predictive=pooled_predictive)\n\n\nsales_mu = pooled_predictive['y'].mean(axis=0)\nsales_std = pooled_predictive['y'].std(axis=0)\n\npredictions = pd.DataFrame({\n    'customers': customers,\n    'category': food_category,\n    'sales': sales, \n    'sales_mu': sales_mu,\n    'sales_std': sales_std,\n    'sales_high': sales_mu + sales_std,\n    'sales_low': sales_mu - sales_std\n})\n\npredictions = predictions.sort_values(by=['customers'])\n\n\nsns.scatterplot(\n    x=predictions['customers'], y=predictions['sales'], \n    hue=predictions['category'], palette='tab10')\nsns.lineplot(\n    x=predictions['customers'], y=predictions['sales_mu'],\n    color='black')\nplt.fill_between(\n    x=predictions['customers'], \n    y1=predictions['sales_low'], \n    y2=predictions['sales_high'],\n    color='grey',\n    alpha=0.25)\nplt.title('Pooled Parameters - MCMC')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPooled - SVI\nUsing LaPlace Approximation doesn’t make the most sense since. . .\npooled_guide learns beta, but does not learn sigma. This could be related to parameter initialization\n\nsigma = dist.TransformedDistribution(\n        dist.Normal(torch.tensor(0.), 0.1 * torch.rand(1)), transforms=transforms.AbsTransform()\n    )\n\nsns.kdeplot(x=sigma.sample((1000,)).reshape(1, -1)[0]);\n\n\n\n\n\n\n\n\n\ndef pooled_model(customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n    beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\ndef pooled_guide(customers, sales=None): \n\n    sigma_scale = pyro.param(\n        'sigma_scale', 0.1 * torch.rand(1), constraint=constraints.positive\n        )\n\n    sigma_loc = pyro.param(\n        'sigma_loc', torch.tensor(0.))\n    \n    sigma = pyro.sample('sigma', dist.TransformedDistribution(\n        dist.Normal(sigma_loc, sigma_scale), transforms=transforms.AbsTransform()\n    ))\n\n    beta_loc = pyro.param('beta_loc', torch.tensor(1.))\n    beta_scale = pyro.param(\n        'beta_scale', 0.1 * torch.rand(1), constraint=constraints.positive)\n    beta = pyro.sample('beta', dist.Normal(beta_loc, beta_scale))\n\n\npyro.render_model(\n    pooled_guide (customers, sales), render_params=True)\n\n\nauto_guide = pyro.infer.autoguide.AutoDiagonalNormal(pooled_model)\npyro.render_model(auto_guide, (customers, sales, ), render_params=True)\n\n\n\n\n\n\n\n\n\npyro.clear_param_store()\n\n#adam_params = {'lr': 0.005, 'betas': (0.95, 0.99)}\nadam_params = {'lr': 0.01}\noptim = Adam(adam_params)\nsvi = SVI(pooled_model, auto_guide, optim, Trace_ELBO())\n\niter = 2000\nelbo_loss = []\nfor i in range(iter):\n    loss = svi.step(customers, sales)\n    #loss = svi.step(food_category, customers, sales)\n    elbo_loss.append(loss)\n\nplt.plot(np.arange(1, iter+1), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}')\nplt.show()\n\n\n\n\n\n\n\n\n\npredictive = Predictive(pooled_model, guide=auto_guide, num_samples=1000)\nposterior_svi_samples = predictive(customers, None)\n\n\nsales_mu = posterior_svi_samples['y'].mean(axis=0)\nsales_std = posterior_svi_samples['y'].std(axis=0)\n\npredictions_svi = pd.DataFrame({\n    'customers': customers,\n    'category': food_category,\n    'sales': sales, \n    'sales_mu': sales_mu,\n    'sales_std': sales_std,\n    'sales_high': sales_mu + sales_std,\n    'sales_low': sales_mu - sales_std\n})\n\npredictions_svi = predictions_svi.sort_values(by=['customers'])\n\n\nsns.scatterplot(\n    x=predictions_svi['customers'], y=predictions_svi['sales'], \n    hue=predictions_svi['category'], palette='tab10')\nsns.lineplot(\n    x=predictions_svi['customers'], y=predictions_svi['sales_mu'],\n    color='black')\nplt.fill_between(\n    x=predictions_svi['customers'], \n    y1=predictions_svi['sales_low'], \n    y2=predictions_svi['sales_high'],\n    color='grey',\n    alpha=0.25)\nplt.title('Pooled Parameters - SVI')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMixing Group and Common Parameters - MCMC\n\ncustomers_z = (customers - customers.mean()) / (customers.std())\nsales_std = sales / sales.max()\n\n\ndef pooled_sigma_model(food_cat, customers, sales=None):\n\n    P = len(np.unique(food_cat))\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', P):\n        beta = pyro.sample('beta', dist.Normal(10., 20.))\n    \n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n\n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\npyro.render_model(\n    pooled_sigma_model, (food_category, customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(pooled_sigma_model)\nmcmc_pooled_sigma = MCMC(kernel, 500, 300)\nmcmc_pooled_sigma.run(food_category, customers, sales)\n\nSample: 100%|██████████| 800/800 [00:03, 210.80it/s, step size=7.51e-01, acc. prob=0.906]\n\n\n\nmcmc_pooled_sigma.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   beta[0]     13.02      0.03     13.02     12.97     13.06    673.42      1.00\n   beta[1]      8.15      0.33      8.14      7.62      8.72    579.90      1.00\n   beta[2]      6.11      0.06      6.11      6.02      6.21    396.32      1.00\n     sigma     39.19      1.24     39.21     36.88     40.96    545.10      1.00\n\nNumber of divergences: 0\n\n\n\npooled_sigma_samples = mcmc_pooled_sigma.get_samples(1000)\npooled_sigma_predictive = Predictive(pooled_sigma_model, pooled_sigma_samples)(food_category, customers, None)\naz_pooled_sigma_inf = az.from_pyro(\n    posterior=mcmc_pooled_sigma, posterior_predictive=pooled_sigma_predictive)\n\n\nsales_df['food_cat_encode'] = food_category\nsales_df['sales_std'] = sales_std\nsales_df['customers_z'] = customers_z\n\nfig, ax = plt.subplots()\n\nfor i in range(3):\n    category_mask = sales_df['food_cat_encode'] == i\n    mu_cat = pooled_sigma_predictive['y'][:, category_mask].mean(axis=0)\n\n    customers = sales_df.loc[category_mask, ['customers']].values.flatten()\n    sales = sales_df.loc[category_mask, ['sales']].values.flatten()\n\n    ax.plot(customers, mu_cat, c='black')\n    ax.scatter(customers, sales)\n    az.plot_hdi(\n        x=customers,\n        y=pooled_sigma_predictive['y'][:, category_mask],\n        color='grey'\n    )\n    \nax.set_xlabel('Customers')\nax.set_ylabel('Sales')\nax.set_title('Pooled Sigma');\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\n\n\n\n\n\nMixing Group and Common Parameters - SVI\nUsing LaPlace approximation\n\ncustomers_z = (customers - customers.mean()) / (customers.std())\nsales_std = sales / sales.max()\n\n\ndef pooled_sigma_model(food_cat, customers, sales=None):\n\n    P = len(np.unique(food_cat))\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', P):\n        beta = pyro.sample('beta', dist.Normal(10., 20.))\n    \n    sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n\n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=sales)\n\n\nfrom torch.distributions import constraints, transforms\n\ndef pool_sigma_guide(food_cat, customers, sales=None):\n\n    P = len(np.unique(food_cat))\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', P):\n        \n        beta_scale = pyro.param(\n            'beta_scale', torch.tensor(1.), constraint=constraints.positive)\n        beta_loc = pyro.param('beta_loc', torch.randn(1))\n        beta = pyro.sample('beta', dist.Normal(beta_loc, beta_scale))\n    \n    sigma_loc = pyro.param(\n        'sigma_loc', torch.randn(1))\n    \n    sigma_scale = pyro.param(\n        'sigma_scale', 0.1 * torch.rand(1), constraint=constraints.positive)\n    \n    sigma = pyro.sample('sigma', dist.TransformedDistribution(\n        dist.Normal(sigma_loc, sigma_scale), transforms=transforms.ExpTransform()\n    ))\n    #sigma = pyro.sample('sigma', dist.HalfCauchy(sigma_scale))\n\n\npyro.render_model(\n    pool_sigma_guide, (food_category, customers, sales), render_params=True)\n\n\nauto_guide = AutoLaplaceApproximation(pooled_sigma_model)\n\n\ncustomers = torch.tensor(sales_df['customers'].values, dtype=torch.float64)\nsales = torch.tensor(sales_df['sales'].values, dtype=torch.float64)\nfood_category = torch.tensor(sales_df['Food_Category'].cat.codes.values, dtype=torch.long)\n\npyro.clear_param_store()\n\nadam_params = {'lr': 0.002}\noptim = Adam(adam_params)\nsvi = SVI(pooled_sigma_model, auto_guide, optim, Trace_ELBO())\n#svi = SVI(pooled_sigma_model, pool_sigma_guide, optim, Trace_ELBO())\n\niter = 2000\nelbo_loss = []\nfor i in range(iter):\n    loss = svi.step(food_category, customers, sales)\n    elbo_loss.append(loss)\n\nplt.plot(np.arange(1, iter+1), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter {len(elbo_loss)}, loss: {elbo_loss[-1]:.4f}')\nplt.show()\n\n\n\n\n\n\n\n\n\npredictive = Predictive(pooled_sigma_model, guide=auto_guide, num_samples=1000)\nposterior_svi_samples = predictive(food_category, customers, None)\n\n\nsales_df['food_cat_encode'] = food_category\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor i in range(3):\n    category_mask = sales_df['food_cat_encode'] == i\n    mu_cat = posterior_svi_samples['y'][:, category_mask].mean(axis=0)\n\n    customers = sales_df.loc[category_mask, ['customers']].values.flatten()\n    sales = sales_df.loc[category_mask, ['sales']].values.flatten()\n\n    ax.plot(customers, mu_cat, c='black')\n    ax.scatter(customers, sales)\n    az.plot_hdi(\n        x=customers,\n        y=posterior_svi_samples['y'][:, category_mask],\n        color='grey'\n    )\n    \nax.set_xlabel('Customers')\nax.set_ylabel('Sales')\nax.set_title('Pooled Sigma')\n\n/Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n/Users/wastechs/opt/anaconda3/envs/probs/lib/python3.8/site-packages/arviz/plots/hdiplot.py:157: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\nText(0.5, 1.0, 'Pooled Sigma')"
  },
  {
    "objectID": "posts/2022-07-22-bmcp-ch-4.html#hierarchical-models",
    "href": "posts/2022-07-22-bmcp-ch-4.html#hierarchical-models",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 4",
    "section": "Hierarchical Models",
    "text": "Hierarchical Models\nIn the multi-level model above, the \\(\\sigma\\) is assumed to be the same for all 3 categories. Instead, we can say that \\(\\sigma\\) comes from the same underlying distribution, but is allowed to vary by category.\n\ndef unpooled_model(food_cat, customers, sales=None):\n\n    P = 3\n    N = len(customers)\n\n    with pyro.plate('food_cat_i', len(np.unique(food_cat))):\n        sigma = pyro.sample('sigma', dist.HalfNormal(20.))\n        beta = pyro.sample('beta', dist.Normal(10., 10.))\n    \n    with pyro.plate('data', N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\ndef hierarchical_model(food_cat, customers, sales=None):\n\n    N = len(customers)\n    P = len(np.unique(food_cat))\n\n    sigma_hyperprior = pyro.sample('sigma_hyperprior', dist.HalfNormal(20.))\n\n    with pyro.plate('food_cat_i', size=P):\n        beta = pyro.sample('beta', dist.Normal(10., 20.))\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_hyperprior))\n    \n    with pyro.plate('output', size=N):\n        mu = pyro.deterministic('mu', beta[food_cat] * customers)\n        output = pyro.sample('y', dist.Normal(mu, sigma[food_cat]), obs=sales)\n\n\npyro.render_model(\n    hierarchical_model, (food_category, customers, sales),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(hierarchical_model)\nmcmc_hm = MCMC(kernel, 500, 300)\nmcmc_hm.run(food_category, customers, sales)\n\nSample: 100%|██████████| 800/800 [00:07, 112.51it/s, step size=4.78e-01, acc. prob=0.868]\n\n\n\nmcmc_hm.summary()\n\n\n                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n           beta[0]     13.02      0.03     13.02     12.97     13.07    392.00      1.00\n           beta[1]      8.13      0.28      8.13      7.76      8.59    419.29      1.00\n           beta[2]      6.12      0.05      6.11      6.04      6.20    581.77      1.00\n          sigma[0]     40.28      1.44     40.20     38.08     42.58    347.26      1.00\n          sigma[1]     26.57     12.56     23.87      8.74     44.99    323.47      1.00\n          sigma[2]     36.24      2.63     36.11     32.35     40.71    503.11      1.00\n  sigma_hyperprior     31.52      9.10     30.20     16.75     45.43    542.93      1.00\n\nNumber of divergences: 2\n\n\n\nPosterior Geometry Matters\n\ndef salad_generator(hyperprior_beta_mean=5, hyperprior_beta_sigma=.2, \nsigma=50, days_per_location=[6, 4, 15, 10, 3, 5], \nsigma_per_location=[50,10,20,80,30,20]):\n    \"\"\"Generate noisy salad data\"\"\"\n\n    beta_hyperprior = stats.norm(hyperprior_beta_mean, hyperprior_beta_sigma)\n    \n    # Generate demands days per restaurant\n    df = pd.DataFrame()\n    for i, days in enumerate(days_per_location):\n        np.random.seed(0)\n\n        num_customers = stats.randint(30, 100).rvs(days)\n        sales_location = beta_hyperprior.rvs()*num_customers + stats.norm(0, sigma_per_location[i]).rvs(num_customers.shape)\n\n        location_df = pd.DataFrame({\"customers\":num_customers, \"sales\":sales_location})\n        location_df[\"location\"] = i\n        location_df.sort_values(by=\"customers\", ascending=True)\n        df = pd.concat([df, location_df])\n        \n    df.reset_index(inplace=True, drop=True)\n    \n    return df\n\nhierarchical_salad_df = salad_generator()\n\n\nfig, axes, = plt.subplots(2,3, sharex=True, sharey=True)\n\nfor i, ax in enumerate(axes.ravel()):\n    location_filter = (hierarchical_salad_df[\"location\"] == i)\n    hierarchical_salad_df[location_filter].plot(\n        kind=\"scatter\", x=\"customers\", y=\"sales\", ax=ax)\n    ax.set_xlabel(\"\")\n    ax.set_ylabel(\"\")\n\naxes[1, 0].set_xlabel(\"Number of Customers\")\naxes[1, 0].set_ylabel(\"Sales\");\n\n\n\n\n\n\n\n\n\ncustomers = torch.tensor(\n    hierarchical_salad_df['customers'].values, dtype=torch.float)\nsales = torch.tensor(\n    hierarchical_salad_df['sales'].values, dtype=torch.float)\nlocation = torch.tensor(\n    hierarchical_salad_df['location'].values, dtype=torch.long)\n\n\ndef hierarchical_salad_model(customers, location, sales=None):\n\n    N = len(customers)\n    P = len(np.unique(location))\n\n    beta_loc_hyper = pyro.sample('beta_loc_hyper', dist.Normal(0., 10.))\n    beta_scale_hyper = pyro.sample('beta_scale_hyper', dist.HalfNormal(.1))\n    sigma_hyper = pyro.sample('sigma_hyper', dist.HalfNormal(30.))\n\n    with pyro.plate('location_i', size=P):\n        beta = pyro.sample('beta', dist.Normal(beta_loc_hyper, beta_scale_hyper))\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_hyper))\n    \n    with pyro.plate('output', size=N):\n        mu = pyro.deterministic('mu', beta[location] * sigma[location])\n        output = pyro.sample('y', dist.Normal(mu, sigma[location]), obs=sales)\n\n\npyro.render_model(\n    hierarchical_salad_model, (customers, location, sales),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(hierarchical_salad_model)\nmcmc_salad = MCMC(kernel, 500, 300)\nmcmc_salad.run(customers, location, sales)\n\nSample: 100%|██████████| 800/800 [00:51, 15.55it/s, step size=8.40e-02, acc. prob=0.714] \n\n\n\nmcmc_salad.summary()\n\n\n                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n           beta[0]      3.97      0.44      4.04      3.27      4.66      8.21      1.37\n           beta[1]      3.97      0.43      4.06      3.29      4.66      8.02      1.38\n           beta[2]      3.95      0.43      4.02      3.34      4.73      7.88      1.38\n           beta[3]      3.94      0.43      4.02      3.29      4.67      7.87      1.38\n           beta[4]      3.97      0.43      4.05      3.35      4.75      7.70      1.38\n           beta[5]      3.97      0.44      4.04      3.35      4.72      7.98      1.39\n    beta_loc_hyper      3.96      0.43      4.04      3.31      4.64      7.56      1.40\n  beta_scale_hyper      0.08      0.06      0.07      0.02      0.16     16.84      1.07\n          sigma[0]     95.53     13.35     94.09     71.52    112.00     31.79      1.11\n          sigma[1]    110.74     16.58    112.76     82.07    129.72     37.98      1.01\n          sigma[2]     97.10     11.26     95.03     77.21    113.18     11.57      1.25\n          sigma[3]    100.95     11.85     98.67     82.71    119.33     13.40      1.20\n          sigma[4]    113.05     18.59    110.91     85.69    139.79     53.15      1.07\n          sigma[5]    104.10     17.04    102.16     81.35    127.71      7.66      1.32\n       sigma_hyper     76.34     13.05     74.29     57.97     98.81     68.83      1.08\n\nNumber of divergences: 77\n\n\n\nsalad_samples = mcmc_salad.get_samples(1000)\nsalad_predictive_samples = Predictive(\n    hierarchical_salad_model, salad_samples)(customers, location, None)\n\naz_salad_inf = az.from_pyro(\n    posterior=mcmc_salad, posterior_predictive=salad_predictive_samples)\n\n\nslope_centered = salad_samples['beta'][..., 4].numpy().flatten()\nsigma_centered = salad_samples['beta_scale_hyper'].numpy().flatten()\ndivergences_centered = np.array(mcmc_salad.diagnostics()['divergences']['chain 0'])\n\n\ndivergent_samples = pd.DataFrame({\n    'slope': slope_centered,\n    'sigma': sigma_centered\n})\n\nmask = divergent_samples.index.isin(pd.Index(divergences_centered))\ndivergent_samples['divergence'] = [1 if booly == True else 0 for booly in mask]\n\nax = sns.jointplot(\n    data=divergent_samples, x='slope', y='sigma', \n    color='grey', hue='divergence', palette=\"muted\")\nax.set_axis_labels(xlabel='$param: \\\\beta_m}$', ylabel='hyperprior: $\\\\beta_{\\sigma}$');\n\n\n\n\n\n\n\n\n\ndef non_centered_hierarchical_salad_model(customers, location, sales=None):\n\n    N = len(customers)\n    P = len(np.unique(location))\n\n    beta_loc_hyper = pyro.sample('beta_loc_hyper', dist.Normal(0., 10.))\n    beta_scale_hyper = pyro.sample('beta_scale_hyper', dist.HalfNormal(.1))\n    sigma_hyper = pyro.sample('sigma_hyper', dist.HalfNormal(30.))\n\n    with pyro.plate('location_i', size=P):\n        beta_offset = pyro.sample('beta_offset', dist.Normal(0., 1.))\n        #beta = pyro.sample('beta', dist.Normal(beta_loc_hyper, beta_scale_hyper))\n        sigma = pyro.sample('sigma', dist.HalfNormal(sigma_hyper))\n        beta = pyro.deterministic('beta', beta_offset * beta_scale_hyper + beta_loc_hyper) \n    \n    with pyro.plate('output', size=N):\n        mu = pyro.deterministic('mu', beta[location] * sigma[location])\n        output = pyro.sample('y', dist.Normal(mu, sigma[location]), obs=sales)\n\n\npyro.render_model(\n    non_centered_hierarchical_salad_model, \n    (customers, location, sales),\n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(non_centered_hierarchical_salad_model)\nmcmc_non_centered_salad = MCMC(kernel, 500, 300)\nmcmc_non_centered_salad.run(customers, location, sales)\n\nSample: 100%|██████████| 800/800 [00:21, 37.13it/s, step size=3.39e-01, acc. prob=0.892]\n\n\n\nmcmc_non_centered_salad.summary()\n\n\n                        mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_loc_hyper      3.96      0.44      3.94      3.16      4.56    191.41      1.01\n    beta_offset[0]     -0.01      1.05      0.03     -1.73      1.88    655.40      1.00\n    beta_offset[1]      0.10      1.05      0.11     -1.50      1.79    433.58      1.00\n    beta_offset[2]     -0.01      0.96     -0.03     -1.53      1.56    710.84      1.00\n    beta_offset[3]     -0.06      1.06     -0.04     -1.61      1.74    758.38      1.00\n    beta_offset[4]      0.11      0.97      0.12     -1.34      1.88    590.92      1.00\n    beta_offset[5]      0.16      0.89      0.11     -1.24      1.64    503.59      1.00\n  beta_scale_hyper      0.08      0.06      0.07      0.00      0.16    422.26      1.00\n          sigma[0]     94.50     13.61     92.37     74.43    116.52    293.44      1.00\n          sigma[1]    105.06     17.08    102.42     76.38    131.42    306.25      1.00\n          sigma[2]     96.19     10.89     95.25     77.80    112.88    233.29      1.02\n          sigma[3]    101.39     12.69    100.16     81.71    121.43    212.00      1.02\n          sigma[4]    110.76     18.83    107.95     83.05    138.54    277.04      1.00\n          sigma[5]    106.44     15.11    104.96     82.36    130.42    272.45      1.00\n       sigma_hyper     76.73     13.90     75.53     54.77    100.05    467.62      1.00\n\nNumber of divergences: 0\n\n\n\nnon_centered_salad_samples = mcmc_non_centered_salad.get_samples(1000)\nsalad_predictive_samples = Predictive(\n    non_centered_hierarchical_salad_model, non_centered_salad_samples)(customers, location, None)\n\naz_salad_inf = az.from_pyro(\n    posterior=mcmc_non_centered_salad, posterior_predictive=salad_predictive_samples)\n\n\nslope_un_centered = salad_predictive_samples['beta'][..., 4].numpy().flatten() ## index 4th beta param\nsigma_un_centered = non_centered_salad_samples['beta_scale_hyper'].numpy().flatten()\ndivergences_un_centered = np.array(mcmc_non_centered_salad.diagnostics()['divergences']['chain 0'])\n\n\nnon_centered = pd.DataFrame({\n    'beta_b4': slope_un_centered,\n    'beta_sigma_hyper': sigma_un_centered,\n    'parameterization': 'non_centered'\n})\n\nnon_centered_mask = non_centered.index.isin(pd.Index(divergences_un_centered))\nnon_centered['divergence'] = [1 if booly == True else 0 for booly in non_centered_mask]\n\ncentered = pd.DataFrame({\n    'beta_b4': slope_centered,\n    'beta_sigma_hyper': sigma_centered,\n    'parameterization': 'centered'\n})\n\ncentered_mask = centered.index.isin(pd.Index(divergences_centered))\ncentered['divergence'] = [1 if booly == True else 0 for booly in centered_mask]\n\ndf = pd.concat([non_centered, centered])\n\n\ng = sns.FacetGrid(df, col='parameterization', hue='divergence', height=5)\ng.map_dataframe(sns.scatterplot, 'beta_b4', 'beta_sigma_hyper')\ng.set_axis_labels(xlabel='$\\\\beta_[4]$', ylabel='$\\\\beta_{\\sigma h}$')\ng.add_legend()\nplt.show()\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df[df['parameterization'] == 'centered']['beta_sigma_hyper'])\nsns.kdeplot(df[df['parameterization'] == 'non_centered']['beta_sigma_hyper'])\nplt.legend(['centered', 'non centered'])\nplt.title('Difference in $\\\\beta_{sigma}$ hyperprior estimates');\n\n\n\n\n\n\n\n\n\n\nPredictions at Multiple Levels\nUsing the fitted parameter estimates to make an out of sample prediction for the distribution of sales for 50 customers. - posterior predictive: learned parameters; the data the model expects to see - customer = data \\(x\\)\n\n# .reshape(-1, 1) to ensure dimensions remain as [1000, 6] when multiplying\n# 6 b/c of 6 locations\nbeta = (\n    non_centered_salad_samples['beta_offset'] * \n    non_centered_salad_samples['beta_scale_hyper'].reshape(-1, 1) + \n    non_centered_salad_samples['beta_loc_hyper'].reshape(-1, 1)\n    )\n\nbeta.size()\n\ntorch.Size([1000, 6])\n\n\n\nbeta_group = dist.Normal(\n    non_centered_salad_samples['beta_loc_hyper'],\n    non_centered_salad_samples['beta_scale_hyper']\n    ).sample((100,))\n\n# aggregate predictions\ngroup_level_sales_prediction = dist.Normal(\n    beta_group * 50, \n    non_centered_salad_samples['sigma_hyper']\n    ).sample((100,))\n\n# location 2 and 4\nlocation_two = dist.Normal(\n    beta[:, 2] * 50,\n    non_centered_salad_samples['sigma'][:, 2]\n    ).sample((100,))\n\nlocation_four = dist.Normal(\n    beta[:, 4] * 50,\n    non_centered_salad_samples['sigma'][:, 4]\n    ).sample((100,))\n\n\nsns.kdeplot(group_level_sales_prediction.flatten(), clip=[0, 600])\nsns.kdeplot(location_two.flatten(), clip=[0, 600])\nsns.kdeplot(location_four.flatten(), clip=[0, 600])\nplt.legend(['All Locations', 'Location 2', 'Location 4'])\nplt.xlabel('Salad Sales')\nplt.title('Predicted Revenue with 50 Customers');"
  },
  {
    "objectID": "posts/2023-06-30-gsoc-update-comparisons.html",
    "href": "posts/2023-06-30-gsoc-update-comparisons.html",
    "title": "Google Summer of Code - Average Predictive Comparisons",
    "section": "",
    "text": "It is currently the end of week five of Google Summer of Code 2023. According to the original deliverables table outlined in my proposal, the goal was to have opened a draft PR for the core functionality of the plot_comparisons. Subsequently, week six and seven were to be spent further developing the plot_comparisons function, and writing tests and a demo notebook for the documentation, respectively. However, at the end of week five, I have a PR open with the majority of the functionality that marginaleffects has. In addition, I also exposed the comparisons function, added tests (which can and will be improved), and have started on documentation."
  },
  {
    "objectID": "posts/2023-06-30-gsoc-update-comparisons.html#average-predictive-differences",
    "href": "posts/2023-06-30-gsoc-update-comparisons.html#average-predictive-differences",
    "title": "Google Summer of Code - Average Predictive Comparisons",
    "section": "Average Predictive Differences",
    "text": "Average Predictive Differences\nHere, we adopt the notation from Chapter 14.4 of Regression and Other Stories to describe average predictive differences. Assume we have fit a Bambi model predicting an outcome \\(Y\\) based on inputs \\(X\\) and parameters \\(\\theta\\). Consider the following scalar inputs:\n\\[w: \\text{the input of interest}\\] \\[c: \\text{all the other inputs}\\] \\[X = (w, c)\\]\nSuppose for the input of interest, we are interested in comparing \\(w^{\\text{high}}\\) to \\(w^{\\text{low}}\\) (perhaps age = \\(60\\) and \\(40\\) respectively) with all other inputs \\(c\\) held constant. The predictive difference in the outcome changing only \\(w\\) is:\n\\[\\text{average predictive difference} = \\mathbb{E}(y|w^{\\text{high}}, c, \\theta) - \\mathbb{E}(y|w^{\\text{low}}, c, \\theta)\\]\nSelecting the maximum and minimum values of \\(w\\) and averaging over all other inputs \\(c\\) in the data gives you a new “hypothetical” dataset and corresponds to counting all pairs of transitions of \\((w^\\text{low})\\) to \\((w^\\text{high})\\), i.e., differences in \\(w\\) with \\(c\\) held constant. The difference between these two terms is the average predictive difference.\n\nComputing Average Predictive Differences\nThe objective of comparisons and plot_comparisons is to compute the expected difference in the outcome corresponding to three different scenarios for \\(w\\) and \\(c\\) where \\(w\\) is either provided by the user, else a default value is computed by Bambi (described in the default values section). The three scenarios are:\n\nuser provided values for \\(c\\).\na grid of equally spaced and central values for \\(c\\).\nempirical distribution (original data used to fit the model) for \\(c\\).\n\nIn the case of (1) and (2) above, Bambi assembles all pairwise combinations (transitions) of \\(w\\) and \\(c\\) into a new “hypothetical” dataset. In (3), Bambi uses the original \\(c\\), but replaces \\(w\\) with the user provided value or the default value computed by Bambi. In each scenario, predictions are made on the data using the fitted model. Once the predictions are made, comparisons are computed using the posterior samples by taking the difference in the predicted outcome for each pair of transitions. The average of these differences is the average predictive difference.\nThus, the goal of comparisons and plot_comparisons is to provide the modeler with a summary and visualization of the average predictive difference. Below, we demonstrate how to compute and plot average predictive differences with comparisons and plot_comparions using several examples.\n\nimport arviz as az\nimport numpy as np\nimport pandas as pd\n\nimport bambi as bmb"
  },
  {
    "objectID": "posts/2023-06-30-gsoc-update-comparisons.html#zero-inflated-poisson",
    "href": "posts/2023-06-30-gsoc-update-comparisons.html#zero-inflated-poisson",
    "title": "Google Summer of Code - Average Predictive Comparisons",
    "section": "Zero Inflated Poisson",
    "text": "Zero Inflated Poisson\nWe model and predict how many fish are caught by visitors at a state park using survey data. Many visitors catch zero fish, either because they did not fish at all, or because they were unlucky. We would like to explicitly model this bimodal behavior (zero versus non-zero) using a Zero Inflated Poisson model, and to compare how different inputs of interest \\(w\\) and other covariate values \\(c\\) are associated with the number of fish caught. The dataset contains data on 250 groups that went to a state park to fish. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), if they used a live bait and whether or not they brought a camper to the park (camper).\n\nfish_data = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\ncols = [\"count\", \"livebait\", \"camper\", \"persons\", \"child\"]\nfish_data = fish_data[cols]\nfish_data[\"livebait\"] = pd.Categorical(fish_data[\"livebait\"])\nfish_data[\"camper\"] = pd.Categorical(fish_data[\"camper\"])\n\n\nfish_model = bmb.Model(\n    \"count ~ livebait + camper + persons + child\", \n    fish_data, \n    family='zero_inflated_poisson'\n)\n\nfish_idata = fish_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [count_psi, Intercept, livebait, camper, persons, child]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\n\nUser Provided Values\nFirst, an example of scenario 1 (user provided values) is given below. In both plot_comparisons and comparisons, \\(w\\) and \\(c\\) are represented by contrast and conditional, respectively. The modeler has the ability to pass their own values for contrast and conditional by using a dictionary where the key-value pairs are the covariate and value(s) of interest. For example, if we wanted to compare the number of fish caught for \\(4\\) versus \\(1\\) persons conditional on a range of child and livebait values, we would pass the following dictionary in the code block below. By default, for \\(w\\), Bambi compares \\(w^\\text{high}\\) to \\(w^\\text{low}\\). Thus, in this example, \\(w^\\text{high}\\) = 4 and \\(w^\\text{low}\\) = 1. The user is not limited to passing a list for the values. A np.array can also be used. Furthermore, Bambi by default, maps the order of the dict keys to the main, group, and panel of the matplotlib figure. Below, since child is the first key, this is used for the x-axis, and livebait is used for the group (color). If a third key was passed, it would be used for the panel (facet).\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast={\"persons\": [1, 4]},\n    conditional={\"child\": [0, 1, 2], \"livebait\": [0, 1]},\n) \nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\nThe plot above shows that, comparing \\(4\\) to \\(1\\) persons given \\(0\\) children and using livebait, the expected difference is about \\(26\\) fish. When not using livebait, the expected difference decreases substantially to about \\(5\\) fish. Using livebait with a group of people is associated with a much larger expected difference in the number of fish caught.\ncomparisons can be called to view a summary dataframe that includes the term \\(w\\) and its contrast, the specified conditional covariate, and the expected difference in the outcome with the uncertainty interval (by default the 94% highest density interval is computed).\n\nbmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast={\"persons\": [1, 4]},\n    conditional={\"child\": [0, 1, 2], \"livebait\": [0, 1]},\n) \n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\nchild\nlivebait\ncamper\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\npersons\ndiff\n(1.0, 4.0)\n0.0\n0.0\n1.0\n4.834472\n2.563472\n7.037150\n\n\n1\npersons\ndiff\n(1.0, 4.0)\n0.0\n1.0\n1.0\n26.423188\n23.739729\n29.072748\n\n\n2\npersons\ndiff\n(1.0, 4.0)\n1.0\n0.0\n1.0\n1.202003\n0.631629\n1.780965\n\n\n3\npersons\ndiff\n(1.0, 4.0)\n1.0\n1.0\n1.0\n6.571943\n5.469275\n7.642248\n\n\n4\npersons\ndiff\n(1.0, 4.0)\n2.0\n0.0\n1.0\n0.301384\n0.143676\n0.467608\n\n\n5\npersons\ndiff\n(1.0, 4.0)\n2.0\n1.0\n1.0\n1.648417\n1.140415\n2.187190\n\n\n\n\n\n\n\nBut why is camper also in the summary dataframe? This is because in order to peform predictions, Bambi is expecting a value for each covariate used to fit the model. Additionally, with GLM models, average predictive comparisons are conditional in the sense that the estimate depends on the values of all the covariates in the model. Thus, for unspecified covariates, comparisons and plot_comparisons computes a default value (mean or mode based on the data type of the covariate). Thus, \\(c\\) = child, livebait, camper. Each row in the summary dataframe is read as “comparing \\(4\\) to \\(1\\) persons conditional on \\(c\\), the expected difference in the outcome is \\(y\\).”\n\n\nMultiple contrast values\nUsers can also perform comparisons on multiple contrast values. For example, if we wanted to compare the number of fish caught between \\((1, 2)\\), \\((1, 4)\\), and \\((2, 4)\\) persons conditional on a range of values for child and livebait.\n\nmultiple_values = bmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast={\"persons\": [1, 2, 4]},\n    conditional={\"child\": [0, 1, 2], \"livebait\": [0, 1]}\n)\n\nmultiple_values\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\nchild\nlivebait\ncamper\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\npersons\ndiff\n(1, 2)\n0.0\n0.0\n1.0\n0.527627\n0.295451\n0.775465\n\n\n1\npersons\ndiff\n(1, 2)\n0.0\n1.0\n1.0\n2.883694\n2.605690\n3.177685\n\n\n2\npersons\ndiff\n(1, 2)\n1.0\n0.0\n1.0\n0.131319\n0.067339\n0.195132\n\n\n3\npersons\ndiff\n(1, 2)\n1.0\n1.0\n1.0\n0.717965\n0.592968\n0.857893\n\n\n4\npersons\ndiff\n(1, 2)\n2.0\n0.0\n1.0\n0.032960\n0.015212\n0.052075\n\n\n5\npersons\ndiff\n(1, 2)\n2.0\n1.0\n1.0\n0.180270\n0.123173\n0.244695\n\n\n6\npersons\ndiff\n(1, 4)\n0.0\n0.0\n1.0\n4.834472\n2.563472\n7.037150\n\n\n7\npersons\ndiff\n(1, 4)\n0.0\n1.0\n1.0\n26.423188\n23.739729\n29.072748\n\n\n8\npersons\ndiff\n(1, 4)\n1.0\n0.0\n1.0\n1.202003\n0.631629\n1.780965\n\n\n9\npersons\ndiff\n(1, 4)\n1.0\n1.0\n1.0\n6.571943\n5.469275\n7.642248\n\n\n10\npersons\ndiff\n(1, 4)\n2.0\n0.0\n1.0\n0.301384\n0.143676\n0.467608\n\n\n11\npersons\ndiff\n(1, 4)\n2.0\n1.0\n1.0\n1.648417\n1.140415\n2.187190\n\n\n12\npersons\ndiff\n(2, 4)\n0.0\n0.0\n1.0\n4.306845\n2.267097\n6.280005\n\n\n13\npersons\ndiff\n(2, 4)\n0.0\n1.0\n1.0\n23.539494\n20.990931\n26.240169\n\n\n14\npersons\ndiff\n(2, 4)\n1.0\n0.0\n1.0\n1.070683\n0.565931\n1.585718\n\n\n15\npersons\ndiff\n(2, 4)\n1.0\n1.0\n1.0\n5.853978\n4.858957\n6.848519\n\n\n16\npersons\ndiff\n(2, 4)\n2.0\n0.0\n1.0\n0.268423\n0.124033\n0.412274\n\n\n17\npersons\ndiff\n(2, 4)\n2.0\n1.0\n1.0\n1.468147\n1.024800\n1.960934\n\n\n\n\n\n\n\nNotice how the contrast \\(w\\) varies while the covariates \\(c\\) are held constant. Currently, however, plotting multiple contrast values can be difficult to interpret since the contrast is “abstracted” away onto the y-axis. Thus, it would be difficult to interpret which portion of the plot corresponds to which contrast value. Therefore, it is currently recommended that if you want to plot multiple contrast values, call comparisons directly to obtain the summary dataframe and plot the results yourself.\n\n\nDefault contrast and conditional values\nNow, we move onto scenario 2 described above (grid of equally spaced and central values) in computing average predictive comparisons. You are not required to pass values for contrast and conditional. If you do not pass values, Bambi will compute default values for you. Below, it is described how these default values are computed.\nThe default value for contrast is a centered difference at the mean for a contrast variable with a numeric dtype, and unique levels for a contrast varaible with a categorical dtype. For example, if the modeler is interested in the comparison of a \\(5\\) unit increase in \\(w\\) where \\(w\\) is a numeric variable, Bambi computes the mean and then subtracts and adds \\(2.5\\) units to the mean to obtain a centered difference. By default, if no value is passed for the contrast covariate, Bambi computes a one unit centered difference at the mean. For example, if only contrast=\"persons\" is passed, then \\(\\pm\\) \\(0.5\\) is applied to the mean of persons. If \\(w\\) is a categorical variable, Bambi computes and returns the unique levels. For example, if \\(w\\) has levels [“high scool”, “vocational”, “university”], Bambi computes and returns the unique values of this variable.\nThe default values for conditional are more involved. Currently, by default, if a dict or list is passed to conditional, Bambi uses the ordering (keys if dict and elements if list) to determine which covariate to use as the main, group (color), and panel (facet) variable. This is the same logic used in plot_comparisons described above. Subsequently, the default values used for the conditional covariates depend on their ordering and dtype. Below, the psuedocode used for computing default values covariates passed to conditional is outlined:\nif v == \"main\":\n    \n    if v == numeric:\n        return np.linspace(v.min(), v.max(), 50)\n    elif v == categorical:\n        return np.unique(v)\n\nelif v == \"group\":\n    \n    if v == numeric:\n        return np.quantile(v, np.linspace(0, 1, 5))\n    elif v == categorical:\n        return np.unique(v)\n\nelif v == \"panel\":\n    \n    if v == numeric:\n        return np.quantile(v, np.linspace(0, 1, 5))\n    elif v == categorical:\n        return np.unique(v)\nThus, letting Bambi compute default values for conditional is equivalent to creating a hypothetical “data grid” of new values. Lets say we are interested in comparing the number of fish caught for the contrast livebait conditional on persons and child. This time, lets call comparisons first to gain an understanding of the data generating the plot.\n\ncontrast_df = bmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=[\"persons\", \"child\"],\n)\n\ncontrast_df.head(10)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\npersons\nchild\ncamper\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0.0, 1.0)\n1.000000\n0.0\n1.0\n1.694646\n1.252803\n2.081207\n\n\n1\nlivebait\ndiff\n(0.0, 1.0)\n1.000000\n1.0\n1.0\n0.422448\n0.299052\n0.551766\n\n\n2\nlivebait\ndiff\n(0.0, 1.0)\n1.000000\n3.0\n1.0\n0.026923\n0.012752\n0.043035\n\n\n3\nlivebait\ndiff\n(0.0, 1.0)\n1.061224\n0.0\n1.0\n1.787412\n1.342979\n2.203158\n\n\n4\nlivebait\ndiff\n(0.0, 1.0)\n1.061224\n1.0\n1.0\n0.445555\n0.317253\n0.580117\n\n\n5\nlivebait\ndiff\n(0.0, 1.0)\n1.061224\n3.0\n1.0\n0.028393\n0.013452\n0.045276\n\n\n6\nlivebait\ndiff\n(0.0, 1.0)\n1.122449\n0.0\n1.0\n1.885270\n1.422937\n2.313218\n\n\n7\nlivebait\ndiff\n(0.0, 1.0)\n1.122449\n1.0\n1.0\n0.469929\n0.335373\n0.609249\n\n\n8\nlivebait\ndiff\n(0.0, 1.0)\n1.122449\n3.0\n1.0\n0.029944\n0.014165\n0.047593\n\n\n9\nlivebait\ndiff\n(0.0, 1.0)\n1.183674\n0.0\n1.0\n1.988500\n1.501650\n2.424762\n\n\n\n\n\n\n\nAs livebait was encoded as a categorical dtype, Bambi returned the unique levels of \\([0, 1]\\) for the contrast. persons and child were passed as the first and second element and thus act as the main and group variables, respectively. It can be see from the output above, that an equally spaced grid was used to compute the values for persons, whereas a quantile based grid was used for child. Furthermore, as camper was unspecified, the mode was used as the default value. Lets go ahead and plot the commparisons.\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=[\"persons\", \"child\"],\n) \nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\nThe plot shows us that the expected differences in fish caught comparing a group of people who use livebait and no livebait is not only conditional on the number of persons, but also children. However, the plotted comparisons for child = \\(3\\) is difficult to interpret on a single plot. Thus, it can be useful to pass specific group and panel arguments to aid in the interpretation of the plot. Therefore, subplot_kwargs allows the user to manipulate the plotting by passing a dictionary where the keys are {\"main\": ..., \"group\": ..., \"panel\": ...} and the values are the names of the covariates to be plotted. Below, we plot the same comparisons as above, but this time we specify group and panel to both be child.\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=[\"persons\", \"child\"],\n    subplot_kwargs={\"main\": \"persons\", \"group\": \"child\", \"panel\": \"child\"},\n    fig_kwargs={\"figsize\":(12, 3), \"sharey\": True},\n    legend=False\n) \n\n\n\n\n\n\n\n\n\n\nUnit level contrasts\nEvaluating average predictive comparisons at central values for the conditional covariates \\(c\\) can be problematic when the inputs have a large variance since no single central value (mean, median, etc.) is representative of the covariate. This is especially true when \\(c\\) exhibits bi or multimodality. Thus, it may be desireable to use the empirical distribution of \\(c\\) to compute the predictive comparisons, and then average over a specific or set of covariates to obtain the average predictive comparisons. To achieve unit level contrasts, do not pass a parameter into conditional and or specify None.\n\nunit_level = bmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n)\n\n# empirical distribution\nprint(unit_level.shape[0] == fish_model.data.shape[0])\nunit_level.head(10)\n\nTrue\n\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ncamper\nchild\npersons\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0.0, 1.0)\n0.0\n0.0\n1.0\n0.864408\n0.627063\n1.116105\n\n\n1\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n0.0\n1.0\n1.694646\n1.252803\n2.081207\n\n\n2\nlivebait\ndiff\n(0.0, 1.0)\n0.0\n0.0\n1.0\n0.864408\n0.627063\n1.116105\n\n\n3\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n1.0\n2.0\n1.009094\n0.755449\n1.249551\n\n\n4\nlivebait\ndiff\n(0.0, 1.0)\n0.0\n0.0\n1.0\n0.864408\n0.627063\n1.116105\n\n\n5\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n2.0\n4.0\n1.453235\n0.964674\n1.956434\n\n\n6\nlivebait\ndiff\n(0.0, 1.0)\n0.0\n1.0\n3.0\n1.233247\n0.900295\n1.569891\n\n\n7\nlivebait\ndiff\n(0.0, 1.0)\n0.0\n3.0\n4.0\n0.188019\n0.090328\n0.289560\n\n\n8\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n2.0\n3.0\n0.606361\n0.390571\n0.818549\n\n\n9\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n0.0\n1.0\n1.694646\n1.252803\n2.081207\n\n\n\n\n\n\n\n\n# empirical (observed) data used to fit the model\nfish_model.data.head(10)\n\n\n\n\n\n\n\n\ncount\nlivebait\ncamper\npersons\nchild\n\n\n\n\n0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n1.0\n1.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n1.0\n2.0\n1.0\n\n\n4\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n5\n0.0\n1.0\n1.0\n4.0\n2.0\n\n\n6\n0.0\n1.0\n0.0\n3.0\n1.0\n\n\n7\n0.0\n1.0\n0.0\n4.0\n3.0\n\n\n8\n0.0\n0.0\n1.0\n3.0\n2.0\n\n\n9\n1.0\n1.0\n1.0\n1.0\n0.0\n\n\n\n\n\n\n\nAbove, unit_level is the comparisons summary dataframe and fish_model.data is the empirical data. Notice how the values for \\(c\\) are identical in both dataframes. However, for \\(w\\), the values are different. However, these unit level contrasts are difficult to interpret as each row corresponds to that unit’s contrast. Therefore, it is useful to average over (marginalize) the estimates to summarize the unit level predictive comparisons.\n\nMarginalizing over covariates\nSince the empirical distrubution is used for computing the average predictive comparisons, the same number of rows (250) is returned as the data used to fit the model. To average over a covariate, use the average_by argument. If True is passed, then comparisons averages over all covariates. Else, if a single or list of covariates are passed, then comparisons averages by the covariates passed.\n\n# marginalize over all covariates\nbmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=True\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0.0, 1.0)\n3.649691\n2.956185\n4.333621\n\n\n\n\n\n\n\nPassing True to average_by averages over all covariates and is equivalent to taking the mean of the estimate and uncertainty columns. For example:\n\nunit_level = bmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n)\n\nunit_level[[\"estimate\", \"lower_3.0%\", \"upper_97.0%\"]].mean()\n\nestimate       3.649691\nlower_3.0%     2.956185\nupper_97.0%    4.333621\ndtype: float64\n\n\n\n\nAverage by subgroups\nAveraging over all covariates may not be desired, and you would rather average by a group or specific covariate. To perform averaging by subgroups, users can pass a single or list of covariates to average_by to average over specific covariates. For example, if we wanted to average by persons:\n\n# average by number of persons\nbmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=\"persons\"\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\npersons\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n1.374203\n1.011290\n1.708711\n\n\n1\nlivebait\ndiff\n(0.0, 1.0)\n2.0\n1.963362\n1.543330\n2.376636\n\n\n2\nlivebait\ndiff\n(0.0, 1.0)\n3.0\n3.701510\n3.056586\n4.357385\n\n\n3\nlivebait\ndiff\n(0.0, 1.0)\n4.0\n7.358662\n6.047642\n8.655654\n\n\n\n\n\n\n\n\n# average by number of persons and camper by passing a list\nbmb.interpret.comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=[\"persons\", \"camper\"]\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\npersons\ncamper\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n0.0\n0.864408\n0.627063\n1.116105\n\n\n1\nlivebait\ndiff\n(0.0, 1.0)\n1.0\n1.0\n1.694646\n1.252803\n2.081207\n\n\n2\nlivebait\ndiff\n(0.0, 1.0)\n2.0\n0.0\n1.424598\n1.078389\n1.777154\n\n\n3\nlivebait\ndiff\n(0.0, 1.0)\n2.0\n1.0\n2.344439\n1.872191\n2.800661\n\n\n4\nlivebait\ndiff\n(0.0, 1.0)\n3.0\n0.0\n2.429459\n1.871578\n2.964242\n\n\n5\nlivebait\ndiff\n(0.0, 1.0)\n3.0\n1.0\n4.443540\n3.747840\n5.170052\n\n\n6\nlivebait\ndiff\n(0.0, 1.0)\n4.0\n0.0\n3.541921\n2.686445\n4.391176\n\n\n7\nlivebait\ndiff\n(0.0, 1.0)\n4.0\n1.0\n10.739204\n9.024702\n12.432764\n\n\n\n\n\n\n\nIt is still possible to use plot_comparisons when passing an argument to average_by. In the plot below, the empirical distribution is used to compute unit level contrasts for livebait and then averaged over persons to obtain the average predictive comparisons. The plot below is similar to the second plot in this notebook. The differences being that: (1) a pairwise transition grid is defined for the second plot above, whereas the empirical distribution is used in the plot below, and (2) in the plot below, we marginalized over the other covariates in the model (thus the reason for not having a camper or child group and panel, and a reduction in the uncertainty interval).\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=fish_model,\n    idata=fish_idata,\n    contrast=\"livebait\",\n    conditional=None,\n    average_by=\"persons\"\n)\nfig.set_size_inches(7, 3)"
  },
  {
    "objectID": "posts/2023-06-30-gsoc-update-comparisons.html#logistic-regression",
    "href": "posts/2023-06-30-gsoc-update-comparisons.html#logistic-regression",
    "title": "Google Summer of Code - Average Predictive Comparisons",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nTo showcase an additional functionality of comparisons and plot_comparisons, we fit a logistic regression model to the titanic dataset with interaction terms to model the probability of survival. The titanic dataset gives the values of four categorical attributes for each of the 2201 people on board the Titanic when it struck an iceberg and sank. The attributes are social class (first class, second class, third class, crewmember), age, sex (0 = female, 1 = male), and whether or not the person survived (0 = deceased, 1 = survived).\n\ndat = pd.read_csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\", index_col=0)\n\ndat[\"PClass\"] = dat[\"PClass\"].str.replace(\"[st, nd, rd]\", \"\", regex=True)\ndat[\"PClass\"] = dat[\"PClass\"].str.replace(\"*\", \"0\").astype(int)\ndat[\"PClass\"] = dat[\"PClass\"].replace(0, np.nan)\ndat[\"PClass\"] = pd.Categorical(dat[\"PClass\"], ordered=True)\ndat[\"SexCode\"] = pd.Categorical(dat[\"SexCode\"], ordered=True)\n\ndat = dat.dropna(axis=0, how=\"any\")\n\n\ntitanic_model = bmb.Model(\n    \"Survived ~ PClass * SexCode * Age\", \n    data=dat, \n    family=\"bernoulli\"\n)\ntitanic_idata = titanic_model.fit(draws=1000, target_accept=0.95, random_seed=1234)\n\nModeling the probability that Survived==1\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, PClass, SexCode, PClass:SexCode, Age, PClass:Age, SexCode:Age, PClass:SexCode:Age]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:15&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 16 seconds.\n\n\n\nComparison types\ncomparisons and plot_comparisons also allow you to specify the type of comparison to be computed. By default, a difference is used. However, it is also possible to take the ratio where comparisons would then become average predictive ratios. To achieve this, pass \"ratio\" into the argument comparison_type. Using different comparison types offers a way to produce alternative insights; especially when there are interaction terms as the value of one covariate depends on the value of the other covariate.\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model=titanic_model,\n    idata=titanic_idata,\n    contrast={\"PClass\": [1, 3]},\n    conditional=[\"Age\", \"SexCode\"],\n    comparison_type=\"ratio\",\n    subplot_kwargs={\"main\": \"Age\", \"group\": \"SexCode\", \"panel\": \"SexCode\"},\n    fig_kwargs={\"figsize\":(12, 3), \"sharey\": True},\n    legend=False\n\n)\n\n\n\n\n\n\n\n\nThe left panel shows that the ratio of the probability of survival comparing PClass \\(3\\) to \\(1\\) conditional on Age is non-constant. Whereas the right panel shows an approximately constant ratio in the probability of survival comparing PClass \\(3\\) to \\(1\\) conditional on Age.\n\n\nConclusion\nOverall, solid progress has been made with comparisons and plot_comparisons to give the Bambi modeller insights into GLMs. One of the most difficult aspects to program is the building of the contrast dataframe (the output above) as it requires tedious shape handling to ensure the predictive comparisons are “mapped” to the correct contrast and conditional values. Writing better tests to ensure shapes are correct is something I am also working on. Additionally, there are still “nice to have” features such as:\n\ncross-contrasts\ncomparisons other than the predictive difference or ratios (e.g., adjusted risk ratio)"
  },
  {
    "objectID": "posts/2022-06-23-ELBO.html",
    "href": "posts/2022-06-23-ELBO.html",
    "title": "Variational Inference - ELBO",
    "section": "",
    "text": "We don’t know the real posterior so we are going to choose a distribution \\(Q(\\theta)\\) from a family of distributions \\(Q^*\\) that are easy to work with and parameterized by \\(\\theta\\). The approximate distribution should be as close as possible to the true posterior. This closeness is measured using KL-Divergence. If we have the joint \\(p(x, z)\\) where \\(x\\) is some observed data, the goal is to perform inference: given what we have observed, what can we infer about the latent states?, i.e , we want the posterior.\nRecall Bayes theorem:\n\\[p(z | x) = \\frac{p(x|z)p(z)}{p(x)}\\]\nThe problem is the marginal \\(p(x = D)\\) as this could require a hundred, thousand, . . .dimensional integral:\n\\[p(x) = \\int_{z_0},...,\\int_{z_{D-1}}p(x, z)dz_0,...,d_{z_D{-1}}\\]\nIf we want the full posterior and can’t compute the marginal, then what’s the solution? Surrogate posterior. We want to approximate the true posterior using some known distribution:\n\\[q(z) \\approx p(z|X=D)\\]\nwhere \\(\\approx\\) can mean you want the approximated posterior to be “as good as possible”. Using variational inference, the objective is to minimize the distance between the surrogate \\(q(z)\\) and the true posterior \\(p(x)\\) using KL-Divergence:\n\\[q^*(z) = argmin_{q(z) \\in Q} (KL(q(z) || p(z|x=D)))\\]\nwhere \\(Q\\) is a more “simple” distribution. We can restate the KL-divergence as the expectation:\n\\[KL(q(z) || p(z|D)) = \\mathbb{E_{z \\sim q(z)}}[log \\frac{q(z)}{p(z|D)}]\\]\nwhich, taking the expectation over \\(z\\), is equivalent to integration:\n\\[\\int_{z_0}, . . .,\\int_{z_{D-1}}q(z)log\\frac{q(z)}{p(z|D)}d_{z_0},...,d_{z_{D-1}}\\]\nBut, sadly we don’t have \\(p(z \\vert D)\\) as this is the posterior! We only have the joint. Solution? Recall our KL-divergence:\n\\[KL(q(z) || p(z|D))\\]\nWe can rearrange the terms inside the \\(log\\) so that we can actually compute something:\n\\[\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz\\]\nWe only have the joint. Not the posterior; nor the marginal. We know from Bayes rule that we can express the posterior in terms of the joint \\(p(z, D)\\) divided by the marginal \\(p(x=D)\\):\n\\[p(z|D) = \\frac{p(Z, D)}{p(D)}\\]\nWe plug this inside of the \\(log\\):\n\\[\\int_{z}q(z)log(\\frac{q(z)p(D)}{p(z, D)})dz\\]\nHowever, the problem now is that we have reformulated our problem into another quantity that we don’t have, i.e., the marginal \\(p(D)\\). But we can put the quantity that we don’t have outside of the \\(log\\) to form two separate integrals.\n\\[\\int_z q(z)log(\\frac{q(z)}{p(z, D)})dz + \\int_zq(z)log(p(D)dz\\]\nThis is a valid rearrangement because of the properties of logarithms. In this case, the numerator is a product, so this turns into a sum of the second integral. What do we see in these two terms? We see an expectation over the quantity \\(\\frac{q(z)}{p(z, D)}\\) and another expectation over \\(p(D)\\). Rewriting in terms of expectation:\n\\[\\mathbb{E_{z{\\sim q(z)}}}[log(\\frac{q(z)}{p(z, D)})] + \\mathbb{E_{z \\sim q(z)}}[log(p(D))]\\]\nThe right term contains information we know—the functional form of the surrogate \\(q(z)\\) and the joint \\(p(z, D)\\) (in the form of a directed graphical model). We still don’t have access to \\(p(D)\\) on the right side, but this is a constant quantity. The expectation of a quantity that does not contain \\(z\\) is just whatever the expectation was taken over. Because of this, we can again rearrange:\n\\[-\\mathbb{E_{z \\sim q(z)}}[log \\frac{p(z, D)}{q(z)}]+log (p(D))\\]\nThe minus sign is a result of the “swapping” of the numerator and denominator and is required to make it a valid change. Looking at this, the left side is a function dependent on \\(q\\). In shorthand form, we can call this \\(\\mathcal{L(q)}\\). Our KL-divergence is:\n\\[KL = \\mathcal{-L(q)} + \\underbrace{log(p(D))}_\\textrm{evidence}\\]\nwhere \\(p(D)\\) is a value between \\([0, 1]\\) and this value is called the evidence which is the log probability of the data. If we apply the \\(log\\) to something between \\([0, 1]\\) then this value will be negative. This value is also constant since we have observed the dataset and thus does not change.\n\\(KL\\) is the distance (between the posterior and the surrogate) so it must be something positive. If the \\(KL\\) is positive and the evidence is negative, then in order to fulfill this equation, \\(\\mathcal{L}\\) must also be negative (negative times a negative is a positive). The \\(\\mathcal{L}\\) should be smaller than the evidence, and thus it is called the lower bound of the evidence \\(\\rightarrow\\) Evidence Lower Bound (ELBO).\nAgain, ELBO is defined as: \\(\\mathcal{L} = \\mathbb{E_{z \\sim q(z)}}[log(\\frac{p(z, D)}{q(z)})]\\) and is important to note that the ELBO is equal to the evidence if and only if the KL-divergence between the surrogate and the true posterior is \\(0\\):\n\\[\\mathcal{L(q)} = log(p(D)) \\textrm{ i.f.f. } KL(q(z)||p(z|D))=0\\]"
  },
  {
    "objectID": "posts/2023-10-10-predict-groups-bambi.html",
    "href": "posts/2023-10-10-predict-groups-bambi.html",
    "title": "Predict New Groups with Hierarchical Models in Bambi",
    "section": "",
    "text": "In Bambi, it is possible to perform predictions on new, unseen, groups of data that were not in the observed data used to fit the model with the argument sample_new_groups in the model.predict() method. This is useful in the context of hierarchical modeling, where groups are assumed to be a sample from a larger group.\nThis blog post is a copy of the zero inflated models documentation I wrote for Bambi. The original post can be found here.\nBelow, it is first described how predictions at multiple levels and for unseen groups are possible with hierarchical models. Then, it is described how this is performed in Bambi. Lastly, a hierarchical model is developed to show how to use the sample_new_groups argument in the model.predict() method, and within the interpret sub-package. For users coming from brms in R, this is equivalent to the sample_new_levels argument.\n\n\nA feature of hierarchical models is that they are able to make predictions at multiple levels. For example, if we were to use the penguin dataset to fit a hierchical regression to estimate the body mass of each penguin species given a set of predictors, we could estimate the mass of all penguins and each individual species at the same time. Thus, in this example, there are predictions for two levels: (1) the population level, and (2) the species level.\nAdditionally, a hierarchical model can be used to make predictions for groups (levels) that were never seen before if a hyperprior is defined over the group-specific effect. With a hyperior defined on group-specific effects, the groups do not share one fixed parameter, but rather share a hyperprior distribution which describes the distribution for the parameter of the prior itself. Lets write a hierarchical model (without intercepts) with a hyperprior defined for group-specific effects in statistical notation so this concept becomes more clear:\n\\[\\beta_{\\mu h} \\sim \\mathcal{N}(0, 10)\\] \\[\\beta_{\\sigma h} \\sim \\mathcal{HN}(10)\\] \\[\\beta_{m} \\sim \\mathcal{N}(\\beta_{\\mu h}, \\beta_{\\sigma h})\\] \\[\\sigma_{h} \\sim \\mathcal{HN}(10)\\] \\[\\sigma_{m} \\sim \\mathcal{HN}(\\sigma_{h})\\] \\[Y \\sim \\mathcal{N}(\\beta_{m} * X_{m}, \\sigma_{m})\\]\nThe parameters \\(\\beta_{\\mu h}, \\beta_{\\sigma h}\\) of the group-specific effect prior \\(\\beta_{m}\\) come from hyperprior distributions. Thus, if we would like to make predictions for a new, unseen, group, we can do so by first sampling from these hyperprior distributions to obtain the parameters for the new group, and then sample from the posterior or posterior predictive distribution to obtain the estimates for the new group. For a more in depth explanation of hierarchical models in Bambi, see either: the radon example, or the sleep study example.\n\n\n\nIf data with unseen groups are passed to the new_data argument of the model.predict() method, Bambi first needs to identify if that group exists, and if not, to evaluate the new group with the respective group-specific term. This evaluation updates the design matrix initially used to fit the model with the new group(s). This is achieved with the .evaluate_new_data method in the formulae package.\nOnce the design matrix has been updated, Bambi can perform predictions on the new, unseen, groups by specifying sample_new_groups=True in model.predict(). Each posterior sample for the new groups is drawn from the posterior draws of a randomly selected existing group. Since different groups may be selected at each draw, the end result represents the variation across existing groups.\n\n\n\nTo demonstrate the sample_new_groups argument, we will develop a hierarchical model on the OSIC Pulmonary Fibrosis Progression dataset. Pulmonary fibrosis is a disorder with no known cause and no known cure, created by scarring of the lungs. Using a hierarchical model, the objective is to predict a patient’s severity of decline in lung function. Lung function is assessed based on output from a spirometer, which measures the forced vital capacity (FVC), i.e. the volume of air exhaled by the patient.\n\n\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport bambi as bmb\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\n\n\nIn the dataset, we were provided with a baseline chest computerized tomography (CT) scan and associated clinical information for a set of patients where the columns represent the following\n\npatient- a unique id for each patient\nweeks- the relative number of weeks pre/post the baseline CT (may be negative)\nfvc - the recorded lung capacity in millilitres (ml)\npercent- a computed field which approximates the patient’s FVC as a percent of the typical FVC for a person of similar characteristics\nsex - male or female\nsmoking_status - ex-smoker, never smoked, currently smokes\nage - age of the patient\n\nA patient has an image acquired at time week = 0 and has numerous follow up visits over the course of approximately 1-2 years, at which time their FVC is measured. Below, we randomly sample three patients and plot their FVC measurements over time.\n\ndata = pd.read_csv(\n    \"https://gist.githubusercontent.com/ucals/\"\n    \"2cf9d101992cb1b78c2cdd6e3bac6a4b/raw/\"\n    \"43034c39052dcf97d4b894d2ec1bc3f90f3623d9/\"\n    \"osic_pulmonary_fibrosis.csv\"\n)\n\ndata.columns = data.columns.str.lower()\ndata.columns = data.columns.str.replace(\"smokingstatus\", \"smoking_status\")\ndata\n\n\n\n\n\n\n\n\npatient\nweeks\nfvc\npercent\nage\nsex\nsmoking_status\n\n\n\n\n0\nID00007637202177411956430\n-4\n2315\n58.253649\n79\nMale\nEx-smoker\n\n\n1\nID00007637202177411956430\n5\n2214\n55.712129\n79\nMale\nEx-smoker\n\n\n2\nID00007637202177411956430\n7\n2061\n51.862104\n79\nMale\nEx-smoker\n\n\n3\nID00007637202177411956430\n9\n2144\n53.950679\n79\nMale\nEx-smoker\n\n\n4\nID00007637202177411956430\n11\n2069\n52.063412\n79\nMale\nEx-smoker\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1544\nID00426637202313170790466\n13\n2712\n66.594637\n73\nMale\nNever smoked\n\n\n1545\nID00426637202313170790466\n19\n2978\n73.126412\n73\nMale\nNever smoked\n\n\n1546\nID00426637202313170790466\n31\n2908\n71.407524\n73\nMale\nNever smoked\n\n\n1547\nID00426637202313170790466\n43\n2975\n73.052745\n73\nMale\nNever smoked\n\n\n1548\nID00426637202313170790466\n59\n2774\n68.117081\n73\nMale\nNever smoked\n\n\n\n\n1549 rows × 7 columns\n\n\n\n\ndef label_encoder(labels):\n    \"\"\"\n    Encode patient IDs as integers.\n    \"\"\"\n    unique_labels = np.unique(labels)\n    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n    encoded_labels = labels.map(label_to_index)\n    return encoded_labels\n\n\npredictors = [\"patient\", \"weeks\", \"fvc\", \"smoking_status\"]\n\ndata[\"patient\"] = label_encoder(data['patient'])\n\ndata[\"weeks\"] = (data[\"weeks\"] - data[\"weeks\"].min()) / (\n    data[\"weeks\"].max() - data[\"weeks\"].min()\n)\ndata[\"fvc\"] = (data[\"fvc\"] - data[\"fvc\"].min()) / (\n    data[\"fvc\"].max() - data[\"fvc\"].min()\n)\n\ndata = data[predictors]\n\n\npatient_id = data.sample(n=3, random_state=42)[\"patient\"].values\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nfor i, p in enumerate(patient_id):\n    patient_data = data[data[\"patient\"] == p]\n    ax[i].scatter(patient_data[\"weeks\"], patient_data[\"fvc\"])\n    ax[i].set_xlabel(\"weeks\")\n    ax[i].set_ylabel(\"fvc\")\n    ax[i].set_title(f\"patient {p}\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe plots show variability in FVC measurements, unequal time intervals between follow up visits, and different number of visits per patient. This is a good scenario to use a hierarchical model, where we can model the FVC measurements for each patient as a function of time, and also model the variability in the FVC measurements across patients.\n\n\n\nThe hierarchical model we will develop is a partially pooled model using the predictors weeks, smoking_status, and patient to predict the response fvc. We will estimate the following model with common and group-effects:\n\ncommon-effects: weeks and smoking_status\ngroup-effects: the slope of weeks will vary by patient\n\nAdditionally, the global intercept is not included. Since the global intercept is excluded, smoking_status uses cell means encoding (i.e. the coefficient represents the estimate for each smoking_status category of the entire group). This logic also applies for weeks. However, a group-effect is also specified for weeks, which means that the association between weeks and the fvc is allowed to vary by individual patients.\nBelow, the default prior for the group-effect sigma is changed from HalfNormal to a Gamma distribution. Additionally, the model graph shows the model has been reparameterized to be non-centered. This is the default when there are group-effects in Bambi.\n\npriors = {\n    \"weeks|patient\": bmb.Prior(\"Normal\", mu=0, sigma=bmb.Prior(\"Gamma\", alpha=3, beta=3)),\n}\n\nmodel = bmb.Model(\n    \"fvc ~ 0 + weeks + smoking_status + (0 + weeks | patient)\",\n    data, \n    priors=priors,\n    categorical=[\"patient\", \"smoking_status\"],\n)\nmodel.build()\nmodel.graph()\n\n\n\n\n\n\n\n\n\nidata = model.fit(\n    draws=1500,\n    tune=1000,\n    target_accept=0.95,\n    chains=4,\n    random_seed=42,\n    cores=10,\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 10 jobs)\nNUTS: [fvc_sigma, weeks, smoking_status, weeks|patient_sigma, weeks|patient_offset]\n\n\n\n\n\n\n\n    \n      \n      100.00% [10000/10000 00:10&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_500 draw iterations (4_000 + 6_000 draws total) took 10 seconds.\n\n\n\n\n\nHierarchical models can induce difficult posterior geometries to sample from. Below, we quickly analyze the traces to ensure sampling went well.\n\naz.plot_trace(idata)\nplt.tight_layout();\n\n\n\n\n\n\n\n\nAnalyzing the marginal posteriors of weeks and weeks|patient, we see that the slope can be very different for some individuals. weeks indicates that as a population, the slope is negative. However, weeks|patients indicates some patients are negative, some are positive, and some are close to zero. Moreover, there are varying levels of uncertainty observed in the coefficients for the three different values of the smoking_status variable.\n\naz.summary(idata, var_names=[\"weeks\", \"smoking_status\", \"fvc_sigma\", \"weeks|patient_sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nweeks\n-0.116\n0.036\n-0.183\n-0.048\n0.002\n0.001\n447.0\n836.0\n1.01\n\n\nsmoking_status[Currently smokes]\n0.398\n0.017\n0.364\n0.429\n0.000\n0.000\n3315.0\n4274.0\n1.00\n\n\nsmoking_status[Ex-smoker]\n0.382\n0.005\n0.373\n0.392\n0.000\n0.000\n5104.0\n4835.0\n1.00\n\n\nsmoking_status[Never smoked]\n0.291\n0.007\n0.277\n0.305\n0.000\n0.000\n3299.0\n4290.0\n1.00\n\n\nfvc_sigma\n0.077\n0.001\n0.074\n0.080\n0.000\n0.000\n8405.0\n4362.0\n1.00\n\n\nweeks|patient_sigma\n0.458\n0.027\n0.412\n0.511\n0.001\n0.001\n767.0\n1559.0\n1.01\n\n\n\n\n\n\n\nThe effective sample size (ESS) is much lower for the weeks and weeks|patient_sigma parameters. This can also be inferred visually by looking at the trace plots for these parameters above. There seems to be some autocorrelation in the samples for these parameters. However, for the sake of this example, we will not worry about this.\n\n\n\nFirst, we will use the posterior distribution to plot the mean and 95% credible interval for the FVC measurements of the three randomly sampled patients above.\n\npreds = model.predict(idata, kind=\"mean\", inplace=False)\nfvc_mean = az.extract(preds[\"posterior\"])[\"fvc_mean\"]\n\n\n# plot posterior predictions\nfig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nfor i, p in enumerate(patient_id):\n    idx = data.index[data[\"patient\"] == p].tolist()\n    weeks = data.loc[idx, \"weeks\"].values\n    fvc = data.loc[idx, \"fvc\"].values\n\n    ax[i].scatter(weeks, fvc)\n    az.plot_hdi(weeks, fvc_mean[idx].T, color=\"C0\", ax=ax[i])\n    ax[i].plot(weeks, fvc_mean[idx].mean(axis=1), color=\"C0\")\n\n    ax[i].set_xlabel(\"weeks\")\n    ax[i].set_ylabel(\"fvc\")\n    ax[i].set_title(f\"patient {p}\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe plots show that the posterior estimates seem to fit the three patients well. Where there are more observations, the credible interval is smaller, and where there are fewer observations, the credible interval is larger. Next, we will predict new, unseen, patients.\n\n\n\nImagine the cost of acquiring a CT scan increases dramatically, and we would like to interopolate the FVC measurement for a new patient with a given set of clinical information smoking_status and weeks. We achieve this by passing this data to the predict method and setting sample_new_groups=True. As outlined in the Sampling new groups in Bambi section, this new data is evaluated by formulae to update the design matrix, and then predictions are made for the new group by sampling from the posterior draws of a randomly selected existing group.\nBelow, we will simulate a new patient and predict their FVC measurements over time. First, we will copy clinical data from patient 39 and use it for patient 176 (the new, unseen, patient). Subsequently, we will construct another new patient, with different clinical data.\n\n# copy patient 39 data to the new patient 176\npatient_39 = data[data[\"patient\"] == 39].reset_index(drop=True)\nnew_data = patient_39.copy()\nnew_data[\"patient\"] = 176\nnew_data = pd.concat([new_data, patient_39]).reset_index(drop=True)[predictors]\nnew_data\n\n\n\n\n\n\n\n\npatient\nweeks\nfvc\nsmoking_status\n\n\n\n\n0\n176\n0.355072\n0.378141\nEx-smoker\n\n\n1\n176\n0.376812\n0.365937\nEx-smoker\n\n\n2\n176\n0.391304\n0.401651\nEx-smoker\n\n\n3\n176\n0.405797\n0.405958\nEx-smoker\n\n\n4\n176\n0.420290\n0.390883\nEx-smoker\n\n\n5\n176\n0.456522\n0.390165\nEx-smoker\n\n\n6\n176\n0.543478\n0.348528\nEx-smoker\n\n\n7\n176\n0.637681\n0.337581\nEx-smoker\n\n\n8\n176\n0.746377\n0.365219\nEx-smoker\n\n\n9\n176\n0.775362\n0.360014\nEx-smoker\n\n\n10\n39\n0.355072\n0.378141\nEx-smoker\n\n\n11\n39\n0.376812\n0.365937\nEx-smoker\n\n\n12\n39\n0.391304\n0.401651\nEx-smoker\n\n\n13\n39\n0.405797\n0.405958\nEx-smoker\n\n\n14\n39\n0.420290\n0.390883\nEx-smoker\n\n\n15\n39\n0.456522\n0.390165\nEx-smoker\n\n\n16\n39\n0.543478\n0.348528\nEx-smoker\n\n\n17\n39\n0.637681\n0.337581\nEx-smoker\n\n\n18\n39\n0.746377\n0.365219\nEx-smoker\n\n\n19\n39\n0.775362\n0.360014\nEx-smoker\n\n\n\n\n\n\n\n\npreds = model.predict(\n    idata, kind=\"mean\",\n    data=new_data, \n    sample_new_groups=True,\n    inplace=False\n)\n\n\n# utility func for plotting\ndef plot_new_patient(idata, data, patient_ids):\n    fvc_mean = az.extract(idata[\"posterior\"])[\"fvc_mean\"]\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n    for i, p in enumerate(patient_ids):\n        idx = data.index[data[\"patient\"] == p].tolist()\n        weeks = data.loc[idx, \"weeks\"].values\n        fvc = data.loc[idx, \"fvc\"].values\n\n        if p == patient_ids[0]:\n            ax[i].scatter(weeks, fvc)\n\n        az.plot_hdi(weeks, fvc_mean[idx].T, color=\"C0\", ax=ax[i])\n        ax[i].plot(weeks, fvc_mean[idx].mean(axis=1), color=\"C0\")\n\n        ax[i].set_xlabel(\"weeks\")\n        ax[i].set_ylabel(\"fvc\")\n        ax[i].set_title(f\"patient {p}\")\n\n\nplot_new_patient(preds, new_data, [39, 176])\n\n\n\n\n\n\n\n\nAlthough identical data was used for both patients, the variability increased consideribly for patient 176. However, the mean predictions for both patients appear to be almost identical. Now, lets construct a new patient with different clinical data and see how the predictions change. We will select 10 time of follow up visits at random, and set the smoking_status = \"Currently smokes\".\n\nnew_data.loc[new_data[\"patient\"] == 176, \"smoking_status\"] = \"Currently smokes\"\nweeks = np.random.choice(sorted(model.data.weeks.unique()), size=10)\nnew_data.loc[new_data[\"patient\"] == 176, \"weeks\"] = weeks \nnew_data\n\n\n\n\n\n\n\n\npatient\nweeks\nfvc\nsmoking_status\n\n\n\n\n0\n176\n0.173913\n0.378141\nCurrently smokes\n\n\n1\n176\n0.137681\n0.365937\nCurrently smokes\n\n\n2\n176\n0.608696\n0.401651\nCurrently smokes\n\n\n3\n176\n0.717391\n0.405958\nCurrently smokes\n\n\n4\n176\n0.521739\n0.390883\nCurrently smokes\n\n\n5\n176\n0.246377\n0.390165\nCurrently smokes\n\n\n6\n176\n0.181159\n0.348528\nCurrently smokes\n\n\n7\n176\n0.702899\n0.337581\nCurrently smokes\n\n\n8\n176\n0.456522\n0.365219\nCurrently smokes\n\n\n9\n176\n0.333333\n0.360014\nCurrently smokes\n\n\n10\n39\n0.355072\n0.378141\nEx-smoker\n\n\n11\n39\n0.376812\n0.365937\nEx-smoker\n\n\n12\n39\n0.391304\n0.401651\nEx-smoker\n\n\n13\n39\n0.405797\n0.405958\nEx-smoker\n\n\n14\n39\n0.420290\n0.390883\nEx-smoker\n\n\n15\n39\n0.456522\n0.390165\nEx-smoker\n\n\n16\n39\n0.543478\n0.348528\nEx-smoker\n\n\n17\n39\n0.637681\n0.337581\nEx-smoker\n\n\n18\n39\n0.746377\n0.365219\nEx-smoker\n\n\n19\n39\n0.775362\n0.360014\nEx-smoker\n\n\n\n\n\n\n\nIf we were to keep the default value of sample_new_groups=False, the following error would be raised: ValueError: There are new groups for the factors ('patient',) and 'sample_new_groups' is False. Thus, we set sample_new_groups=True and obtain predictions for the new patient.\n\npreds = model.predict(\n    idata, kind=\"mean\",\n    data=new_data, \n    sample_new_groups=True,\n    inplace=False\n)\n\n\nplot_new_patient(preds, new_data, [39, 176])\n\n\n\n\n\n\n\n\nWith smoking_status = \"Currently smokes\", and the time of follow up visit randomly selected, we can see that the intercept is slightly higher, and it appears that the slope is steeper for this new patient. Again, the variability is much higher for patient 176, and in particular, where there are fewer fvc measurements.\n\n\nThe interpret sub-package in Bambi allows us to easily interpret the predictions for new patients. In particular, using bmb.interpret.comparisons, we can compare the predictions made for a new patient and an existing similar patient. Below, we will compare the predictions made for patient 176 and patient 39. We will use the same clinical data for both patients as we did in the first exampe above.\n\ntime_of_follow_up = list(new_data.query(\"patient == 39\")[\"weeks\"].values)\ntime_of_follow_up\n\n[0.35507246376811596,\n 0.37681159420289856,\n 0.391304347826087,\n 0.4057971014492754,\n 0.42028985507246375,\n 0.45652173913043476,\n 0.5434782608695652,\n 0.6376811594202898,\n 0.7463768115942029,\n 0.7753623188405797]\n\n\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model,\n    idata,\n    contrast={\"patient\": [39, 176]},\n    conditional={\"weeks\": time_of_follow_up, \"smoking_status\": \"Ex-smoker\"},\n    sample_new_groups=True,\n    fig_kwargs={\"figsize\": (7, 3)}\n)\nplt.title(\"Difference in predictions for patient 176 vs 39\");\n\n\n\n\n\n\n\n\nReferring to the plots where patient 39 and 176 use identical data, the mean fvc predictions “look” about the same. When this comparison is made quantitatively using the comparisons function, we can see that mean fvc measurements are slightly below 0.0, and have a constant slope across weeks indicating there is a slight difference in mean fvc measurements between the two patients.\n\n\n\n\n\nIn this notebook, it was shown how predictions at multiple levels and for unseen groups are possible with hierarchical models. To utilize this feature of hierarchical models, Bambi first updates the design matrix to include the new group. Then, predictions are made for the new group by sampling from the posterior draws of a randomly selected existing group.\nTo predict new groups in Bambi, you can either: (1) create a dataset with new groups and pass it to the model.predict() method while specifying sample_new_groups=True, or (2) use the functions comparisons or slopes in the interpret sub-package with sample_new_groups=True to compare predictions or slopes for new groups and existing groups.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Tue Oct 10 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\nmatplotlib: 3.7.1\narviz     : 0.16.1\npandas    : 2.1.0\nnumpy     : 1.24.2\nbambi     : 0.13.0.dev0\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2023-10-10-predict-groups-bambi.html#hierarchical-models-and-predictions-at-multiple-levels",
    "href": "posts/2023-10-10-predict-groups-bambi.html#hierarchical-models-and-predictions-at-multiple-levels",
    "title": "Predict New Groups with Hierarchical Models in Bambi",
    "section": "",
    "text": "A feature of hierarchical models is that they are able to make predictions at multiple levels. For example, if we were to use the penguin dataset to fit a hierchical regression to estimate the body mass of each penguin species given a set of predictors, we could estimate the mass of all penguins and each individual species at the same time. Thus, in this example, there are predictions for two levels: (1) the population level, and (2) the species level.\nAdditionally, a hierarchical model can be used to make predictions for groups (levels) that were never seen before if a hyperprior is defined over the group-specific effect. With a hyperior defined on group-specific effects, the groups do not share one fixed parameter, but rather share a hyperprior distribution which describes the distribution for the parameter of the prior itself. Lets write a hierarchical model (without intercepts) with a hyperprior defined for group-specific effects in statistical notation so this concept becomes more clear:\n\\[\\beta_{\\mu h} \\sim \\mathcal{N}(0, 10)\\] \\[\\beta_{\\sigma h} \\sim \\mathcal{HN}(10)\\] \\[\\beta_{m} \\sim \\mathcal{N}(\\beta_{\\mu h}, \\beta_{\\sigma h})\\] \\[\\sigma_{h} \\sim \\mathcal{HN}(10)\\] \\[\\sigma_{m} \\sim \\mathcal{HN}(\\sigma_{h})\\] \\[Y \\sim \\mathcal{N}(\\beta_{m} * X_{m}, \\sigma_{m})\\]\nThe parameters \\(\\beta_{\\mu h}, \\beta_{\\sigma h}\\) of the group-specific effect prior \\(\\beta_{m}\\) come from hyperprior distributions. Thus, if we would like to make predictions for a new, unseen, group, we can do so by first sampling from these hyperprior distributions to obtain the parameters for the new group, and then sample from the posterior or posterior predictive distribution to obtain the estimates for the new group. For a more in depth explanation of hierarchical models in Bambi, see either: the radon example, or the sleep study example."
  },
  {
    "objectID": "posts/2023-10-10-predict-groups-bambi.html#sampling-new-groups-in-bambi",
    "href": "posts/2023-10-10-predict-groups-bambi.html#sampling-new-groups-in-bambi",
    "title": "Predict New Groups with Hierarchical Models in Bambi",
    "section": "",
    "text": "If data with unseen groups are passed to the new_data argument of the model.predict() method, Bambi first needs to identify if that group exists, and if not, to evaluate the new group with the respective group-specific term. This evaluation updates the design matrix initially used to fit the model with the new group(s). This is achieved with the .evaluate_new_data method in the formulae package.\nOnce the design matrix has been updated, Bambi can perform predictions on the new, unseen, groups by specifying sample_new_groups=True in model.predict(). Each posterior sample for the new groups is drawn from the posterior draws of a randomly selected existing group. Since different groups may be selected at each draw, the end result represents the variation across existing groups."
  },
  {
    "objectID": "posts/2023-10-10-predict-groups-bambi.html#hierarchical-regression",
    "href": "posts/2023-10-10-predict-groups-bambi.html#hierarchical-regression",
    "title": "Predict New Groups with Hierarchical Models in Bambi",
    "section": "",
    "text": "To demonstrate the sample_new_groups argument, we will develop a hierarchical model on the OSIC Pulmonary Fibrosis Progression dataset. Pulmonary fibrosis is a disorder with no known cause and no known cure, created by scarring of the lungs. Using a hierarchical model, the objective is to predict a patient’s severity of decline in lung function. Lung function is assessed based on output from a spirometer, which measures the forced vital capacity (FVC), i.e. the volume of air exhaled by the patient.\n\n\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport bambi as bmb\n\nwarnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n\n\n\nIn the dataset, we were provided with a baseline chest computerized tomography (CT) scan and associated clinical information for a set of patients where the columns represent the following\n\npatient- a unique id for each patient\nweeks- the relative number of weeks pre/post the baseline CT (may be negative)\nfvc - the recorded lung capacity in millilitres (ml)\npercent- a computed field which approximates the patient’s FVC as a percent of the typical FVC for a person of similar characteristics\nsex - male or female\nsmoking_status - ex-smoker, never smoked, currently smokes\nage - age of the patient\n\nA patient has an image acquired at time week = 0 and has numerous follow up visits over the course of approximately 1-2 years, at which time their FVC is measured. Below, we randomly sample three patients and plot their FVC measurements over time.\n\ndata = pd.read_csv(\n    \"https://gist.githubusercontent.com/ucals/\"\n    \"2cf9d101992cb1b78c2cdd6e3bac6a4b/raw/\"\n    \"43034c39052dcf97d4b894d2ec1bc3f90f3623d9/\"\n    \"osic_pulmonary_fibrosis.csv\"\n)\n\ndata.columns = data.columns.str.lower()\ndata.columns = data.columns.str.replace(\"smokingstatus\", \"smoking_status\")\ndata\n\n\n\n\n\n\n\n\npatient\nweeks\nfvc\npercent\nage\nsex\nsmoking_status\n\n\n\n\n0\nID00007637202177411956430\n-4\n2315\n58.253649\n79\nMale\nEx-smoker\n\n\n1\nID00007637202177411956430\n5\n2214\n55.712129\n79\nMale\nEx-smoker\n\n\n2\nID00007637202177411956430\n7\n2061\n51.862104\n79\nMale\nEx-smoker\n\n\n3\nID00007637202177411956430\n9\n2144\n53.950679\n79\nMale\nEx-smoker\n\n\n4\nID00007637202177411956430\n11\n2069\n52.063412\n79\nMale\nEx-smoker\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1544\nID00426637202313170790466\n13\n2712\n66.594637\n73\nMale\nNever smoked\n\n\n1545\nID00426637202313170790466\n19\n2978\n73.126412\n73\nMale\nNever smoked\n\n\n1546\nID00426637202313170790466\n31\n2908\n71.407524\n73\nMale\nNever smoked\n\n\n1547\nID00426637202313170790466\n43\n2975\n73.052745\n73\nMale\nNever smoked\n\n\n1548\nID00426637202313170790466\n59\n2774\n68.117081\n73\nMale\nNever smoked\n\n\n\n\n1549 rows × 7 columns\n\n\n\n\ndef label_encoder(labels):\n    \"\"\"\n    Encode patient IDs as integers.\n    \"\"\"\n    unique_labels = np.unique(labels)\n    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n    encoded_labels = labels.map(label_to_index)\n    return encoded_labels\n\n\npredictors = [\"patient\", \"weeks\", \"fvc\", \"smoking_status\"]\n\ndata[\"patient\"] = label_encoder(data['patient'])\n\ndata[\"weeks\"] = (data[\"weeks\"] - data[\"weeks\"].min()) / (\n    data[\"weeks\"].max() - data[\"weeks\"].min()\n)\ndata[\"fvc\"] = (data[\"fvc\"] - data[\"fvc\"].min()) / (\n    data[\"fvc\"].max() - data[\"fvc\"].min()\n)\n\ndata = data[predictors]\n\n\npatient_id = data.sample(n=3, random_state=42)[\"patient\"].values\n\nfig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nfor i, p in enumerate(patient_id):\n    patient_data = data[data[\"patient\"] == p]\n    ax[i].scatter(patient_data[\"weeks\"], patient_data[\"fvc\"])\n    ax[i].set_xlabel(\"weeks\")\n    ax[i].set_ylabel(\"fvc\")\n    ax[i].set_title(f\"patient {p}\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe plots show variability in FVC measurements, unequal time intervals between follow up visits, and different number of visits per patient. This is a good scenario to use a hierarchical model, where we can model the FVC measurements for each patient as a function of time, and also model the variability in the FVC measurements across patients.\n\n\n\nThe hierarchical model we will develop is a partially pooled model using the predictors weeks, smoking_status, and patient to predict the response fvc. We will estimate the following model with common and group-effects:\n\ncommon-effects: weeks and smoking_status\ngroup-effects: the slope of weeks will vary by patient\n\nAdditionally, the global intercept is not included. Since the global intercept is excluded, smoking_status uses cell means encoding (i.e. the coefficient represents the estimate for each smoking_status category of the entire group). This logic also applies for weeks. However, a group-effect is also specified for weeks, which means that the association between weeks and the fvc is allowed to vary by individual patients.\nBelow, the default prior for the group-effect sigma is changed from HalfNormal to a Gamma distribution. Additionally, the model graph shows the model has been reparameterized to be non-centered. This is the default when there are group-effects in Bambi.\n\npriors = {\n    \"weeks|patient\": bmb.Prior(\"Normal\", mu=0, sigma=bmb.Prior(\"Gamma\", alpha=3, beta=3)),\n}\n\nmodel = bmb.Model(\n    \"fvc ~ 0 + weeks + smoking_status + (0 + weeks | patient)\",\n    data, \n    priors=priors,\n    categorical=[\"patient\", \"smoking_status\"],\n)\nmodel.build()\nmodel.graph()\n\n\n\n\n\n\n\n\n\nidata = model.fit(\n    draws=1500,\n    tune=1000,\n    target_accept=0.95,\n    chains=4,\n    random_seed=42,\n    cores=10,\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 10 jobs)\nNUTS: [fvc_sigma, weeks, smoking_status, weeks|patient_sigma, weeks|patient_offset]\n\n\n\n\n\n\n\n    \n      \n      100.00% [10000/10000 00:10&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_500 draw iterations (4_000 + 6_000 draws total) took 10 seconds.\n\n\n\n\n\nHierarchical models can induce difficult posterior geometries to sample from. Below, we quickly analyze the traces to ensure sampling went well.\n\naz.plot_trace(idata)\nplt.tight_layout();\n\n\n\n\n\n\n\n\nAnalyzing the marginal posteriors of weeks and weeks|patient, we see that the slope can be very different for some individuals. weeks indicates that as a population, the slope is negative. However, weeks|patients indicates some patients are negative, some are positive, and some are close to zero. Moreover, there are varying levels of uncertainty observed in the coefficients for the three different values of the smoking_status variable.\n\naz.summary(idata, var_names=[\"weeks\", \"smoking_status\", \"fvc_sigma\", \"weeks|patient_sigma\"])\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nweeks\n-0.116\n0.036\n-0.183\n-0.048\n0.002\n0.001\n447.0\n836.0\n1.01\n\n\nsmoking_status[Currently smokes]\n0.398\n0.017\n0.364\n0.429\n0.000\n0.000\n3315.0\n4274.0\n1.00\n\n\nsmoking_status[Ex-smoker]\n0.382\n0.005\n0.373\n0.392\n0.000\n0.000\n5104.0\n4835.0\n1.00\n\n\nsmoking_status[Never smoked]\n0.291\n0.007\n0.277\n0.305\n0.000\n0.000\n3299.0\n4290.0\n1.00\n\n\nfvc_sigma\n0.077\n0.001\n0.074\n0.080\n0.000\n0.000\n8405.0\n4362.0\n1.00\n\n\nweeks|patient_sigma\n0.458\n0.027\n0.412\n0.511\n0.001\n0.001\n767.0\n1559.0\n1.01\n\n\n\n\n\n\n\nThe effective sample size (ESS) is much lower for the weeks and weeks|patient_sigma parameters. This can also be inferred visually by looking at the trace plots for these parameters above. There seems to be some autocorrelation in the samples for these parameters. However, for the sake of this example, we will not worry about this.\n\n\n\nFirst, we will use the posterior distribution to plot the mean and 95% credible interval for the FVC measurements of the three randomly sampled patients above.\n\npreds = model.predict(idata, kind=\"mean\", inplace=False)\nfvc_mean = az.extract(preds[\"posterior\"])[\"fvc_mean\"]\n\n\n# plot posterior predictions\nfig, ax = plt.subplots(1, 3, figsize=(12, 3), sharey=True)\nfor i, p in enumerate(patient_id):\n    idx = data.index[data[\"patient\"] == p].tolist()\n    weeks = data.loc[idx, \"weeks\"].values\n    fvc = data.loc[idx, \"fvc\"].values\n\n    ax[i].scatter(weeks, fvc)\n    az.plot_hdi(weeks, fvc_mean[idx].T, color=\"C0\", ax=ax[i])\n    ax[i].plot(weeks, fvc_mean[idx].mean(axis=1), color=\"C0\")\n\n    ax[i].set_xlabel(\"weeks\")\n    ax[i].set_ylabel(\"fvc\")\n    ax[i].set_title(f\"patient {p}\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nThe plots show that the posterior estimates seem to fit the three patients well. Where there are more observations, the credible interval is smaller, and where there are fewer observations, the credible interval is larger. Next, we will predict new, unseen, patients.\n\n\n\nImagine the cost of acquiring a CT scan increases dramatically, and we would like to interopolate the FVC measurement for a new patient with a given set of clinical information smoking_status and weeks. We achieve this by passing this data to the predict method and setting sample_new_groups=True. As outlined in the Sampling new groups in Bambi section, this new data is evaluated by formulae to update the design matrix, and then predictions are made for the new group by sampling from the posterior draws of a randomly selected existing group.\nBelow, we will simulate a new patient and predict their FVC measurements over time. First, we will copy clinical data from patient 39 and use it for patient 176 (the new, unseen, patient). Subsequently, we will construct another new patient, with different clinical data.\n\n# copy patient 39 data to the new patient 176\npatient_39 = data[data[\"patient\"] == 39].reset_index(drop=True)\nnew_data = patient_39.copy()\nnew_data[\"patient\"] = 176\nnew_data = pd.concat([new_data, patient_39]).reset_index(drop=True)[predictors]\nnew_data\n\n\n\n\n\n\n\n\npatient\nweeks\nfvc\nsmoking_status\n\n\n\n\n0\n176\n0.355072\n0.378141\nEx-smoker\n\n\n1\n176\n0.376812\n0.365937\nEx-smoker\n\n\n2\n176\n0.391304\n0.401651\nEx-smoker\n\n\n3\n176\n0.405797\n0.405958\nEx-smoker\n\n\n4\n176\n0.420290\n0.390883\nEx-smoker\n\n\n5\n176\n0.456522\n0.390165\nEx-smoker\n\n\n6\n176\n0.543478\n0.348528\nEx-smoker\n\n\n7\n176\n0.637681\n0.337581\nEx-smoker\n\n\n8\n176\n0.746377\n0.365219\nEx-smoker\n\n\n9\n176\n0.775362\n0.360014\nEx-smoker\n\n\n10\n39\n0.355072\n0.378141\nEx-smoker\n\n\n11\n39\n0.376812\n0.365937\nEx-smoker\n\n\n12\n39\n0.391304\n0.401651\nEx-smoker\n\n\n13\n39\n0.405797\n0.405958\nEx-smoker\n\n\n14\n39\n0.420290\n0.390883\nEx-smoker\n\n\n15\n39\n0.456522\n0.390165\nEx-smoker\n\n\n16\n39\n0.543478\n0.348528\nEx-smoker\n\n\n17\n39\n0.637681\n0.337581\nEx-smoker\n\n\n18\n39\n0.746377\n0.365219\nEx-smoker\n\n\n19\n39\n0.775362\n0.360014\nEx-smoker\n\n\n\n\n\n\n\n\npreds = model.predict(\n    idata, kind=\"mean\",\n    data=new_data, \n    sample_new_groups=True,\n    inplace=False\n)\n\n\n# utility func for plotting\ndef plot_new_patient(idata, data, patient_ids):\n    fvc_mean = az.extract(idata[\"posterior\"])[\"fvc_mean\"]\n\n    fig, ax = plt.subplots(1, 2, figsize=(10, 3), sharey=True)\n    for i, p in enumerate(patient_ids):\n        idx = data.index[data[\"patient\"] == p].tolist()\n        weeks = data.loc[idx, \"weeks\"].values\n        fvc = data.loc[idx, \"fvc\"].values\n\n        if p == patient_ids[0]:\n            ax[i].scatter(weeks, fvc)\n\n        az.plot_hdi(weeks, fvc_mean[idx].T, color=\"C0\", ax=ax[i])\n        ax[i].plot(weeks, fvc_mean[idx].mean(axis=1), color=\"C0\")\n\n        ax[i].set_xlabel(\"weeks\")\n        ax[i].set_ylabel(\"fvc\")\n        ax[i].set_title(f\"patient {p}\")\n\n\nplot_new_patient(preds, new_data, [39, 176])\n\n\n\n\n\n\n\n\nAlthough identical data was used for both patients, the variability increased consideribly for patient 176. However, the mean predictions for both patients appear to be almost identical. Now, lets construct a new patient with different clinical data and see how the predictions change. We will select 10 time of follow up visits at random, and set the smoking_status = \"Currently smokes\".\n\nnew_data.loc[new_data[\"patient\"] == 176, \"smoking_status\"] = \"Currently smokes\"\nweeks = np.random.choice(sorted(model.data.weeks.unique()), size=10)\nnew_data.loc[new_data[\"patient\"] == 176, \"weeks\"] = weeks \nnew_data\n\n\n\n\n\n\n\n\npatient\nweeks\nfvc\nsmoking_status\n\n\n\n\n0\n176\n0.173913\n0.378141\nCurrently smokes\n\n\n1\n176\n0.137681\n0.365937\nCurrently smokes\n\n\n2\n176\n0.608696\n0.401651\nCurrently smokes\n\n\n3\n176\n0.717391\n0.405958\nCurrently smokes\n\n\n4\n176\n0.521739\n0.390883\nCurrently smokes\n\n\n5\n176\n0.246377\n0.390165\nCurrently smokes\n\n\n6\n176\n0.181159\n0.348528\nCurrently smokes\n\n\n7\n176\n0.702899\n0.337581\nCurrently smokes\n\n\n8\n176\n0.456522\n0.365219\nCurrently smokes\n\n\n9\n176\n0.333333\n0.360014\nCurrently smokes\n\n\n10\n39\n0.355072\n0.378141\nEx-smoker\n\n\n11\n39\n0.376812\n0.365937\nEx-smoker\n\n\n12\n39\n0.391304\n0.401651\nEx-smoker\n\n\n13\n39\n0.405797\n0.405958\nEx-smoker\n\n\n14\n39\n0.420290\n0.390883\nEx-smoker\n\n\n15\n39\n0.456522\n0.390165\nEx-smoker\n\n\n16\n39\n0.543478\n0.348528\nEx-smoker\n\n\n17\n39\n0.637681\n0.337581\nEx-smoker\n\n\n18\n39\n0.746377\n0.365219\nEx-smoker\n\n\n19\n39\n0.775362\n0.360014\nEx-smoker\n\n\n\n\n\n\n\nIf we were to keep the default value of sample_new_groups=False, the following error would be raised: ValueError: There are new groups for the factors ('patient',) and 'sample_new_groups' is False. Thus, we set sample_new_groups=True and obtain predictions for the new patient.\n\npreds = model.predict(\n    idata, kind=\"mean\",\n    data=new_data, \n    sample_new_groups=True,\n    inplace=False\n)\n\n\nplot_new_patient(preds, new_data, [39, 176])\n\n\n\n\n\n\n\n\nWith smoking_status = \"Currently smokes\", and the time of follow up visit randomly selected, we can see that the intercept is slightly higher, and it appears that the slope is steeper for this new patient. Again, the variability is much higher for patient 176, and in particular, where there are fewer fvc measurements.\n\n\nThe interpret sub-package in Bambi allows us to easily interpret the predictions for new patients. In particular, using bmb.interpret.comparisons, we can compare the predictions made for a new patient and an existing similar patient. Below, we will compare the predictions made for patient 176 and patient 39. We will use the same clinical data for both patients as we did in the first exampe above.\n\ntime_of_follow_up = list(new_data.query(\"patient == 39\")[\"weeks\"].values)\ntime_of_follow_up\n\n[0.35507246376811596,\n 0.37681159420289856,\n 0.391304347826087,\n 0.4057971014492754,\n 0.42028985507246375,\n 0.45652173913043476,\n 0.5434782608695652,\n 0.6376811594202898,\n 0.7463768115942029,\n 0.7753623188405797]\n\n\n\nfig, ax = bmb.interpret.plot_comparisons(\n    model,\n    idata,\n    contrast={\"patient\": [39, 176]},\n    conditional={\"weeks\": time_of_follow_up, \"smoking_status\": \"Ex-smoker\"},\n    sample_new_groups=True,\n    fig_kwargs={\"figsize\": (7, 3)}\n)\nplt.title(\"Difference in predictions for patient 176 vs 39\");\n\n\n\n\n\n\n\n\nReferring to the plots where patient 39 and 176 use identical data, the mean fvc predictions “look” about the same. When this comparison is made quantitatively using the comparisons function, we can see that mean fvc measurements are slightly below 0.0, and have a constant slope across weeks indicating there is a slight difference in mean fvc measurements between the two patients."
  },
  {
    "objectID": "posts/2023-10-10-predict-groups-bambi.html#summary",
    "href": "posts/2023-10-10-predict-groups-bambi.html#summary",
    "title": "Predict New Groups with Hierarchical Models in Bambi",
    "section": "",
    "text": "In this notebook, it was shown how predictions at multiple levels and for unseen groups are possible with hierarchical models. To utilize this feature of hierarchical models, Bambi first updates the design matrix to include the new group. Then, predictions are made for the new group by sampling from the posterior draws of a randomly selected existing group.\nTo predict new groups in Bambi, you can either: (1) create a dataset with new groups and pass it to the model.predict() method while specifying sample_new_groups=True, or (2) use the functions comparisons or slopes in the interpret sub-package with sample_new_groups=True to compare predictions or slopes for new groups and existing groups.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Tue Oct 10 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\nmatplotlib: 3.7.1\narviz     : 0.16.1\npandas    : 2.1.0\nnumpy     : 1.24.2\nbambi     : 0.13.0.dev0\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html",
    "href": "posts/2023-10-25-survival-models-bambi.html",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "Survival models, also known as time-to-event models, are specialized statistical methods designed to analyze the time until the occurrence of an event of interest. In this notebook, a review of survival analysis (using non-parametric and parametric methods) and censored data is provided, followed by a survival model implementation in Bambi.\nThis blog post is a copy of the survival models documentation I wrote for Bambi. The original post can be found here.\n\n\nSometimes the right way to model discrete, countable events is to model not the counts themselves but rather the time between events. This gives us information regarding the rate of an event. Survival models are models for countable things, but the outcomes we want to predict are durations. Durations are continuous deviations from some point of reference (so they are all positive values).\nThe tricky part with survival models is not the probability distribution assigned to the durations, but dealing with censoring. Censoring occurs when the event of interest does not occur in the window of observation. In a simple scenario, this can happen because the observation period ends before the event occurred. Censored individuals (or units) can not just be dropped from the sample. As an example, we use Richard McElreath’s cat adoption example from chapter 11.4 of Statistical Rethinking: Imagine a cohort of 100 cats who start waiting for adoption at the same time. After one month, half of them have been adopted. Now what is the rate of adoption? You can’t compute it using only the cats who have been adopted. You need to also account for the cats who haven’t yet been adopted. The cats who haven’t been adopted yet, but eventually will be adopted, clearly have longer waiting times than the cats who have already been adopted. So the average rate among those who are already adopted is biased upwards—it is confounded by conditioning on adoption.\nIncluding censored observations requires a new type of model. The key idea is that the same distribution assumption for the outcome tells us both the probability of any observed duration that end in the event as well as the probability that we would wait the observed duration without seeing the event. For each unit, we assume there is a true survival time \\(T\\) as well as a true censoring time \\(C\\). The survival time represents the time at which the event of interest occurs. The censoring time is the time at which censoring occurs. We observe either: the survival, or the censoring time:\n\\[Y = \\text{min}(T, C)\\]\nIf the event occurs, then we observe the survival time, else we observe the censoring time. In order to analyze survival data, we first need to understand the two types of censoring: left and right censoring, and how to estimate the survival function.\n\n\n\nThere are two main “types” of censoring: right and left. Right censoring occurs when \\(T \\ge Y\\), i.e. the true event time \\(T\\) is at least as large as the observed time \\(Y\\). This is a consequence of \\(Y = \\text{min}(T, C)\\). Right censoring derives its name from the notion that time is typically read and displayed from left to right. Left sensoring occurs when the true event time \\(T\\) is less than or equal to the observed time \\(Y\\). An example of left censoring could be in a study of pregnancy duration. Suppose that patients are surveyed 250 days (8.2 months) after conception. Some patients may have already had their babies. For these patients, pregnancy duration is less than 250 days.\n\n\n\nSurvival analysis is concerned with estimating the survival function \\(S(t)\\)\n\\[S(t) = Pr(T &gt; t) = 1 - F(t)\\]\nwhich is a decreasing function that quantifies the probability of surviving past time \\(t\\). Alternatively, \\(S(t)\\) can be expressed as one minus the cumulative distribution function (CDF) \\(F\\) of the event time \\(T\\)—referred to as the complementary cumulative distribution function (CCDF). The focus on the survival function is important because for censored observations, we only know that the time-to-event exceeds the observed time \\(Y\\).\nHere, continuing with the cat adoption example, we consider the task of estimating the survival function for cat adoptions. To estimate \\(S(30) = Pr(T &gt; 30)\\), the probability that a cat is not adopted after 30 days, it is tempting to compute the proportion of cats who were adopted before 30 days and subtract this from 1. However, this would be incorrect because it ignores the cats who were not adopted before 30 days but who will be adopted later—these cats clearly have longer adoption rates. Thus, if we continued with the naive approach, the average rate of adoption would be biased upwards—it is confounded by conditioning on adoption.\nHowever, it is possible to overcome this challenge by using the Kaplan-Meier estimator. The Kaplan-Meier estimator is a non-parametric estimator of the survival function that accounts for censoring. Let \\(d_1 &lt; d_2 &lt; . . . &lt; d_K\\) denote the \\(K\\) unique adoption times among the non-censored cats, and \\(q_k\\) denote the number of cats adopted at time \\(d_k\\). For \\(k = 1,...,K\\), let \\(r_k\\) denote the number of cats not adopted at time \\(d_k\\). By the law of total probability\n\\[Pr(T &gt; d_k) = Pr(T &gt; d_k | T &gt; d_{k-1}) Pr(T &gt; d_{k-1}) + Pr(T &gt; d_k | T \\leq d_{k-1}) Pr(T \\leq d_{k-1})\\]\nThe fact that \\(d_{k-1} &lt; d_k\\) implies that \\(Pr(T &gt; d_k | T \\leq d_{k-1}) = 0\\) (as it is impossible for a cat to be adopted past time \\(d_k\\) if the cat was adopted before time \\(d_{k-1}\\)). Thus, if we simplify the above equation and plug into the survival function, we obtain\n\\[S(d_k) = Pr(T &gt; d_k | T &gt; d_{k-1})S(d_{k-1})\\]\nNow we must estimate the terms on the right-hand side. It is common to use the following estimator\n\\[\\hat{Pr}(T &gt; d_j | T &gt; d_{j-1}) = \\frac{r_j - q_j}{r_j}\\]\nwhich leads us to the Kaplan-Meier estimator of the survival function\n\\[\\hat{S}(d_k) = \\prod_{j=1}^k \\frac{r_j - q_j}{r_j}\\]\nwhere \\(\\hat{S}(d_k)\\) represents the estimated survival probability up to time \\(d_k\\). The product is taken over all time points up to \\(k\\), where an event occurred. The variables \\(r_j\\) and \\(q_j\\) denote the number of subjects at risk and the number of events at time \\(d_j\\), respectively. The term \\(\\frac{r_j - q_j}{r_j}\\) is the conditional probability of surviving the \\(j\\)-th time point given that an individual has survived just before \\(d_j\\). Specifically, \\(r_j - q_j\\) are the number of individuals who survived just before \\(d_j\\) and \\(r_j\\) is the number of individuals who survived just after \\(d_j\\), and \\(r_j\\) are those who were at risk \\(d_j\\).\n\n\nBelow we use the KaplanMeierFitter class of the lifelines package to compute and visualize the survival curve for cat adoptions from an animal shelter in Austin, Texas beginning October 1st, 2013 until May 30th, 2018 (the last day the shelter rescued a cat). The dataset comes from the City of Austin Open Data Portal and contains columns such as animal name, date of birth, species, and many more. However, for the purpose of this notebook we are interested in the following columns: - days_to_event - number of days until the cat was adopted (date_in - date_out) - out_event - the reason for the cat leaving this particular shelter, e.g. adopted or transfered. - color - the color of the cat, e.g. white, blue, brown tabby, black.\n\n\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy\n\nfrom lifelines import KaplanMeierFitter\n\nimport bambi as bmb\n\n\n\nurl = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/AustinCats.csv\"\ncats_df = pd.read_csv(url, sep=\";\")\n\n\nplt.figure(figsize=(7, 3))\nplt.hist(cats_df[\"days_to_event\"], bins=250, label=\"Uncensored data\")\nplt.xlim(0, 186) # limit to 6 months for visibility\nplt.title(\"Days Until Adoption\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Days\")\nplt.legend();\n\n\n\n\n\n\n\n\nThe distribution of days until adoption exhibits a long tail with most cats (if we observe the adopt event) being adopted within the first month of inception. Note that the plot has been truncated to six months for better visibility. Below, we estimate the survival function using the KaplanMeierFitter class from the lifelines package.\n\nkm = KaplanMeierFitter()\nkm_adoptions = km.fit(\n    cats_df[\"days_to_event\"], \n    cats_df[\"out_event\"].apply(lambda x: 1 if x == \"Adoption\" else 0)\n)\n\n\nfig, ax = plt.subplots(figsize=(7, 3))\nkm_adoptions.plot(label=\"Kaplan-Meier Estimator\", ax=ax)\nax.set_ylabel(\"Probability of Adoption\")\nax.set_xlabel(\"Days\")\nax.set_xlim(0, 365)\nax.grid(True)\nax.set_title(\"Cat Adoption Survival Curve\");\n\n\n\n\n\n\n\n\nThe Kaplan-Meier estimator shows that by 100 days, the probability of a cat not being adopted is about \\(0.15\\) percent. After 100 days, the probability of cat not being adopted decreases, albeit at a much slower rate. Thus, if a cat hasn’t been adopted by the 100th day, it is more likely the cat will continue to wait for adoption. In the next section, we discuss pm.Censored, a PyMC distrbution that allows us to model censored data.\n\n\n\n\nThe censored distribution from PyMC allows us to make use of a sequential construction, similar to the Kaplan-Meier estimator outlined above, to model censored data. To understand the pm.Censored distribution, lets reason how a distribution may be used to model censored data. For observed adoptions, the probability of observed waiting time can be distributed according to an exponential with some rate \\(\\lambda\\) \\[D_i \\sim \\text{Exponential}(\\lambda_i)\\] or \\[f(D_i | \\lambda_i) = \\lambda_i \\text{exp}(-\\lambda_i D_i)\\] It’s the censored cats that are tricky. If something else happened before a cat could be adopted, or it simply hasn’t been adopted yet, then we need the probability of not being adopted, conditional on the observation time so far. One way to motivate this is to image a cohort of 100 cats, all joining the shelter on the same day. - If half have been adopted after 30 days, then the probability of waiting 30 days and still not being adopted is 0.5. - If after 60 days, only 25 remain, then the probability of waiting 60 days and not yet being adopted is 0.25.\nThus, any given rate of adoption implies a proportion of the cohort of 100 cats that will remain after any given number of days. This probability comes from the cumulative probability distribution. A cumulative distribution gives the proportion of cats adopted before or at a certain number of days. So \\(1 - \\text{CDF}\\), which is the CCDF, gives the probability a cat is not adopted by the same number of days. Remember from the Estimating the survival function section, this is equivalent to the survival function. If the exponential distribution is used, the CDF is\n\\[F(D_i | \\lambda_i) = 1 - \\text{exp}(-\\lambda_i D_i)\\]\nwhere the complement is (here we use \\(S\\) to denote the equivalence of the survival function and CCDF)\n\\[S(D_i|\\lambda) = \\text{exp}(-\\lambda_i D_i)\\]\nWhich is what we need in our model since it is the probability of waiting \\(D_i\\) days without being adopted yet. The pm.Censored from PyMC offers a convenient way to model censored data and the probability density function (PDF) is defined as\n\\[\\begin{cases}\n0 & \\text{for } x &lt; \\text{lower}, \\\\\n\\text{CDF}(\\text{lower}, \\text{dist}) & \\text{for } x = \\text{lower}, \\\\\n\\text{PDF}(x, \\text{dist}) & \\text{for } \\text{lower} &lt; x &lt; \\text{upper}, \\\\\n1 - \\text{CDF}(\\text{upper}, \\text{dist}) & \\text{for } x = \\text{upper}, \\\\\n0 & \\text{for } x &gt; \\text{upper}.\n\\end{cases}\\]\nwhere lower is left-censored and upper is right-censored. Our cat adoption dataset is right-censored. Therefore, lower can be None, and upper is the observed times when an event occurs. The pm.Censored uses the CCDF to answer the question we are interested in: what is the probability of not being adopted yet, given the observation time so far?\n\n\nTo understand how this is used, lets use Bambi to recover the parameters of the censored distribution with no predictors. Before the model is fit, days_to_event is scaled to represent months as the raw values contain very large values. This scaling ensures a smoother sampling process.\nAdditionally, modeling censored data in Bambi requires a new formula syntax censored(time, event) on the response term. censored indicates we want to model censored data and gets parsed where time and event are passed into a Bambi transformation function censored. This function takes two arguments: the first being the observed value \\(Y\\) (in this example time), and the second being the type of censoring of the event. In Bambi, it is possible to have left, none, right, and interval censoring. event needs to be encoded as one of the censoring types. In our cat adoption example, we will encode the adoption event as right.\nLastly, the exponential distribution is used to model the cat adoption rate parameter. But why not enter censored as the likelihood like we normally do in Bambi? The pm.Censored is indeed eventually used as the likelihood. However, there also needs to be a distribution that models the rate parameter. In this example it is the exponential distribution. This distribution is then used as input into the pm.Censored distribution. For more information on how to use the pm.Censored distribution, see the following PyMC documentation: Bayesian regression models with truncated and censored data and Censored data models.\n\ncats = cats_df.copy()\ncats[\"adopt\"] = np.where(cats[\"out_event\"] == \"Adoption\", \"right\", \"none\")\ncats[\"color_id\"] = np.where(cats[\"color\"] == \"Black\", 1, 0)\ncats = cats[[\"days_to_event\", \"adopt\", \"color_id\"]]\n\n\nmodel_1 = bmb.Model(\n    \"censored(days_to_event / 31, adopt) ~ 1\", \n    data=cats,\n    family=\"exponential\",\n    link=\"log\"\n)\nmodel_1.build()\nmodel_1.graph()\n\n\n\n\n\n\n\n\n\nidata_1 = model_1.fit(\n    tune=500,\n    draws=500,\n    random_seed=42, \n    chains=4, \n    cores=10\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 10 jobs)\nNUTS: [Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:05&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 5 seconds.\n\n\n\naz.plot_trace(idata_1);\n\n\n\n\n\n\n\n\n\nsummary = az.summary(idata_1)\nsummary\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/xarray/core/concat.py:546: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.501\n0.01\n0.483\n0.52\n0.0\n0.0\n801.0\n1099.0\n1.0\n\n\n\n\n\n\n\nInterpreting the intercept (the cat adoption rate parameter) alone is of not much value. Therefore, lets use the survival function to compute the probability of not being adopted after a range of months, given the learned rate parameter \\(\\lambda\\). We could dervive the survival function and pass the intercept parameter to it, but SciPy already implements it as scipy.stats.expon.sf, so we will just use this implementation.\n\n\n\nThe plot below shows the estimated survival function and CCDF for cat adoptions. First, we compute the \\(0.95\\) credible interval (CI) and median value for the intercept. Then, since a log-link was used, the values are exponentiated.\n\nlambda_preds = np.quantile(\n    idata_1[\"posterior\"][\"Intercept\"], \n    [0.025, 0.5, 0.975]\n)\n\nlambda_lower = 1 / np.exp(lambda_preds[0])\nlambda_median = 1 / np.exp(lambda_preds[1])\nlambda_upper = 1 / np.exp(lambda_preds[2])\n\n\nt = np.linspace(0, max(cats[\"days_to_event\"] / 31), 100)\nS0 = scipy.stats.expon.sf\ncdf = scipy.stats.expon.cdf\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3), sharey=True)\n\nax[0].plot(t, S0(lambda_median * t))\nax[0].fill_between(t, S0(lambda_lower * t), S0(lambda_upper * t), alpha=0.25)\nax[0].grid(True)\nax[0].set_xlim(0, 12)\nax[0].set_xlabel(\"Months\")\nax[0].set_ylabel(\"Probability\")\nax[0].set_title(\"Probability Not Being Adopted by Time $d_k$\")\n\nax[1].plot(t, cdf(lambda_median * t))\nax[1].fill_between(t, cdf(lambda_lower * t), cdf(lambda_upper * t), alpha=0.25)\nax[1].grid(True)\nax[1].vlines(1, 1, 1, linestyles=\"dashed\")\nax[1].set_xlabel(\"Months\")\nax[1].set_ylabel(\"Probability\")\nax[1].set_title(\"Probability of Being Adopted by Time $d_k$\");\n\n\n\n\n\n\n\n\nAnalyzing the CCDF (the left plot), the probability of a cat waiting one month without being adopted is about \\(0.60\\), whereas the probability of a cat being adopted by the first month is about \\(0.40\\). Analyzing the CDF (right plot), the majority of cats, about \\(0.97\\), are adopted by the sixth month. Now that we have an intuition on how pm.Censored is used for modeling censored data, in the next section, we will discuss how to model censored data with predictors.\n\n\n\n\nIt is often the case that we would like to understand how various predictors are associated with the survival function. For example, we may want to know if the survival function for cats of different colors or species is different. As outlined above, we cannot simply run a regression on the observed times \\(Y\\) given some predictors \\(X\\). What we are actually interested in is predicting the survival time \\(T\\) given the predictors \\(X\\). To achieve this, we first need to understand the Hazard function and the Cox proportional hazards model.\n\n\nIn survival analysis, it is often more convenient to express the survival function in terms of the hazards rate, which is closely related to the survival function \\(S(t)\\), and is the instantaneous rate of an event occuring at time \\(t\\) given that the event has not yet occured.\n\\[\\begin{split}\\begin{align*}\n\\lambda(t)\n    & = \\lim_{\\Delta t \\to 0} \\frac{P(t &lt; T &lt; t + \\Delta t\\ |\\ T &gt; t) / \\Delta t}{Pr(T &gt; t)} \\\\\n    & = \\lim_{{\\Delta t \\to 0}} \\frac{Pr(t &lt; T \\leq t + \\Delta t) / \\Delta t}{Pr(T &gt; t)} \\\\\n    & = \\frac{f(t)}{S(t)}\n\\end{align*}\\end{split}\\]\nwhere\n\\[f(t) = \\lim_{{\\Delta t \\to 0}} \\frac{Pr(t &lt; T \\leq t + \\Delta t)}{\\Delta t}\\]\nwhere \\(T\\) is the (unobserved) survival time and \\(f(t)\\) is the PDF associated with \\(T\\). The relationship between the hazard function and the survival function can be described in terms of the likelihood \\(L\\)\n\\[\\begin{equation}\nL_i =\n\\begin{cases}\nf(y_i) & \\text{if the } i\\text{th observation is not censored} \\\\\nS(y_i) & \\text{if the } i\\text{th observation is censored}\n\\end{cases}\n\\end{equation}\\]\nIf \\(Y = y_i\\) and the \\(i\\text{th}\\) observation is not censored, then the likelihood is the probability of the event in a tiny interval around time \\(y_i\\). If the \\(i\\text{th}\\) observation is censored, then the likelihood is the probability of surviving at least until time \\(y_i\\). We have now seen two ways to model the survival times: (1) a non-parametric estimator such as Kaplan-Meier, and (2) a parametric model using the PDF \\(f(t)\\) to estimate the hazard rate. However, what we would really like to do is to model the survival time as a function of the predictors. Thus, instead of working with the PDF \\(f(t)\\), we work directly with the hazard function to model the survival time as a function of predictors.\n\n\n\nAbove, we developed a model with no predictors to recover the parameters of the cat adoption rate, and used this as input into the pm.Censored distribution. Since we would now like to add predictor(s), we need to reformulate our modeling task into a risk regression model as it allows us to model the hazard rate as a function of our predictors. Specifically, the Cox proportional hazards model. With predictors \\(x_j\\) and regression coefficients \\(\\beta\\), the hazard rate is modeled as\n\\[\\lambda(t|x_i) = \\lambda_0(t)\\text{exp}(\\sum_{j=1}^p x_{ij}\\beta_j)\\]\nwhere \\(\\lambda_{0}t\\) is the baseline hazard rate independent of the predictors. This baseline hazard rate is unspecified (or unidentified) and means that we allow the instantaneous probability of an event at time \\(t\\), given that a subject has survived at least until time \\(t\\), to take any form. Practically speaking, this means that the hazard function is very flexible and can model a wide range of relationships between the covariates and survival time. One can interpret the Cox proportional hazards model as a one-unit increase in \\(x_{ij}\\) corresponds to an increase in \\(\\lambda(t, x_i)\\) by a factor of \\(\\text{exp}(\\beta_j)\\). In the next section, it is discussed how to develop a regression model with survival responses and predictors.\n\n\n\n\nAdding predictors to model the hazard rate as a function of our predictors is trivial in Bambi. We simply continue to use the formula syntax. In the backend, the rate is modeled as a function of the specified predictors in the Bambi model. For example, if in the Bambi model, we specified censored(y, event) ~ 1 + x with an exponential likelihood, then the latent rate \\(\\lambda\\) is modeled as an exponential distribution according to\n\\[\\alpha \\sim \\mathcal{N}(0, 1)\\] \\[\\beta \\sim \\mathcal{N}(0, 1)\\] \\[\\mu = \\text{exp}(\\alpha + \\beta X)\\] \\[\\lambda = 1 / \\mu\\] \\[Y \\sim \\text{Exponential}(\\lambda)\\]\nwhere \\(Y\\) is then passed to the dist argument of the pm.Censored distribution.\n\n\nHowever, thanks to Bambi’s formula syntax, we can just include the predictors of interest. Below, color_id is added to model the survival probability of black and other colored cats.\n\ncat_model = bmb.Model(\n    \"censored(days_to_event / 31, adopt) ~ 0 + color_id\", \n    data=cats,\n    center_predictors=False,\n    priors={\"color_id\": bmb.Prior(\"Normal\", mu=0, sigma=1)},\n    categorical=[\"color_id\"],\n    family=\"exponential\",\n    link=\"log\"\n)\ncat_model.build()\ncat_model.graph()\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/formulae/terms/variable.py:87: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  elif is_string_dtype(x) or is_categorical_dtype(x):\n\n\n\n\n\n\n\n\n\n\ncat_model\n\n       Formula: censored(days_to_event / 31, adopt) ~ 0 + color_id\n        Family: exponential\n          Link: mu = log\n  Observations: 22356\n        Priors: \n    target = mu\n        Common-level effects\n            color_id ~ Normal(mu: 0.0, sigma: 1.0)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\n\nidata = cat_model.fit(\n    tune=500,\n    draws=500,\n    random_seed=42, \n    chains=4, \n    cores=10\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 10 jobs)\nNUTS: [color_id]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 06:07&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 368 seconds.\n\n\n\naz.summary(idata)\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/xarray/core/concat.py:546: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ncolor_id[0]\n0.490\n0.011\n0.471\n0.511\n0.000\n0.0\n2072.0\n1445.0\n1.0\n\n\ncolor_id[1]\n0.568\n0.026\n0.521\n0.618\n0.001\n0.0\n2098.0\n1652.0\n1.0\n\n\n\n\n\n\n\nThe summary output informs us that, on average, the rate parameter for other cats color_id[0] is lower than the rate for black cats color_id[1]. As performed above, lets plot the survival curves and CDFs for black and other colored cats to get a better understanding of the rate parameters.\n\n\n\nIn the inference data, we have posterior draws for color_id (with corresponding coordinates for other and black cats) where the values represent the sampled rates. However, it is also possible to obtain \\(Y\\), in this example, months to event (as our data has been scaled) by calling model.predict() on the observed or new data. This will add a new data variable censored(y, event) to the posterior group of the inference data.\n\nnew_data = pd.DataFrame({\"color_id\": [0, 1]})\ncat_model.predict(idata, data=new_data, kind=\"mean\")\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/xarray/core/concat.py:546: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n\n\n\nother_cats = (idata[\"posterior\"][\"censored(days_to_event / 31, adopt)_mean\"]\n              .sel({\"censored(days_to_event / 31, adopt)_obs\": 0})\n              .values\n              .flatten()\n              )\nother_cats_preds = np.quantile(other_cats, [0.025, 0.5, 0.975])\n\nblack_cats = (idata[\"posterior\"][\"censored(days_to_event / 31, adopt)_mean\"]\n                    .sel({\"censored(days_to_event / 31, adopt)_obs\": 1})\n                    .values.\n                    flatten()\n                    )\nblack_cats_preds = np.quantile(black_cats, [0.025, 0.5, 0.975])\n\nlambdas = {\n        \"Other cats\": 1 / other_cats_preds,\n        \"Black cats\": 1 / black_cats_preds\n        }\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3), sharey=True)\n\nfor key, value in lambdas.items():\n    lower, median, upper = value\n    ax[0].plot(t, S0(median * t), label=f\"{key}\")\n    ax[0].fill_between(t, S0(lower * t), S0(upper * t), alpha=0.25)\n\nax[0].grid(True)\nax[0].set_xlim(0, 10)\nax[0].legend()\nax[0].set_title(\"Probability Not Being Adopted by Time $d_k$\")\n\nfor key, value in lambdas.items():\n    lower, median, upper = value\n    ax[1].plot(t, cdf(median * t), label=f\"{key}\")\n    ax[1].fill_between(t, cdf(lower * t), cdf(upper * t), alpha=0.25)\n\nax[1].grid(True)\nax[1].set_xlim(0, 10)\nax[1].legend()\nax[1].set_title(\"Probability of Being Adopted by Time $d_k$\");\n\n\n\n\n\n\n\n\nAnalyzing the CCDF (left plot), we can see that black cats have a slightly higher probability of not being adopted throughout the whole range of \\(k\\). Furthermore, analyzing the CDF (right plot), we can see it also takes a longer time for the majority of black cats to be adopted compared to other colored cats. Below, we plot the distribution of days until adoption for the two groups.\n\nplt.figure(figsize=(7, 3))\nplt.hist(\n    other_cats * 31, \n    bins=50, \n    density=True,\n    label=\"Other cats\"\n)\nplt.hist(\n    black_cats * 31, \n    bins=50,\n    density=True,\n    label=\"Black cats\"\n)\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Density\")\nplt.title(\"Distribution of Adoption Times\");\n\n\n\n\n\n\n\n\nScaling adoption times back to days (multiplying by 31), we can see that black cats have longer and a wider range of time until adoptions (about 55 days) than cats that are not black (about 51 days).\n\n\n\n\nIn this notebook, we introduced censored data, left and right censoring, and why such data lends itself to specialized statistical methods and models. First, the non-parametric Kaplan-Meier estimator to estimate the survival curve of censored data was introduced. Subsequently, motivated by modeling the survival function as a function of predictors, the hazards rate and Cox proportional hazards model was introduced. Modeling censored data in Bambi requires defining the response as censored(y, event) where event is left or right censoring. To add predictors to the model, simply include them in the formula. Bambi leverages the pm.Censored distribution from PyMC as the likelihood for censored data.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nLast updated: Mon Oct 23 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\npymc      : 5.8.1\narviz     : 0.16.1\npandas    : 2.1.0\nbambi     : 0.13.0.dev0\nmatplotlib: 3.7.1\nnumpy     : 1.24.2\nscipy     : 1.11.2\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#survival-and-censoring-times",
    "href": "posts/2023-10-25-survival-models-bambi.html#survival-and-censoring-times",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "Sometimes the right way to model discrete, countable events is to model not the counts themselves but rather the time between events. This gives us information regarding the rate of an event. Survival models are models for countable things, but the outcomes we want to predict are durations. Durations are continuous deviations from some point of reference (so they are all positive values).\nThe tricky part with survival models is not the probability distribution assigned to the durations, but dealing with censoring. Censoring occurs when the event of interest does not occur in the window of observation. In a simple scenario, this can happen because the observation period ends before the event occurred. Censored individuals (or units) can not just be dropped from the sample. As an example, we use Richard McElreath’s cat adoption example from chapter 11.4 of Statistical Rethinking: Imagine a cohort of 100 cats who start waiting for adoption at the same time. After one month, half of them have been adopted. Now what is the rate of adoption? You can’t compute it using only the cats who have been adopted. You need to also account for the cats who haven’t yet been adopted. The cats who haven’t been adopted yet, but eventually will be adopted, clearly have longer waiting times than the cats who have already been adopted. So the average rate among those who are already adopted is biased upwards—it is confounded by conditioning on adoption.\nIncluding censored observations requires a new type of model. The key idea is that the same distribution assumption for the outcome tells us both the probability of any observed duration that end in the event as well as the probability that we would wait the observed duration without seeing the event. For each unit, we assume there is a true survival time \\(T\\) as well as a true censoring time \\(C\\). The survival time represents the time at which the event of interest occurs. The censoring time is the time at which censoring occurs. We observe either: the survival, or the censoring time:\n\\[Y = \\text{min}(T, C)\\]\nIf the event occurs, then we observe the survival time, else we observe the censoring time. In order to analyze survival data, we first need to understand the two types of censoring: left and right censoring, and how to estimate the survival function."
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#left-and-right-censoring",
    "href": "posts/2023-10-25-survival-models-bambi.html#left-and-right-censoring",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "There are two main “types” of censoring: right and left. Right censoring occurs when \\(T \\ge Y\\), i.e. the true event time \\(T\\) is at least as large as the observed time \\(Y\\). This is a consequence of \\(Y = \\text{min}(T, C)\\). Right censoring derives its name from the notion that time is typically read and displayed from left to right. Left sensoring occurs when the true event time \\(T\\) is less than or equal to the observed time \\(Y\\). An example of left censoring could be in a study of pregnancy duration. Suppose that patients are surveyed 250 days (8.2 months) after conception. Some patients may have already had their babies. For these patients, pregnancy duration is less than 250 days."
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#estimating-the-survival-function",
    "href": "posts/2023-10-25-survival-models-bambi.html#estimating-the-survival-function",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "Survival analysis is concerned with estimating the survival function \\(S(t)\\)\n\\[S(t) = Pr(T &gt; t) = 1 - F(t)\\]\nwhich is a decreasing function that quantifies the probability of surviving past time \\(t\\). Alternatively, \\(S(t)\\) can be expressed as one minus the cumulative distribution function (CDF) \\(F\\) of the event time \\(T\\)—referred to as the complementary cumulative distribution function (CCDF). The focus on the survival function is important because for censored observations, we only know that the time-to-event exceeds the observed time \\(Y\\).\nHere, continuing with the cat adoption example, we consider the task of estimating the survival function for cat adoptions. To estimate \\(S(30) = Pr(T &gt; 30)\\), the probability that a cat is not adopted after 30 days, it is tempting to compute the proportion of cats who were adopted before 30 days and subtract this from 1. However, this would be incorrect because it ignores the cats who were not adopted before 30 days but who will be adopted later—these cats clearly have longer adoption rates. Thus, if we continued with the naive approach, the average rate of adoption would be biased upwards—it is confounded by conditioning on adoption.\nHowever, it is possible to overcome this challenge by using the Kaplan-Meier estimator. The Kaplan-Meier estimator is a non-parametric estimator of the survival function that accounts for censoring. Let \\(d_1 &lt; d_2 &lt; . . . &lt; d_K\\) denote the \\(K\\) unique adoption times among the non-censored cats, and \\(q_k\\) denote the number of cats adopted at time \\(d_k\\). For \\(k = 1,...,K\\), let \\(r_k\\) denote the number of cats not adopted at time \\(d_k\\). By the law of total probability\n\\[Pr(T &gt; d_k) = Pr(T &gt; d_k | T &gt; d_{k-1}) Pr(T &gt; d_{k-1}) + Pr(T &gt; d_k | T \\leq d_{k-1}) Pr(T \\leq d_{k-1})\\]\nThe fact that \\(d_{k-1} &lt; d_k\\) implies that \\(Pr(T &gt; d_k | T \\leq d_{k-1}) = 0\\) (as it is impossible for a cat to be adopted past time \\(d_k\\) if the cat was adopted before time \\(d_{k-1}\\)). Thus, if we simplify the above equation and plug into the survival function, we obtain\n\\[S(d_k) = Pr(T &gt; d_k | T &gt; d_{k-1})S(d_{k-1})\\]\nNow we must estimate the terms on the right-hand side. It is common to use the following estimator\n\\[\\hat{Pr}(T &gt; d_j | T &gt; d_{j-1}) = \\frac{r_j - q_j}{r_j}\\]\nwhich leads us to the Kaplan-Meier estimator of the survival function\n\\[\\hat{S}(d_k) = \\prod_{j=1}^k \\frac{r_j - q_j}{r_j}\\]\nwhere \\(\\hat{S}(d_k)\\) represents the estimated survival probability up to time \\(d_k\\). The product is taken over all time points up to \\(k\\), where an event occurred. The variables \\(r_j\\) and \\(q_j\\) denote the number of subjects at risk and the number of events at time \\(d_j\\), respectively. The term \\(\\frac{r_j - q_j}{r_j}\\) is the conditional probability of surviving the \\(j\\)-th time point given that an individual has survived just before \\(d_j\\). Specifically, \\(r_j - q_j\\) are the number of individuals who survived just before \\(d_j\\) and \\(r_j\\) is the number of individuals who survived just after \\(d_j\\), and \\(r_j\\) are those who were at risk \\(d_j\\).\n\n\nBelow we use the KaplanMeierFitter class of the lifelines package to compute and visualize the survival curve for cat adoptions from an animal shelter in Austin, Texas beginning October 1st, 2013 until May 30th, 2018 (the last day the shelter rescued a cat). The dataset comes from the City of Austin Open Data Portal and contains columns such as animal name, date of birth, species, and many more. However, for the purpose of this notebook we are interested in the following columns: - days_to_event - number of days until the cat was adopted (date_in - date_out) - out_event - the reason for the cat leaving this particular shelter, e.g. adopted or transfered. - color - the color of the cat, e.g. white, blue, brown tabby, black.\n\n\nCode\nimport arviz as az\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport scipy\n\nfrom lifelines import KaplanMeierFitter\n\nimport bambi as bmb\n\n\n\nurl = \"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/AustinCats.csv\"\ncats_df = pd.read_csv(url, sep=\";\")\n\n\nplt.figure(figsize=(7, 3))\nplt.hist(cats_df[\"days_to_event\"], bins=250, label=\"Uncensored data\")\nplt.xlim(0, 186) # limit to 6 months for visibility\nplt.title(\"Days Until Adoption\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Days\")\nplt.legend();\n\n\n\n\n\n\n\n\nThe distribution of days until adoption exhibits a long tail with most cats (if we observe the adopt event) being adopted within the first month of inception. Note that the plot has been truncated to six months for better visibility. Below, we estimate the survival function using the KaplanMeierFitter class from the lifelines package.\n\nkm = KaplanMeierFitter()\nkm_adoptions = km.fit(\n    cats_df[\"days_to_event\"], \n    cats_df[\"out_event\"].apply(lambda x: 1 if x == \"Adoption\" else 0)\n)\n\n\nfig, ax = plt.subplots(figsize=(7, 3))\nkm_adoptions.plot(label=\"Kaplan-Meier Estimator\", ax=ax)\nax.set_ylabel(\"Probability of Adoption\")\nax.set_xlabel(\"Days\")\nax.set_xlim(0, 365)\nax.grid(True)\nax.set_title(\"Cat Adoption Survival Curve\");\n\n\n\n\n\n\n\n\nThe Kaplan-Meier estimator shows that by 100 days, the probability of a cat not being adopted is about \\(0.15\\) percent. After 100 days, the probability of cat not being adopted decreases, albeit at a much slower rate. Thus, if a cat hasn’t been adopted by the 100th day, it is more likely the cat will continue to wait for adoption. In the next section, we discuss pm.Censored, a PyMC distrbution that allows us to model censored data."
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#the-pm.censored-distribution",
    "href": "posts/2023-10-25-survival-models-bambi.html#the-pm.censored-distribution",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "The censored distribution from PyMC allows us to make use of a sequential construction, similar to the Kaplan-Meier estimator outlined above, to model censored data. To understand the pm.Censored distribution, lets reason how a distribution may be used to model censored data. For observed adoptions, the probability of observed waiting time can be distributed according to an exponential with some rate \\(\\lambda\\) \\[D_i \\sim \\text{Exponential}(\\lambda_i)\\] or \\[f(D_i | \\lambda_i) = \\lambda_i \\text{exp}(-\\lambda_i D_i)\\] It’s the censored cats that are tricky. If something else happened before a cat could be adopted, or it simply hasn’t been adopted yet, then we need the probability of not being adopted, conditional on the observation time so far. One way to motivate this is to image a cohort of 100 cats, all joining the shelter on the same day. - If half have been adopted after 30 days, then the probability of waiting 30 days and still not being adopted is 0.5. - If after 60 days, only 25 remain, then the probability of waiting 60 days and not yet being adopted is 0.25.\nThus, any given rate of adoption implies a proportion of the cohort of 100 cats that will remain after any given number of days. This probability comes from the cumulative probability distribution. A cumulative distribution gives the proportion of cats adopted before or at a certain number of days. So \\(1 - \\text{CDF}\\), which is the CCDF, gives the probability a cat is not adopted by the same number of days. Remember from the Estimating the survival function section, this is equivalent to the survival function. If the exponential distribution is used, the CDF is\n\\[F(D_i | \\lambda_i) = 1 - \\text{exp}(-\\lambda_i D_i)\\]\nwhere the complement is (here we use \\(S\\) to denote the equivalence of the survival function and CCDF)\n\\[S(D_i|\\lambda) = \\text{exp}(-\\lambda_i D_i)\\]\nWhich is what we need in our model since it is the probability of waiting \\(D_i\\) days without being adopted yet. The pm.Censored from PyMC offers a convenient way to model censored data and the probability density function (PDF) is defined as\n\\[\\begin{cases}\n0 & \\text{for } x &lt; \\text{lower}, \\\\\n\\text{CDF}(\\text{lower}, \\text{dist}) & \\text{for } x = \\text{lower}, \\\\\n\\text{PDF}(x, \\text{dist}) & \\text{for } \\text{lower} &lt; x &lt; \\text{upper}, \\\\\n1 - \\text{CDF}(\\text{upper}, \\text{dist}) & \\text{for } x = \\text{upper}, \\\\\n0 & \\text{for } x &gt; \\text{upper}.\n\\end{cases}\\]\nwhere lower is left-censored and upper is right-censored. Our cat adoption dataset is right-censored. Therefore, lower can be None, and upper is the observed times when an event occurs. The pm.Censored uses the CCDF to answer the question we are interested in: what is the probability of not being adopted yet, given the observation time so far?\n\n\nTo understand how this is used, lets use Bambi to recover the parameters of the censored distribution with no predictors. Before the model is fit, days_to_event is scaled to represent months as the raw values contain very large values. This scaling ensures a smoother sampling process.\nAdditionally, modeling censored data in Bambi requires a new formula syntax censored(time, event) on the response term. censored indicates we want to model censored data and gets parsed where time and event are passed into a Bambi transformation function censored. This function takes two arguments: the first being the observed value \\(Y\\) (in this example time), and the second being the type of censoring of the event. In Bambi, it is possible to have left, none, right, and interval censoring. event needs to be encoded as one of the censoring types. In our cat adoption example, we will encode the adoption event as right.\nLastly, the exponential distribution is used to model the cat adoption rate parameter. But why not enter censored as the likelihood like we normally do in Bambi? The pm.Censored is indeed eventually used as the likelihood. However, there also needs to be a distribution that models the rate parameter. In this example it is the exponential distribution. This distribution is then used as input into the pm.Censored distribution. For more information on how to use the pm.Censored distribution, see the following PyMC documentation: Bayesian regression models with truncated and censored data and Censored data models.\n\ncats = cats_df.copy()\ncats[\"adopt\"] = np.where(cats[\"out_event\"] == \"Adoption\", \"right\", \"none\")\ncats[\"color_id\"] = np.where(cats[\"color\"] == \"Black\", 1, 0)\ncats = cats[[\"days_to_event\", \"adopt\", \"color_id\"]]\n\n\nmodel_1 = bmb.Model(\n    \"censored(days_to_event / 31, adopt) ~ 1\", \n    data=cats,\n    family=\"exponential\",\n    link=\"log\"\n)\nmodel_1.build()\nmodel_1.graph()\n\n\n\n\n\n\n\n\n\nidata_1 = model_1.fit(\n    tune=500,\n    draws=500,\n    random_seed=42, \n    chains=4, \n    cores=10\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 10 jobs)\nNUTS: [Intercept]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 00:05&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 5 seconds.\n\n\n\naz.plot_trace(idata_1);\n\n\n\n\n\n\n\n\n\nsummary = az.summary(idata_1)\nsummary\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/xarray/core/concat.py:546: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.501\n0.01\n0.483\n0.52\n0.0\n0.0\n801.0\n1099.0\n1.0\n\n\n\n\n\n\n\nInterpreting the intercept (the cat adoption rate parameter) alone is of not much value. Therefore, lets use the survival function to compute the probability of not being adopted after a range of months, given the learned rate parameter \\(\\lambda\\). We could dervive the survival function and pass the intercept parameter to it, but SciPy already implements it as scipy.stats.expon.sf, so we will just use this implementation.\n\n\n\nThe plot below shows the estimated survival function and CCDF for cat adoptions. First, we compute the \\(0.95\\) credible interval (CI) and median value for the intercept. Then, since a log-link was used, the values are exponentiated.\n\nlambda_preds = np.quantile(\n    idata_1[\"posterior\"][\"Intercept\"], \n    [0.025, 0.5, 0.975]\n)\n\nlambda_lower = 1 / np.exp(lambda_preds[0])\nlambda_median = 1 / np.exp(lambda_preds[1])\nlambda_upper = 1 / np.exp(lambda_preds[2])\n\n\nt = np.linspace(0, max(cats[\"days_to_event\"] / 31), 100)\nS0 = scipy.stats.expon.sf\ncdf = scipy.stats.expon.cdf\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3), sharey=True)\n\nax[0].plot(t, S0(lambda_median * t))\nax[0].fill_between(t, S0(lambda_lower * t), S0(lambda_upper * t), alpha=0.25)\nax[0].grid(True)\nax[0].set_xlim(0, 12)\nax[0].set_xlabel(\"Months\")\nax[0].set_ylabel(\"Probability\")\nax[0].set_title(\"Probability Not Being Adopted by Time $d_k$\")\n\nax[1].plot(t, cdf(lambda_median * t))\nax[1].fill_between(t, cdf(lambda_lower * t), cdf(lambda_upper * t), alpha=0.25)\nax[1].grid(True)\nax[1].vlines(1, 1, 1, linestyles=\"dashed\")\nax[1].set_xlabel(\"Months\")\nax[1].set_ylabel(\"Probability\")\nax[1].set_title(\"Probability of Being Adopted by Time $d_k$\");\n\n\n\n\n\n\n\n\nAnalyzing the CCDF (the left plot), the probability of a cat waiting one month without being adopted is about \\(0.60\\), whereas the probability of a cat being adopted by the first month is about \\(0.40\\). Analyzing the CDF (right plot), the majority of cats, about \\(0.97\\), are adopted by the sixth month. Now that we have an intuition on how pm.Censored is used for modeling censored data, in the next section, we will discuss how to model censored data with predictors."
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#regression-models-with-a-survival-response",
    "href": "posts/2023-10-25-survival-models-bambi.html#regression-models-with-a-survival-response",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "It is often the case that we would like to understand how various predictors are associated with the survival function. For example, we may want to know if the survival function for cats of different colors or species is different. As outlined above, we cannot simply run a regression on the observed times \\(Y\\) given some predictors \\(X\\). What we are actually interested in is predicting the survival time \\(T\\) given the predictors \\(X\\). To achieve this, we first need to understand the Hazard function and the Cox proportional hazards model.\n\n\nIn survival analysis, it is often more convenient to express the survival function in terms of the hazards rate, which is closely related to the survival function \\(S(t)\\), and is the instantaneous rate of an event occuring at time \\(t\\) given that the event has not yet occured.\n\\[\\begin{split}\\begin{align*}\n\\lambda(t)\n    & = \\lim_{\\Delta t \\to 0} \\frac{P(t &lt; T &lt; t + \\Delta t\\ |\\ T &gt; t) / \\Delta t}{Pr(T &gt; t)} \\\\\n    & = \\lim_{{\\Delta t \\to 0}} \\frac{Pr(t &lt; T \\leq t + \\Delta t) / \\Delta t}{Pr(T &gt; t)} \\\\\n    & = \\frac{f(t)}{S(t)}\n\\end{align*}\\end{split}\\]\nwhere\n\\[f(t) = \\lim_{{\\Delta t \\to 0}} \\frac{Pr(t &lt; T \\leq t + \\Delta t)}{\\Delta t}\\]\nwhere \\(T\\) is the (unobserved) survival time and \\(f(t)\\) is the PDF associated with \\(T\\). The relationship between the hazard function and the survival function can be described in terms of the likelihood \\(L\\)\n\\[\\begin{equation}\nL_i =\n\\begin{cases}\nf(y_i) & \\text{if the } i\\text{th observation is not censored} \\\\\nS(y_i) & \\text{if the } i\\text{th observation is censored}\n\\end{cases}\n\\end{equation}\\]\nIf \\(Y = y_i\\) and the \\(i\\text{th}\\) observation is not censored, then the likelihood is the probability of the event in a tiny interval around time \\(y_i\\). If the \\(i\\text{th}\\) observation is censored, then the likelihood is the probability of surviving at least until time \\(y_i\\). We have now seen two ways to model the survival times: (1) a non-parametric estimator such as Kaplan-Meier, and (2) a parametric model using the PDF \\(f(t)\\) to estimate the hazard rate. However, what we would really like to do is to model the survival time as a function of the predictors. Thus, instead of working with the PDF \\(f(t)\\), we work directly with the hazard function to model the survival time as a function of predictors.\n\n\n\nAbove, we developed a model with no predictors to recover the parameters of the cat adoption rate, and used this as input into the pm.Censored distribution. Since we would now like to add predictor(s), we need to reformulate our modeling task into a risk regression model as it allows us to model the hazard rate as a function of our predictors. Specifically, the Cox proportional hazards model. With predictors \\(x_j\\) and regression coefficients \\(\\beta\\), the hazard rate is modeled as\n\\[\\lambda(t|x_i) = \\lambda_0(t)\\text{exp}(\\sum_{j=1}^p x_{ij}\\beta_j)\\]\nwhere \\(\\lambda_{0}t\\) is the baseline hazard rate independent of the predictors. This baseline hazard rate is unspecified (or unidentified) and means that we allow the instantaneous probability of an event at time \\(t\\), given that a subject has survived at least until time \\(t\\), to take any form. Practically speaking, this means that the hazard function is very flexible and can model a wide range of relationships between the covariates and survival time. One can interpret the Cox proportional hazards model as a one-unit increase in \\(x_{ij}\\) corresponds to an increase in \\(\\lambda(t, x_i)\\) by a factor of \\(\\text{exp}(\\beta_j)\\). In the next section, it is discussed how to develop a regression model with survival responses and predictors."
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#implementation-in-bambi-1",
    "href": "posts/2023-10-25-survival-models-bambi.html#implementation-in-bambi-1",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "Adding predictors to model the hazard rate as a function of our predictors is trivial in Bambi. We simply continue to use the formula syntax. In the backend, the rate is modeled as a function of the specified predictors in the Bambi model. For example, if in the Bambi model, we specified censored(y, event) ~ 1 + x with an exponential likelihood, then the latent rate \\(\\lambda\\) is modeled as an exponential distribution according to\n\\[\\alpha \\sim \\mathcal{N}(0, 1)\\] \\[\\beta \\sim \\mathcal{N}(0, 1)\\] \\[\\mu = \\text{exp}(\\alpha + \\beta X)\\] \\[\\lambda = 1 / \\mu\\] \\[Y \\sim \\text{Exponential}(\\lambda)\\]\nwhere \\(Y\\) is then passed to the dist argument of the pm.Censored distribution.\n\n\nHowever, thanks to Bambi’s formula syntax, we can just include the predictors of interest. Below, color_id is added to model the survival probability of black and other colored cats.\n\ncat_model = bmb.Model(\n    \"censored(days_to_event / 31, adopt) ~ 0 + color_id\", \n    data=cats,\n    center_predictors=False,\n    priors={\"color_id\": bmb.Prior(\"Normal\", mu=0, sigma=1)},\n    categorical=[\"color_id\"],\n    family=\"exponential\",\n    link=\"log\"\n)\ncat_model.build()\ncat_model.graph()\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/formulae/terms/variable.py:87: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n  elif is_string_dtype(x) or is_categorical_dtype(x):\n\n\n\n\n\n\n\n\n\n\ncat_model\n\n       Formula: censored(days_to_event / 31, adopt) ~ 0 + color_id\n        Family: exponential\n          Link: mu = log\n  Observations: 22356\n        Priors: \n    target = mu\n        Common-level effects\n            color_id ~ Normal(mu: 0.0, sigma: 1.0)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\n\nidata = cat_model.fit(\n    tune=500,\n    draws=500,\n    random_seed=42, \n    chains=4, \n    cores=10\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 10 jobs)\nNUTS: [color_id]\n\n\n\n\n\n\n\n    \n      \n      100.00% [4000/4000 06:07&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 368 seconds.\n\n\n\naz.summary(idata)\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/xarray/core/concat.py:546: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\ncolor_id[0]\n0.490\n0.011\n0.471\n0.511\n0.000\n0.0\n2072.0\n1445.0\n1.0\n\n\ncolor_id[1]\n0.568\n0.026\n0.521\n0.618\n0.001\n0.0\n2098.0\n1652.0\n1.0\n\n\n\n\n\n\n\nThe summary output informs us that, on average, the rate parameter for other cats color_id[0] is lower than the rate for black cats color_id[1]. As performed above, lets plot the survival curves and CDFs for black and other colored cats to get a better understanding of the rate parameters.\n\n\n\nIn the inference data, we have posterior draws for color_id (with corresponding coordinates for other and black cats) where the values represent the sampled rates. However, it is also possible to obtain \\(Y\\), in this example, months to event (as our data has been scaled) by calling model.predict() on the observed or new data. This will add a new data variable censored(y, event) to the posterior group of the inference data.\n\nnew_data = pd.DataFrame({\"color_id\": [0, 1]})\ncat_model.predict(idata, data=new_data, kind=\"mean\")\n\n/Users/gabestechschulte/miniforge3/envs/bambinos/lib/python3.11/site-packages/xarray/core/concat.py:546: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  common_dims = tuple(pd.unique([d for v in vars for d in v.dims]))\n\n\n\nother_cats = (idata[\"posterior\"][\"censored(days_to_event / 31, adopt)_mean\"]\n              .sel({\"censored(days_to_event / 31, adopt)_obs\": 0})\n              .values\n              .flatten()\n              )\nother_cats_preds = np.quantile(other_cats, [0.025, 0.5, 0.975])\n\nblack_cats = (idata[\"posterior\"][\"censored(days_to_event / 31, adopt)_mean\"]\n                    .sel({\"censored(days_to_event / 31, adopt)_obs\": 1})\n                    .values.\n                    flatten()\n                    )\nblack_cats_preds = np.quantile(black_cats, [0.025, 0.5, 0.975])\n\nlambdas = {\n        \"Other cats\": 1 / other_cats_preds,\n        \"Black cats\": 1 / black_cats_preds\n        }\n\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3), sharey=True)\n\nfor key, value in lambdas.items():\n    lower, median, upper = value\n    ax[0].plot(t, S0(median * t), label=f\"{key}\")\n    ax[0].fill_between(t, S0(lower * t), S0(upper * t), alpha=0.25)\n\nax[0].grid(True)\nax[0].set_xlim(0, 10)\nax[0].legend()\nax[0].set_title(\"Probability Not Being Adopted by Time $d_k$\")\n\nfor key, value in lambdas.items():\n    lower, median, upper = value\n    ax[1].plot(t, cdf(median * t), label=f\"{key}\")\n    ax[1].fill_between(t, cdf(lower * t), cdf(upper * t), alpha=0.25)\n\nax[1].grid(True)\nax[1].set_xlim(0, 10)\nax[1].legend()\nax[1].set_title(\"Probability of Being Adopted by Time $d_k$\");\n\n\n\n\n\n\n\n\nAnalyzing the CCDF (left plot), we can see that black cats have a slightly higher probability of not being adopted throughout the whole range of \\(k\\). Furthermore, analyzing the CDF (right plot), we can see it also takes a longer time for the majority of black cats to be adopted compared to other colored cats. Below, we plot the distribution of days until adoption for the two groups.\n\nplt.figure(figsize=(7, 3))\nplt.hist(\n    other_cats * 31, \n    bins=50, \n    density=True,\n    label=\"Other cats\"\n)\nplt.hist(\n    black_cats * 31, \n    bins=50,\n    density=True,\n    label=\"Black cats\"\n)\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Density\")\nplt.title(\"Distribution of Adoption Times\");\n\n\n\n\n\n\n\n\nScaling adoption times back to days (multiplying by 31), we can see that black cats have longer and a wider range of time until adoptions (about 55 days) than cats that are not black (about 51 days)."
  },
  {
    "objectID": "posts/2023-10-25-survival-models-bambi.html#summary",
    "href": "posts/2023-10-25-survival-models-bambi.html#summary",
    "title": "Survival Models in Bambi",
    "section": "",
    "text": "In this notebook, we introduced censored data, left and right censoring, and why such data lends itself to specialized statistical methods and models. First, the non-parametric Kaplan-Meier estimator to estimate the survival curve of censored data was introduced. Subsequently, motivated by modeling the survival function as a function of predictors, the hazards rate and Cox proportional hazards model was introduced. Modeling censored data in Bambi requires defining the response as censored(y, event) where event is left or right censoring. To add predictors to the model, simply include them in the formula. Bambi leverages the pm.Censored distribution from PyMC as the likelihood for censored data.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nThe watermark extension is already loaded. To reload it, use:\n  %reload_ext watermark\nLast updated: Mon Oct 23 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\npymc      : 5.8.1\narviz     : 0.16.1\npandas    : 2.1.0\nbambi     : 0.13.0.dev0\nmatplotlib: 3.7.1\nnumpy     : 1.24.2\nscipy     : 1.11.2\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2024-08-19-dbs-file-formats.html",
    "href": "posts/2024-08-19-dbs-file-formats.html",
    "title": "Data formats and encoding",
    "section": "",
    "text": "As the business landscape embraces data-driven approaches for analysis and decision-making, there is a rapid surge in the volume of data requiring storage and processing. This surge has led to the growing popularity of OLAP database systems.\nAn OLAP system workload is characterized by long running, complex queries and reads on large portions of the database. In OLAP workloads, the database system is often analyzing and deriving new data from existing data collected on the OLTP side. An OLAP workload is in contrast to OLTP systems, where the workload is characterized by short running repetitive operations that operate on a single entity at a time.\nThis blog aims to provide an overview of the popular data storage representations and encoding schemes within OLTP and OLAP database systems. First, OLTP data storage will be discussed followed by OLAP systems."
  },
  {
    "objectID": "posts/2024-08-19-dbs-file-formats.html#oltp",
    "href": "posts/2024-08-19-dbs-file-formats.html#oltp",
    "title": "Data formats and encoding",
    "section": "OLTP",
    "text": "OLTP\n\nFile storage\nA DBMS stores a DB as files on disk (as long as the DB is not an in-memory DB). The DBMS storage manager is responsible for managing a DB’s files, e.g. keeping track of what has been read and written to pages as well as much free space is in these pages. It represents the files as a collection of pages.\n\n\nPages\nThe DBMS organizes the DB across one or more files in fixed-size blocks called pages. Pages can contain different kinds of data such as tuples, indexes, and log records. But most systems will not mix these types within pages. Additionally, some systems require that pages are self-contained, meaning that all the information needed to read each page is on the page itself.\nEach page is given a unique identifier. Most DBMSs have an indirection layer that maps a page id to a file path and offset. The upper levels of the system will ask for a specific page number. Then, the storage manager will have to turn that page number into a file and an offset to find the page.\nMost DBMSs use fixed-size pages to avoid the engineering overhead needed to support variable-sized pages. There are three concepts of pages within a DBMS: 1. Hardware page (usually 4KB) 2. OS page (4KB) 3. Database page (1-16KB)\n\nPage storage architecture\nDifferent DBMSs manage pages in files on disk in different ways. A few of the common organizations are: - Heap file organization - Tree file organization - Sequential/sorted organization (ISAM) - Hash organization ##### Heap files\nA heap file is an unordered collection of pages where tuples are stored in random order. - Create/get/write/delete page - Must also support iterating over all pages\nTODO: Diagram here…\nIt is easy to use if there is only one file. Need meta-data to keep track of what pages exist in multiple files and which ones have free space. The DBMS can locate a page on disk given a page id by using a linked list of pages or a page directory. - Linked list: Header page holds pointers to a list of free pages and a list of data pages. However, if the DBMS is looking for a specific page, it has to do a sequential scan on the data page list until it finds the page it is looking for. - Page directory: The DBMS maintains special pages that tracks the location of data pages in the database files.\n\n\nPage layout\nEvery page contains a header of metadata about the page’s: - Page size - Checksum - DBMS version - Transaction visibility\nThere are two main approaches to laying out data in pages: (1) slotted-pages, and (2) log-structured.\n\n\nTuple oriented (slotted pages)\nThe most common layout is called slotted pages.\nSlotted pages: Page maps slots to offsets - Most common approach in row-oriented DBMSs today - Header keeps track of the number of used slots, the offset of the starting location of the last used slot, and a slot array, which keeps track of the location of the start of each tuple - To add a tuple, the slot array will grow from the beginning to the end, and the data of the tuples will grow from end to beginning. The page is considered full when the slot array and the tuple data meet.\nTODO: Diagram here…\n\n\nTuple layout\nThe DBMS assigns each logical tuple a unique identifier that represents its physical location in the DB - File id –&gt; Page id –&gt; Slot number\nA tuple is essentially a sequence of bytes. It is the job of the DBMS to interpret those bytes into attributes (columns) and values. ##### Tuple header\nEach tuple is prefixed with a header that contains meta-data about it - Visibility information for the DBMS’s concurrency control protocol (i.e., information about which transaction created/modified that tuple). - Bit Map for NULL values. - Note that the DBMS does not need to store meta-data about the schema of the database here.\n\nTuple data\nAttributes are typically stored in the order that you specify them when you create the table. Most DBMSs do not allow a tuple to exceed the size of a page.\nTODO: Diagram here…\n\n\nTuple identifier\nEach tuple in the DB is assigned a unique identifier. Most commonly this is page_id + (offset or slot).\n\n\nDenormalized tuple data\nA DBMS can physically denormalize (e.g. pre-join) related tuples and store them together in the same page. - Potentially reduces the amount of I/O for common workload patterns. - Can make updates more expensive.\nTODO: Diagram here…\n\n\n\nConclusion\nDatabase is organized in pages. Different ways to track pages. Different ways to store pages. Different ways to store tuples."
  },
  {
    "objectID": "posts/2024-08-19-dbs-file-formats.html#olap",
    "href": "posts/2024-08-19-dbs-file-formats.html#olap",
    "title": "Data formats and encoding",
    "section": "OLAP",
    "text": "OLAP\nOLAP workloads typically require scanning large portions of a table(s) to analyze data. However, OLAP queries rarely select a single column, i.e. the projection and predicates often involve different columns.\nSELECT product_id,\n    avg(price)\nFROM sales\nWHERE time &gt; '2024-01-01'\nGROUP BY product_id;\nThe DBMS needs to store data in a columnar format for storage and execution benefits. Thus, a columnar scheme that still stores attributes separately but keeps the data for each tuple physically close to each other is desired.\n\nPAX storage model\nPartition attributes model (PAX) is a hybrid storage model that vertically partitions attributes within a DB page. - This is what Parquet and ORC use. - The goal is to get the benefit of faster processing on columnar storage while retaining the spatial locality benefits of row storage.\nHorizontally partition data into row groups. Then, vertically partition their attributes into column chunks.\nGlobal meta-data directory (a zone map) contains offsets to the file’s row groups. Each row group contains its own meta-data header about its contents. The meta-data directory is at the bottom (the footer) because these files are big and we don’t know what the metadata is going to be (min/max value) until we have processed all of the data. This also comes from the Hadoop world and the file is an append-only file, and we can’t make in-place updates. When we are done writing all of the rows groups of the file, we then close the file.\nZone maps, originally known as Small Materialized Aggregates (SMA), are a type of metadata used in database systems to improve query performance by enabling efficient data pruning. A zone map is essentially a pre-computed aggregate that summarizes the attribute values within a block of tuples. For each block, the zone map stores metadata such as the minimum (MIN), maximum (MAX), average (AVG), sum (SUM), and count (COUNT) of the values in that block. This metadata allows the database management system (DBMS) to quickly determine whether a block contains relevant data for a query without having to scan the entire block.\nTODO: diagram…\n\n\nFile format decisions\nThere are a number of decisions when architecting a new file format for OLAP. Of which may include: - File meta-data - Format layout - Type system - Encoding schemes - Block comparison - Filters - Nested data\n\nFile meta-data\nFiles are self-contained to increase portability, i.e. they contain all the relevant information to interpret their contents without external data dependencies.\nEach file maintains global meta-data (usually in the footer) about its contents - Table schema - Row group offsets - Tuple counts / zone counts\nThis is opposite to PostgreSQL because in PSQL you have a bunch of files that keep track of the catalog (schema, tables, types, etc.). Then you have pages for the actual data. In order for us to understand what is in our data pages, you need to go read the catalog.\n\n\nFormat layout\nMost common formats like Apache Parquet and ORC use the PAX storage model that splits data row groups that contain one or more column chunks.\n\n\nType system\nDefines the data types that the format supports. - Logical: Auxiliary types that map to physical types - Physical: low-level byte representation\nFormats vary in the complexity of their type systems that determine how much upstream producer / consumers need to implement.\n\n\nEncoding schemes\nAn encoding scheme specifies how the format stores the bytes for contiguous data (can apply multiple encoding schemes on top of each other to further improve compression). - Dictionary encoding (the most common) - Run-length encoding (RLE) - Bitpacking - Delta encoding - Frame-of-reference (FOR)\n\nDictionary compression\nHow we convert variable length data, e.g. Strings to fixed-length data that we can then compress.\nReplace frequent values with smaller fixed-length codes and then maintain a mapping (dictionary) from the codes to the original values. - Codes could either be positions (using a hash table) or byte offsets into a dictionary. - Optionally sort values in the dictionary. - Further compress dictionary and encoded columns.\nFormat must handle when the number of distinct values in a column is too large. - Parquet: Max dict. size = 1MB - ORC: Pre-compute cardinality and disable if too large.\n\n\nBlock compression\nCompress data using general-purpose algorithm. Scope of compression is only based on the data provided as input - LZO (1996) - LZ4 (2011) - Snappy (2011) - Zstd (2015)\nConsiderations: - Computational overhead - Compress vs. decompress speed - Data opaqueness\nOpaque compression schemes - if you run something through Snappy or Zstd the DB system does not know what those bytes mean and you cannot go and jump to arbitrary offsets to find the data you are looking for. You need to decompress the whole block.\nThis made sense back in the 2000s and 2010s because the main bottleneck was disk and network, so we were willing to pay the CPU costs. But know the CPU is actually one of the slower components. ##### Filters\nWhat is the difference between a filter and an index?—An index tells you where data is, a filter tells you if something does exist. ###### Zone maps\n\nMaintain min/max values per column at the file-level and row group-level.\nMore effective if values are clustered.\nParquet and ORC store zone maps in the header of each row group. ###### Bloom filters\nA probabilistic data-structure (can get false positives but never false negatives).\nTrack the existence of values for each column in a row group. #### Nested data\n\nReal-world data sets often contain semi-structured objects, e.g. JSON, Protobufs.\nA file format will want to encode the contents of these objects as if they were regular columns.\nTwo approaches: 1. Record shredding 2. Length + presence encoding\n\n\nShredding\nInstead of storing the semi-structured data as a “blob” in a column because every single time you need to parse the blob, you need to run JSON functions to extract the structure from it.\nWe are going to split it up so that every level in the path is treated as a separate column. Now we can rip through a column for a given field in the JSON, does it have this attribute with a certain value.\nThere is always a schema because it doesn’t make sense to have random applications inserting random documents into a table.\nTODO: insert diagram\nFor each path, store it as a separate column and record how many steps deep we are into a given document for that hierarchy. Essentially, we are storing paths as separate columns with additional meta-data about paths."
  },
  {
    "objectID": "posts/2022-10-07-inference-mc-approximation.html",
    "href": "posts/2022-10-07-inference-mc-approximation.html",
    "title": "Inference - Monte Carlo Approximation",
    "section": "",
    "text": "In the probabilistic approach to machine learning, all unknown quantities—predictions about the future, hidden states of a system, or parameters of a model—are treated as random variables, and endowed with probability distributions. The process of inference corresponds to computing the posterior distribution over these quantities, conditioning on whatever data is available. Given that the posterior is a probability distribution, we can draw samples from it. The samples in this case are parameter values. The Bayesian formalism treats parameter distributions as the degrees of relative plausibility, i.e., if this parameter is chosen, how likely is the data to have arisen? We use Bayes’ rule for this process of inference. Let \\(h\\) represent the uknown variables and \\(D\\) the known variables, i.e., the data. Given a likelihood \\(p(D|h)\\) and a prior \\(p(h)\\), we can compute the posterior \\(p(h|D)\\) using Bayes’ rule:\n\\[p(h|D) = \\frac{p(D|h)p(h)}{p(D)}\\]\nThe main problem is the \\(p(D)\\) in the demoninator. \\(p(D)\\) is a normalization constant and ensures the probability distribution sums to 1. When the number of unknown variables \\(h\\) is large, computing \\(p(D)\\) requires a high dimensional integral of the form:\n\\[p(D) = \\int p(D|h)p(h)dh\\]\nThe integral is needed to convert the unnormalized joint probability of some parameter value \\(p(h, D)\\) to a normalized probability \\(p(h|D)\\). This also allows us to take into account all the other plausible values of \\(h\\) that could have generated the data. There are three ways for computing the posterior:\n\nAnalytical Solution\nGrid Approximation\nApproximate Inference\n\nMany problems are complex and require a model where computing the posterior distribution using a grid of parameters or in exact mathematical form is not feasible (or possible). Therefore, you adopt the approximate inference / sampling approach. The sampling approach has a major benefit. Working with samples transforms a problem in calculus \\(\\rightarrow\\) into a problem of data summary \\(\\rightarrow\\) into a frequency format problem. An integral in a typical Bayesian context is just the total probability in some interval. Once you have samples from the probability distribution, it’s just a matter of counting values in the interval. Therefore, once you fit a model to the data using some sampling algorithm, then interpreting the model is a matter of interpreting the frequency of parameter samples (though this is easier said than done).\nTo gain a better conceptual understanding of algorithmic techniques for computing (approximate) posteriors, I will be diving deeper into the main inference algorithms over the next couple of posts.\n\n\nAs discussed above, it is often difficult to compute the posterior distribution analytically. In this example, suppose \\(x\\) is a random variable, and \\(y = f(x)\\) is some function of \\(x\\). Here, \\(y\\) is our target distribution (think the posterior). Instead of computing \\(p(y)\\) analytically, it is possible to draw a large number of samples from \\(p(x)\\), and then use these samples to approximate \\(p(y)\\).\nIf \\(x\\) is distributed uniformly in an interval between \\(-1, 1\\) and \\(y = f(x) = x^2\\), we can approximate \\(p(y)\\) by drawing samples from \\(p(x)\\). By using a large number of samples, a good approximation can be computed.\n\n\nCode\ndef plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y):\n\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))\n\n    ax[0].set_title('True Uniform Distribution')\n    ax[0].plot(x_samples, probs_x)\n    ax[0].set_xlabel('$x$')\n    ax[0].set_ylabel('$p(x)$')\n\n    ax[1].set_title('True $y$ PDF')\n    ax[1].plot(true_y, pdf_y)\n    ax[1].set_xlabel('$y$')\n    ax[1].set_ylabel('$p(y)$')\n\n    ax[2].set_title('Approximated $y$ PDF')\n    ax[2].hist(approx_y, bins=30, density=True)\n    ax[2].set_xlabel('$y$')\n    ax[2].set_ylabel('$p(y)$')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\ndef main():\n    \n    square_func = lambda x: x**2\n\n    # True p(x) \n    lower, upper = -1, 1\n    x_samples = np.linspace(lower, upper, 200)\n    \n    # Analytical solution\n    probs_x = 1 / (upper - lower) * np.ones(len(x_samples)) # p(X = x)\n    true_y = square_func(x_samples) # true output y\n    pdf_y = 1 / (2 * np.sqrt(true_y + 1e-2)) # true pdf of output y\n\n    # Approximation p(y) \n    uniform_samples = np.random.uniform(-1, 1, 1000) # sample from Uniform\n    approx_y = square_func(uniform_samples) # approx. output y\n\n    plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y)\n\nmain()\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, define the squaring function \\(f(x)\\) as square_func\nx_samples is an array of 200 samples in the interval \\([-1, 1]\\)\nThe probability of each element in x_samples: \\(p(X=x)\\) is computed\nCompute true_y using the known x_samples\nCompute the empirical probability density of the output true_y.\nDraw 1000 samples from a Uniform distribution\nUse these samples to approximate approx_y the empirical probability density"
  },
  {
    "objectID": "posts/2022-10-07-inference-mc-approximation.html#inference",
    "href": "posts/2022-10-07-inference-mc-approximation.html#inference",
    "title": "Inference - Monte Carlo Approximation",
    "section": "",
    "text": "In the probabilistic approach to machine learning, all unknown quantities—predictions about the future, hidden states of a system, or parameters of a model—are treated as random variables, and endowed with probability distributions. The process of inference corresponds to computing the posterior distribution over these quantities, conditioning on whatever data is available. Given that the posterior is a probability distribution, we can draw samples from it. The samples in this case are parameter values. The Bayesian formalism treats parameter distributions as the degrees of relative plausibility, i.e., if this parameter is chosen, how likely is the data to have arisen? We use Bayes’ rule for this process of inference. Let \\(h\\) represent the uknown variables and \\(D\\) the known variables, i.e., the data. Given a likelihood \\(p(D|h)\\) and a prior \\(p(h)\\), we can compute the posterior \\(p(h|D)\\) using Bayes’ rule:\n\\[p(h|D) = \\frac{p(D|h)p(h)}{p(D)}\\]\nThe main problem is the \\(p(D)\\) in the demoninator. \\(p(D)\\) is a normalization constant and ensures the probability distribution sums to 1. When the number of unknown variables \\(h\\) is large, computing \\(p(D)\\) requires a high dimensional integral of the form:\n\\[p(D) = \\int p(D|h)p(h)dh\\]\nThe integral is needed to convert the unnormalized joint probability of some parameter value \\(p(h, D)\\) to a normalized probability \\(p(h|D)\\). This also allows us to take into account all the other plausible values of \\(h\\) that could have generated the data. There are three ways for computing the posterior:\n\nAnalytical Solution\nGrid Approximation\nApproximate Inference\n\nMany problems are complex and require a model where computing the posterior distribution using a grid of parameters or in exact mathematical form is not feasible (or possible). Therefore, you adopt the approximate inference / sampling approach. The sampling approach has a major benefit. Working with samples transforms a problem in calculus \\(\\rightarrow\\) into a problem of data summary \\(\\rightarrow\\) into a frequency format problem. An integral in a typical Bayesian context is just the total probability in some interval. Once you have samples from the probability distribution, it’s just a matter of counting values in the interval. Therefore, once you fit a model to the data using some sampling algorithm, then interpreting the model is a matter of interpreting the frequency of parameter samples (though this is easier said than done).\nTo gain a better conceptual understanding of algorithmic techniques for computing (approximate) posteriors, I will be diving deeper into the main inference algorithms over the next couple of posts.\n\n\nAs discussed above, it is often difficult to compute the posterior distribution analytically. In this example, suppose \\(x\\) is a random variable, and \\(y = f(x)\\) is some function of \\(x\\). Here, \\(y\\) is our target distribution (think the posterior). Instead of computing \\(p(y)\\) analytically, it is possible to draw a large number of samples from \\(p(x)\\), and then use these samples to approximate \\(p(y)\\).\nIf \\(x\\) is distributed uniformly in an interval between \\(-1, 1\\) and \\(y = f(x) = x^2\\), we can approximate \\(p(y)\\) by drawing samples from \\(p(x)\\). By using a large number of samples, a good approximation can be computed.\n\n\nCode\ndef plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y):\n\n    fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 4))\n\n    ax[0].set_title('True Uniform Distribution')\n    ax[0].plot(x_samples, probs_x)\n    ax[0].set_xlabel('$x$')\n    ax[0].set_ylabel('$p(x)$')\n\n    ax[1].set_title('True $y$ PDF')\n    ax[1].plot(true_y, pdf_y)\n    ax[1].set_xlabel('$y$')\n    ax[1].set_ylabel('$p(y)$')\n\n    ax[2].set_title('Approximated $y$ PDF')\n    ax[2].hist(approx_y, bins=30, density=True)\n    ax[2].set_xlabel('$y$')\n    ax[2].set_ylabel('$p(y)$')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\ndef main():\n    \n    square_func = lambda x: x**2\n\n    # True p(x) \n    lower, upper = -1, 1\n    x_samples = np.linspace(lower, upper, 200)\n    \n    # Analytical solution\n    probs_x = 1 / (upper - lower) * np.ones(len(x_samples)) # p(X = x)\n    true_y = square_func(x_samples) # true output y\n    pdf_y = 1 / (2 * np.sqrt(true_y + 1e-2)) # true pdf of output y\n\n    # Approximation p(y) \n    uniform_samples = np.random.uniform(-1, 1, 1000) # sample from Uniform\n    approx_y = square_func(uniform_samples) # approx. output y\n\n    plot_mc(x_samples, probs_x, true_y, pdf_y, approx_y)\n\nmain()\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, define the squaring function \\(f(x)\\) as square_func\nx_samples is an array of 200 samples in the interval \\([-1, 1]\\)\nThe probability of each element in x_samples: \\(p(X=x)\\) is computed\nCompute true_y using the known x_samples\nCompute the empirical probability density of the output true_y.\nDraw 1000 samples from a Uniform distribution\nUse these samples to approximate approx_y the empirical probability density"
  },
  {
    "objectID": "posts/2022-10-12-inference-gibbs.html",
    "href": "posts/2022-10-12-inference-gibbs.html",
    "title": "Inference - Gibbs Sampling from Scratch",
    "section": "",
    "text": "A variant of the Metropolis-Hastings (MH) algorithm that uses clever proposals and is therefore more efficient (you can get a good approximate of the posterior with far fewer samples) is Gibbs sampling. A problem with MH is the need to choose the proposal distribution, and the fact that the acceptance rate may be low.\nThe improvement arises from adaptive proposals in which the distribution of proposed parameter values adjusts itself intelligently, depending upon the parameter values at the moment. This dependence upon the parameters at that moment is an exploitation of conditional independence properties of a graphical model to automatically create a good proposal, with acceptance probability equal to one.\n\nMain Idea\nSuppose we have a 3-dimensional joint distribution. Estimating this joint distribution is much harder than a 1-dimensional distribution. Subsequently, sampling is also harder in \\(\\mathbb{R}^3\\). In Gibbs sampling, you condition each variable on the values of all the other variables in the distribution. For example, if we have \\(D=3\\) variables:\n\\[x_{1}^{s+1} \\sim p(x_1 | x_2^s,x_3^s)\\]\n\\[x_{2}^{s+1} \\sim p(x_2 | x_1^{s+1},x_3^s)\\]\n\\[x_{3}^{s+1} \\sim p(x_3 | x_1^{s+1},x_2^{s+1})\\]\nwhere \\(x_1, x_2,...,x_n\\) are variable \\(1, 2,...,n\\), respectively. By conditioning on the values of the other variables, sampling from the conditional distribution is much easier than the joint. Because of the exploitation of conditional independence properties of graphical models, the Gibbs algorithm can readily generalize to \\(D\\) variables.\n\n\nGibbs Sampling - Bayesian Gaussian Mixture Model\nHere, the Gibbs sampling algorithm is implemented in PyTorch for a 2-dimensional Bayesian Gaussian Mixture Model (GMM). The Bayesian GMM is given by:\n\\[p(z = k, x | \\theta) = \\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)\\]\nwhere the parameters \\(\\theta\\) are known and implemented using PyTorch’s MixtureSameFamily distribution class. This class implements a batch of mixture distributions where all components are from different parameterizations of the same distribution type (Normal distributions in this example). It is then parameterized by a Categorical distribution over \\(k\\) components.\nFor a GMM, the full conditional distributions are:\n\\[p(x|z = k, \\theta) = \\mathcal{N}(x|\\mu_k, \\sum_k)\\]\nThis conditional distribution reads; “the probability of data \\(x\\) given component \\(k\\) parameterized by \\(\\theta\\) is distributed according to a Normal distribution with a mean vector \\(\\mu\\) and covariance \\(\\sum\\) according to component \\(k\\)”.\n\\[p(z = k | x) = \\frac{\\pi_k \\mathcal{N}(x|\\mu_k, \\sum_k)}{\\sum_{k'} \\mathcal{N}(x|\\mu_{k'}, \\sum_{k'})}\\]\nThis conditional distribution is given by Bayes rule and reads; “the probability of component \\(k\\) given we observe some data \\(x\\) is equal to the prior probability \\(\\pi\\) of component \\(k\\) times the likelihood of data \\(x\\) being distributed according to a Normal distribution with a mean vector \\(\\mu\\) and covariance \\(\\sum\\) according to component \\(k\\)” over the total probability of components \\(k\\).\nWith the conditional distributions defined, the Gibbs sampling algorithm can be implemented. Below is the full code to reproduce the results and each main step is outlined below the code block.\n\n\nCode\ndef plot_gibbs(trace_hist, probs, scales, mus, n_iters, n_eval=500):\n\n    mix = dist.Categorical(probs=probs)\n    comp = dist.Independent(dist.Normal(loc=mus, scale=scales), 1)\n    norm_mixture = dist.MixtureSameFamily(mix, comp)\n    \n    x = torch.arange(-1, 2, 0.01)\n    y = torch.arange(-1, 2, 0.01) \n\n    X, Y = torch.meshgrid(x, y)\n    Z = torch.dstack((X, Y))\n    probs_z = torch.exp(norm_mixture.log_prob(Z))\n\n    fig = plt.figure(figsize=(12, 5))\n    plt.contourf(X, Y, probs_z, levels=15)\n    plt.scatter(trace_hist[:, 0], trace_hist[:, 1], alpha=0.25, color='red')\n    plt.xlim(-1, 2)\n    plt.ylim(-1, 2)\n    plt.colorbar()\n    plt.xlabel(xlabel='$X$')\n    plt.ylabel(ylabel='$Y$')\n    plt.title('Gibbs Sampling for a Mixture of 2d Gaussians')\n    plt.show()\n\n\n\ndef gibbs_sampler(x0, z0, kv, probs, mu, scale, n_iterations, rng_key=None):\n    \"\"\"\n    implements the gibbs sampling algorithm for known params. of\n    a 2d GMM\n    \"\"\"\n\n    x_current = x0\n    z_current = z0\n\n    x_samples = torch.zeros(n_iterations, 2)\n    z_samples = torch.zeros(n_iterations)\n     \n    for n in range(1, n_iterations):\n\n        # p(Z = k | X = x)\n        probs_z = torch.exp(\n            dist.Independent(\n                dist.Normal(loc=mu, scale=scale), 1).log_prob(x_current))\n        # p(Z = k) * p(Z = k | X = x)\n        probs_z *= probs\n        # denom. of Bayes\n        probs_z = probs_z / torch.sum(probs_z)\n        # indexing component Z = k\n        z_current = kv[-1] if probs_z[-1] &gt; probs[0] else kv[0]\n        # draw new sample X conditioned on Z = k\n        x_current = dist.Normal(loc=mu[z_current], scale=scale[z_current]).sample()\n\n        x_samples[n] = x_current\n        z_samples[n] = z_current\n\n    return x_samples, z_samples\n\n\ndef main(args):\n\n    # initial sample values\n    x0 = torch.randn((2,))\n    # initial component values\n    z0 = torch.randint(0, 2, (2,))\n    # for indexing\n    kv = np.arange(2)\n    # defining prior mixture probability p(Z = k)\n    mixture_probs = torch.tensor([0.4, 0.6])\n    # defining mu vector and covariance matrix\n    mus = torch.randn(2, 2)\n    scales = torch.rand(2, 2)\n\n    x_samples, z_samples = gibbs_sampler(\n        x0, z0, kv, mixture_probs, mus, scales, args.iters\n        )\n\n    plot_gibbs(x_samples, mixture_probs, scales, mus, args.iters)\n\n\nparser = argparse.ArgumentParser(description='rw-mh')\nparser.add_argument('--iters', type=int, default=1000)\nargs = parser.parse_args(\"\")\n\nmain(args)\n\n\n\n\n\n\n\n\n\n\nExplanation of Code\nThe main steps to implement the Gibbs sampling algorithm:\n\nThe main() function defines the initial values for the sample, component, and mixture distribution.\nThe gibbs_sampler() function first sets the values of x_current and z_current for current data point \\(x\\) and component \\(z\\), respectively. To analyze the trace history of the sampler, x_samples and z_samples are empty lists.\nIn the for loop, the conditional \\(p(z = k \\vert x)\\) is computed using x_current, i.e., given we have observed datum \\(x\\), what is the log probability of component \\(Z = k\\).\nprobs_z is then multiplied by the prior probability of the mixture components.\nThen, the denominator for the conditional \\(p(z = k \\vert x)\\) is computed by dividing probs_z by the total probability.\nSince there are only two components in this GMM, we use logic to determine which component is most likely with the x_current. As zero-based indexing is used, the components are \\(k = 0, 1\\). Therefore, if the probability of \\(k=1 &gt; k=0\\), then use index\\(=1\\), else index\\(=0\\).\nz_current defines the component using zero-based indexing. Thus, indexing mu and scale by the current, most likely component, new samples are drawn according to the conditional Normal distribution.\nData and component samples are appended for analyzing the trace history.\n\n\n\nLimitations\nAlthough Gibbs sampling can generalize to \\(D\\) variables, the algorithm becomes inefficient as it tends to get stuck in small regions of the posterior for, potentially, a long number of iterations. This isn’t because of the large number of variables, but rather, because models with many parameters tend to have regions of high correlation in the posterior. High correlation between parameters means a narrow ridge of probability combinations, resulting in the sampler getting “stuck”."
  },
  {
    "objectID": "posts/2023-08-18-gsoc-final-report.html",
    "href": "posts/2023-08-18-gsoc-final-report.html",
    "title": "Google Summer of Code - Final Report",
    "section": "",
    "text": "My project “Better tools to interpret complex Bambi regression models” was completed under the organization of NumFOCUS, and mentors Tomás Capretto and Osvaldo Martin. Before I describe the project, objectives, and work completed, I would like to thank my mentors Tomás and Osvaldo for their precious time and support throughout the summer. They were always available and timely in communicating over Slack and GitHub, and provided valuable feedback during code reviews. Additionally, I would like to thank NumFOCUS and the Google Summer of Code (GSoC) program for providing the opportunity to work on such an open source project over the summer. It has been an invaluable experience, and I look forward to contributing to open source projects in the future.\n\nProject Description\nBayesian modeling has increased significantly in academia and industry over the past years thanks to the development of high quality and user friendly open source probabilistic programming languages (PPL) in Python and R. Of these is Bambi, a Python library built on top of the PyMC PPL, that makes it easy to specify complex generalized linear multilevel models using a formula notation similar to those found in R. However, as the model building portion of the Bayesian workflow becomes easier, the interpretation of these models has not.\nTo aid in model interpretability, Bambi (before this project) had a sub-package plots that supported conditional adjusted predictions plots. The original objective was to extend upon the existing plotting functionality of conditional adjusted predictions by supporting the plotting of posterior predictive samples, and to provide the additional plotting functions predictive comparisons and predictive slopes. However, after discussion with my mentors, and taking inspiration from marginaleffects, it was decided that in addition to the plotting functions, the plots sub-package should also have functions that allow the user to return the dataframe used for plotting. For example, calling plot_slopes() would plot the slopes, and calling slopes() would return the dataframe used to plot the slopes. To this extent, the plots sub-package was renamed to interpret to better reflect all of the supported functionality. These three features allow Bambi modelers to compute and interpret three different “quantities of interest” in a more automatic and effective manner.\nThus, the three main deliverables of this project were to add (or enhance) the following functions in the interpret sub-package. Additionally, for each feature, tests and documentation need to be added:\n\nSupport posterior predictive samples (pps) in plot_predictions()\n\nWrite tests\nAdd documentation\n\nAdd comparisons() and plot_comparisons()\n\nWrite tests\nAdd documentation\n\nAdd slopes() and plot_slopes()\n\nWrite tests\nAdd documentation\n\n\n\n\nWork Completed\nAll three main deliverables (and their associated sub-deliverables) were completed (merged into the main branch of the Bambi repository) on time. In the table below, the name of the deliverable, link to the feature’s pull request (PR), and link to the documentation are provided.\n\n\n\nDeliverable\nFeature PR\nDocumentation PR\n\n\n\n\nAllow plot_predictions to plot posterior predictive samples\nPR #668\nPR #670\n\n\nAdd comparisons and plot_comparisons\nPR #684\nPR #695\n\n\nAdd slopes and plot_slopes\nPR #699\nPR #701\n\n\n\nIn order to quickly obtain a sense of the work completed over the GSoC program, it is probably best to view the documentation PRs. The documentation PRs consist of a Jupyter notebook that demonstrates: (1) why the new feature is useful in interpreting GLMs, (2) a brief overview of how the function computes the quantity of interest based on the user’s inputs, and (3) a demonstration of the function using different GLMs and datasets.\n\n\nFuture Work\nAs the PRs have been merged upstream, some Bambi users have already been utilizing the new sub-package. The feedback has been positive, and a GitHub issue has been opened to request additional functionality. Going forward, I will continue to contribute to Bambi, maintain the interpret sub-package, and interact with the Bambi community."
  },
  {
    "objectID": "posts/2023-12-09-interpret-advanced-usage.html",
    "href": "posts/2023-12-09-interpret-advanced-usage.html",
    "title": "Advanced Interpret Usage in Bambi",
    "section": "",
    "text": "The interpret module is inspired by the R package marginaleffects and ports the core functionality of {marginaleffects} to Bambi. To close the gap of non-supported functionality in Bambi, interpret now provides a set of helper functions to aid the user in more advanced and complex analysis not covered within the comparisons, predictions, and slopes functions.\nThese helper functions are data_grid and select_draws. The data_grid can be used to create a pairwise grid of data points for the user to pass to model.predict. Subsequently, select_draws is used to select the draws from the posterior (or posterior predictive) group of the InferenceData object returned by the predict method that correspond to the data points that “produced” that draw.\nWith access to the appropriately indexed draws, and data used to generate those draws, it enables for more complex analysis such as cross-comparisons and the choice of which model parameter to compute a quantity of interest for. Additionally, the user has more control over the data passed to model.predict. Below, it will be demonstrated how to use these helper functions. First, to reproduce the results from the standard interpret API, and then to compute cross-comparisons.\n\nimport warnings\n\nimport arviz as az\nimport numpy as np\nimport pandas as pd\n\nimport bambi as bmb\n\nfrom bambi.interpret.helpers import data_grid, select_draws\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\n\nWe will adopt the zero inflated Poisson (ZIP) model from the comparisons documentation to demonstrate the helper functions introduced above.\nThe ZIP model will be used to predict how many fish are caught by visitors at a state park using survey data. Many visitors catch zero fish, either because they did not fish at all, or because they were unlucky. We would like to explicitly model this bimodal behavior (zero versus non-zero) using a Zero Inflated Poisson model, and to compare how different inputs of interest \\(w\\) and other covariate values \\(c\\) are associated with the number of fish caught. The dataset contains data on 250 groups that went to a state park to fish. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), if they used a live bait and whether or not they brought a camper to the park (camper).\n\nfish_data = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\ncols = [\"count\", \"livebait\", \"camper\", \"persons\", \"child\"]\nfish_data = fish_data[cols]\nfish_data[\"child\"] = fish_data[\"child\"].astype(np.int8)\nfish_data[\"persons\"] = fish_data[\"persons\"].astype(np.int8)\nfish_data[\"livebait\"] = pd.Categorical(fish_data[\"livebait\"])\nfish_data[\"camper\"] = pd.Categorical(fish_data[\"camper\"])\n\n\nfish_model = bmb.Model(\n    \"count ~ livebait + camper + persons + child\", \n    fish_data, \n    family='zero_inflated_poisson'\n)\n\nfish_idata = fish_model.fit(random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [count_psi, Intercept, livebait, camper, persons, child]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\n\n\n\ndata_grid allows you to create a pairwise grid, also known as a cross-join or cartesian product, of data using the covariates passed to the conditional and the optional variable parameter. Covariates not passed to conditional, but are terms in the Bambi model, are set to typical values (e.g., mean or mode). If you are coming from R, this function is partially inspired from the data_grid function in {modelr}.\nThere are two ways to create a pairwise grid of data:\n\nuser-provided values are passed as a dictionary to conditional where the keys are the names of the covariates and the values are the values to use in the grid.\na list of covariates where the elements are the names of the covariates to use in the grid. As only the names of the covariates were passed, default values are computed to construct the grid.\n\nAny unspecified covariates, i.e., covariates not passed to conditional but are terms in the Bambi model, are set to their “typical” values such as mean or mode depending on the data type of the covariate.\n\n\nTo construct a pairwise grid of data for specific covariate values, pass a dictionary to conditional. The values of the dictionary can be of type int, float, list, or np.ndarray.\n\nconditional = {\n    \"camper\": np.array([0, 1]),\n    \"persons\": np.arange(1, 5, 1),\n    \"child\": np.array([1, 2, 3]),\n}\nuser_passed_grid = data_grid(fish_model, conditional)\nuser_passed_grid.query(\"camper == 0\")\n\nDefault computed for unspecified variable: livebait\n\n\n\n\n\n\n\n\n\ncamper\npersons\nchild\nlivebait\n\n\n\n\n0\n0\n1\n1\n1.0\n\n\n1\n0\n1\n2\n1.0\n\n\n2\n0\n1\n3\n1.0\n\n\n3\n0\n2\n1\n1.0\n\n\n4\n0\n2\n2\n1.0\n\n\n5\n0\n2\n3\n1.0\n\n\n6\n0\n3\n1\n1.0\n\n\n7\n0\n3\n2\n1.0\n\n\n8\n0\n3\n3\n1.0\n\n\n9\n0\n4\n1\n1.0\n\n\n10\n0\n4\n2\n1.0\n\n\n11\n0\n4\n3\n1.0\n\n\n\n\n\n\n\nSubsetting by camper = 0, it can be seen that a combination of all possible pairs of values from the dictionary (including the unspecified variable livebait) results in a dataframe containing every possible combination of values from the original sets. livebait has been set to 1 as this is the mode of the unspecified categorical variable.\n\n\n\nAlternatively, a list of covariates can be passed to conditional where the elements are the names of the covariates to use in the grid. By doing this, you are telling interpret to compute default values for these covariates. The psuedocode below outlines the logic and functions used to compute these default values:\nif is_numeric_dtype(x) or is_float_dtype(x):\n    values = np.linspace(np.min(x), np.max(x), 50)\n\nelif is_integer_dtype(x):\n    values = np.quantile(x, np.linspace(0, 1, 5))\n\nelif is_categorical_dtype(x) or is_string_dtype(x) or is_object_dtype(x):\n    values = np.unique(x)\n\nconditional = [\"camper\", \"persons\", \"child\"]\ndefault_grid = data_grid(fish_model, conditional)\n\ndefault_grid.shape, user_passed_grid.shape\n\nDefault computed for conditional variable: camper, persons, child\nDefault computed for unspecified variable: livebait\n\n\n((32, 4), (24, 4))\n\n\nNotice how the resulting length is different between the user passed and default grid. This is due to the fact that values for child range from 0 to 3 for the default grid.\n\ndefault_grid.query(\"camper == 0\")\n\n\n\n\n\n\n\n\ncamper\npersons\nchild\nlivebait\n\n\n\n\n0\n0.0\n1\n0\n1.0\n\n\n1\n0.0\n1\n1\n1.0\n\n\n2\n0.0\n1\n2\n1.0\n\n\n3\n0.0\n1\n3\n1.0\n\n\n4\n0.0\n2\n0\n1.0\n\n\n5\n0.0\n2\n1\n1.0\n\n\n6\n0.0\n2\n2\n1.0\n\n\n7\n0.0\n2\n3\n1.0\n\n\n8\n0.0\n3\n0\n1.0\n\n\n9\n0.0\n3\n1\n1.0\n\n\n10\n0.0\n3\n2\n1.0\n\n\n11\n0.0\n3\n3\n1.0\n\n\n12\n0.0\n4\n0\n1.0\n\n\n13\n0.0\n4\n1\n1.0\n\n\n14\n0.0\n4\n2\n1.0\n\n\n15\n0.0\n4\n3\n1.0\n\n\n\n\n\n\n\n\n\n\n\nTo use data_grid to help generate data in computing comparisons or slopes, additional data is passed to the optional variable parameter. The name variable is an abstraction for the comparisons parameter contrast and slopes parameter wrt. If you have used any of the interpret functions, these parameter names should be familiar and the use of data_grid should be analogous to comparisons, predictions, and slopes.\nvariable can also be passed user-provided data (as a dictionary), or a string indicating the name of the covariate of interest. If the latter, a default value will be computed. Additionally, if an argument is passed for variable, then the effect_type needs to be passed. This is because for comparisons and slopes an epsilon value eps needs to be determined to compute the centered and finite difference, respectively. You can also pass a value for eps as a kwarg.\n\nconditional = {\n    \"camper\": np.array([0, 1]),\n    \"persons\": np.arange(1, 5, 1),\n    \"child\": np.array([1, 2, 3, 4])\n}\nvariable = \"livebait\"\n\ngrid = data_grid(fish_model, conditional, variable, effect_type=\"comparisons\")\n\nDefault computed for contrast variable: livebait\n\n\n\nidata_grid = fish_model.predict(fish_idata, data=grid, inplace=False)\n\n\n\nThe second helper function to aid in more advanced analysis is select_draws. This is a function that selects the posterior or posterior predictive draws from the ArviZ InferenceData object returned by model.predict given a conditional dictionary. The conditional dictionary represents the values that correspond to that draw.\nFor example, if we wanted to select posterior draws where livebait = [0, 1], then all we need to do is pass a dictionary where the key is the name of the covariate and the value is the value that we want to condition on (or select). The resulting InferenceData object will contain the draws that correspond to the data points where livebait = [0, 1]. Additionally, you must pass the InferenceData object returned by model.predict, the data used to generate the predictions, and the name of the data variable data_var you would like to select from the InferenceData posterior group. If you specified to return the posterior predictive samples by passing model.predict(..., kind=\"pps\"), you can use this group instead of the posterior group by passing group=\"posterior_predictive\".\nBelow, it is demonstrated how to compute comparisons for count_mean for the contrast livebait = [0, 1] using the posterior draws.\n\nidata_grid\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (chain: 4, draw: 1000, livebait_dim: 1, camper_dim: 1,\n                   count_obs: 64)\nCoordinates:\n  * chain         (chain) int64 0 1 2 3\n  * draw          (draw) int64 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * livebait_dim  (livebait_dim) &lt;U3 '1.0'\n  * camper_dim    (camper_dim) &lt;U3 '1.0'\n  * count_obs     (count_obs) int64 0 1 2 3 4 5 6 7 ... 56 57 58 59 60 61 62 63\nData variables:\n    Intercept     (chain, draw) float64 -2.454 -2.31 -2.91 ... -2.652 -2.887\n    livebait      (chain, draw, livebait_dim) float64 1.629 1.58 ... 1.799 1.967\n    camper        (chain, draw, camper_dim) float64 0.7037 0.7089 ... 0.7128\n    persons       (chain, draw) float64 0.8707 0.8369 0.9457 ... 0.8847 0.912\n    child         (chain, draw) float64 -1.345 -1.412 -1.418 ... -1.293 -1.573\n    count_psi     (chain, draw) float64 0.6311 0.6201 0.6342 ... 0.6768 0.5745\n    count_mean    (chain, draw, count_obs) float64 0.05349 0.2728 ... 0.05777\nAttributes:\n    created_at:                  2023-12-09T06:01:42.032935\n    arviz_version:               0.16.1\n    inference_library:           pymc\n    inference_library_version:   5.8.1\n    sampling_time:               2.652681827545166\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0.dev0xarray.DatasetDimensions:chain: 4draw: 1000livebait_dim: 1camper_dim: 1count_obs: 64Coordinates: (5)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])livebait_dim(livebait_dim)&lt;U3'1.0'array(['1.0'], dtype='&lt;U3')camper_dim(camper_dim)&lt;U3'1.0'array(['1.0'], dtype='&lt;U3')count_obs(count_obs)int640 1 2 3 4 5 6 ... 58 59 60 61 62 63array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63])Data variables: (7)Intercept(chain, draw)float64-2.454 -2.31 ... -2.652 -2.887array([[-2.45369792, -2.31017959, -2.91023957, ..., -2.12445232,\n        -2.13810127, -2.40815127],\n       [-2.21843503, -2.81570517, -2.81570517, ..., -3.23806838,\n        -2.43778314, -2.40094452],\n       [-2.57030767, -2.71197967, -3.13096258, ..., -2.75122519,\n        -2.81845915, -2.77702878],\n       [-2.27327526, -2.76201826, -2.72978474, ..., -2.43555099,\n        -2.65150277, -2.88742403]])livebait(chain, draw, livebait_dim)float641.629 1.58 1.858 ... 1.799 1.967array([[[1.62921077],\n        [1.57971577],\n        [1.85814553],\n        ...,\n        [1.39320044],\n        [1.31938529],\n        [2.0303655 ]],\n\n       [[1.44441014],\n        [2.03180494],\n        [2.03180494],\n        ...,\n        [2.00383874],\n        [2.0219552 ],\n        [1.77554631]],\n\n       [[1.44183823],\n        [2.12560898],\n        [1.84301056],\n        ...,\n        [1.65875902],\n        [1.69372851],\n        [1.78166201]],\n\n       [[1.64438961],\n        [1.88379206],\n        [1.95128032],\n        ...,\n        [1.73718918],\n        [1.79946112],\n        [1.96668979]]])camper(chain, draw, camper_dim)float640.7037 0.7089 ... 0.6445 0.7128array([[[0.70368097],\n        [0.70888183],\n        [0.6432408 ],\n        ...,\n        [0.64457622],\n        [0.68265053],\n        [0.59371655]],\n\n       [[0.73004824],\n        [0.63806292],\n        [0.63806292],\n        ...,\n        [0.92181193],\n        [0.44431118],\n        [0.52257123]],\n\n       [[0.67251782],\n        [0.61970006],\n        [0.80110109],\n        ...,\n        [0.71626836],\n        [0.7386686 ],\n        [0.80126267]],\n\n       [[0.68695406],\n        [0.66147853],\n        [0.7129354 ],\n        ...,\n        [0.62523222],\n        [0.64449304],\n        [0.71282881]]])persons(chain, draw)float640.8707 0.8369 ... 0.8847 0.912array([[0.87073326, 0.83691824, 0.94570525, ..., 0.86127639, 0.85807333,\n        0.7827932 ],\n       [0.85684603, 0.88279463, 0.88279463, ..., 0.93383647, 0.82809407,\n        0.84387704],\n       [0.93868593, 0.83511835, 0.9966305 , ..., 0.92864502, 0.9583216 ,\n        0.9046778 ],\n       [0.82278573, 0.89846761, 0.85979591, ..., 0.85493891, 0.88465389,\n        0.91197904]])child(chain, draw)float64-1.345 -1.412 ... -1.293 -1.573array([[-1.34537519, -1.41232517, -1.41808649, ..., -1.28364874,\n        -1.21311483, -1.343049  ],\n       [-1.36022475, -1.38360737, -1.38360737, ..., -1.34137848,\n        -1.37005354, -1.32761054],\n       [-1.42361978, -1.45390876, -1.59921868, ..., -1.57170719,\n        -1.60156825, -1.49976267],\n       [-1.33972561, -1.29366616, -1.38679881, ..., -1.29405166,\n        -1.29343353, -1.57284431]])count_psi(chain, draw)float640.6311 0.6201 ... 0.6768 0.5745array([[0.63114698, 0.62012886, 0.63423439, ..., 0.57386427, 0.56121415,\n        0.57771985],\n       [0.61291154, 0.607627  , 0.607627  , ..., 0.63015121, 0.54061116,\n        0.54315861],\n       [0.60778057, 0.68015035, 0.57385421, ..., 0.75656914, 0.74951665,\n        0.68320354],\n       [0.61202421, 0.58032736, 0.57834964, ..., 0.65743098, 0.67681655,\n        0.57454481]])count_mean(chain, draw, count_obs)float640.05349 0.2728 ... 0.008082 0.05777array([[[0.05348576, 0.27276925, 0.01392994, ..., 0.50966648,\n         0.02602794, 0.13273855],\n        [0.05582204, 0.27093652, 0.01359692, ..., 0.40216834,\n         0.02018278, 0.09795866],\n        [0.03395834, 0.21773528, 0.00822393, ..., 0.41466193,\n         0.01566191, 0.10042158],\n        ...,\n        [0.07833   , 0.31549128, 0.02169934, ..., 0.61108678,\n         0.04203026, 0.16928611],\n        [0.08264981, 0.30920293, 0.0245693 , ..., 0.70955544,\n         0.05638135, 0.21092947],\n        [0.0513851 , 0.3913936 , 0.013414  , ..., 0.50558281,\n         0.01732754, 0.13198164]],\n\n       [[0.06575538, 0.27876013, 0.01687303, ..., 0.49794444,\n         0.03014001, 0.12777409],\n        [0.03627894, 0.27673   , 0.00909414, ..., 0.46511021,\n         0.01528485, 0.11659041],\n        [0.03627894, 0.27673   , 0.00909414, ..., 0.46511021,\n         0.01528485, 0.11659041],\n...\n        [0.03356447, 0.17630703, 0.00697101, ..., 0.25240022,\n         0.00997967, 0.05242108],\n        [0.03137619, 0.17067787, 0.00632482, ..., 0.25730828,\n         0.00953508, 0.05186824],\n        [0.03431703, 0.20383353, 0.00765898, ..., 0.34140667,\n         0.01282825, 0.07619621]],\n\n       [[0.061408  , 0.31796133, 0.01608383, ..., 0.51172625,\n         0.02588528, 0.13403007],\n        [0.04254398, 0.27987149, 0.01166826, ..., 0.60418447,\n         0.02518935, 0.16570571],\n        [0.03851191, 0.271035  , 0.00962312, ..., 0.45530771,\n         0.01616574, 0.11376952],\n        ...,\n        [0.05643511, 0.32062773, 0.01547212, ..., 0.5853596 ,\n         0.02824695, 0.16048086],\n        [0.04687446, 0.28342116, 0.01285894, ..., 0.57739214,\n         0.02619653, 0.1583944 ],\n        [0.02877382, 0.2056459 , 0.00596925, ..., 0.27844848,\n         0.00808248, 0.05776533]]])Indexes: (5)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))livebait_dimPandasIndexPandasIndex(Index(['1.0'], dtype='object', name='livebait_dim'))camper_dimPandasIndexPandasIndex(Index(['1.0'], dtype='object', name='camper_dim'))count_obsPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n      dtype='int64', name='count_obs'))Attributes: (8)created_at :2023-12-09T06:01:42.032935arviz_version :0.16.1inference_library :pymcinference_library_version :5.8.1sampling_time :2.652681827545166tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0.dev0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables: (12/17)\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    max_energy_error       (chain, draw) float64 0.595 1.645 ... -0.2578 0.4279\n    tree_depth             (chain, draw) int64 3 3 3 3 3 3 3 3 ... 3 3 3 3 3 4 4\n    process_time_diff      (chain, draw) float64 0.000894 0.000888 ... 0.001654\n    diverging              (chain, draw) bool False False False ... False False\n    step_size_bar          (chain, draw) float64 0.4502 0.4502 ... 0.4541 0.4541\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 0.7512 0.5492 ... 0.9919 0.8264\n    n_steps                (chain, draw) float64 7.0 7.0 7.0 ... 7.0 11.0 15.0\n    energy                 (chain, draw) float64 751.9 752.2 ... 755.7 756.8\n    index_in_trajectory    (chain, draw) int64 6 4 -6 -5 -2 4 ... 7 1 4 4 -1 9\n    lp                     (chain, draw) float64 -750.1 -750.2 ... -751.9 -754.0\n    energy_error           (chain, draw) float64 0.04379 0.02705 ... 0.415\nAttributes:\n    created_at:                  2023-12-09T06:01:42.040761\n    arviz_version:               0.16.1\n    inference_library:           pymc\n    inference_library_version:   5.8.1\n    sampling_time:               2.652681827545166\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0.dev0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (17)largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])max_energy_error(chain, draw)float640.595 1.645 ... -0.2578 0.4279array([[ 0.595031  ,  1.64520146,  0.21527509, ...,  0.78075972,\n         5.80798519,  0.43782396],\n       [-0.37301973, -0.20402685,  2.5627162 , ...,  0.36741839,\n         3.6828103 , -0.50001872],\n       [ 1.27621281, -0.68221199,  0.44120051, ...,  2.43620146,\n        -0.08386685, -0.22408853],\n       [ 0.14004462,  0.99694174, -0.07864808, ...,  0.22969662,\n        -0.25775022,  0.42794799]])tree_depth(chain, draw)int643 3 3 3 3 3 3 3 ... 3 3 3 3 3 3 4 4array([[3, 3, 3, ..., 3, 2, 3],\n       [3, 3, 3, ..., 3, 4, 3],\n       [3, 3, 3, ..., 3, 2, 3],\n       [3, 3, 4, ..., 3, 4, 4]])process_time_diff(chain, draw)float640.000894 0.000888 ... 0.001654array([[0.000894, 0.000888, 0.000882, ..., 0.00088 , 0.000451, 0.000883],\n       [0.000893, 0.000889, 0.000886, ..., 0.000891, 0.001756, 0.000893],\n       [0.000895, 0.000891, 0.000886, ..., 0.000868, 0.00044 , 0.000865],\n       [0.000894, 0.000895, 0.001761, ..., 0.000882, 0.001246, 0.001654]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])step_size_bar(chain, draw)float640.4502 0.4502 ... 0.4541 0.4541array([[0.45015324, 0.45015324, 0.45015324, ..., 0.45015324, 0.45015324,\n        0.45015324],\n       [0.42774415, 0.42774415, 0.42774415, ..., 0.42774415, 0.42774415,\n        0.42774415],\n       [0.44696235, 0.44696235, 0.44696235, ..., 0.44696235, 0.44696235,\n        0.44696235],\n       [0.45405445, 0.45405445, 0.45405445, ..., 0.45405445, 0.45405445,\n        0.45405445]])perf_counter_start(chain, draw)float641.067e+03 1.067e+03 ... 1.069e+03array([[1067.40390204, 1067.40486017, 1067.40580821, ..., 1068.46292929,\n        1068.46387517, 1068.46438762],\n       [1067.41511467, 1067.41606821, 1067.41704075, ..., 1068.53776558,\n        1068.53871471, 1068.54055937],\n       [1067.46033633, 1067.461301  , 1067.46227354, ..., 1068.56037796,\n        1068.56130604, 1068.56180675],\n       [1067.44745625, 1067.44842704, 1067.44939542, ..., 1068.58570321,\n        1068.58666521, 1068.58797033]])step_size(chain, draw)float640.415 0.415 0.415 ... 0.3918 0.3918array([[0.41500359, 0.41500359, 0.41500359, ..., 0.41500359, 0.41500359,\n        0.41500359],\n       [0.42993239, 0.42993239, 0.42993239, ..., 0.42993239, 0.42993239,\n        0.42993239],\n       [0.55799239, 0.55799239, 0.55799239, ..., 0.55799239, 0.55799239,\n        0.55799239],\n       [0.39177214, 0.39177214, 0.39177214, ..., 0.39177214, 0.39177214,\n        0.39177214]])perf_counter_diff(chain, draw)float640.000894 0.0008885 ... 0.001654array([[0.000894  , 0.00088854, 0.00088317, ..., 0.00087933, 0.00045042,\n        0.00088333],\n       [0.00089242, 0.00088767, 0.00088663, ..., 0.00089146, 0.00178467,\n        0.0008925 ],\n       [0.00089392, 0.00089162, 0.00088608, ..., 0.00086804, 0.00044117,\n        0.00086496],\n       [0.00089838, 0.00089992, 0.00178875, ..., 0.00090033, 0.00124583,\n        0.00165379]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float640.7512 0.5492 ... 0.9919 0.8264array([[0.75118358, 0.54917103, 0.90600545, ..., 0.64640509, 0.24651666,\n        0.84703996],\n       [1.        , 1.        , 0.18921988, ..., 0.81098596, 0.34035628,\n        0.99696537],\n       [0.56560841, 0.95846614, 0.85780695, ..., 0.50105863, 0.99947793,\n        1.        ],\n       [0.9683953 , 0.61509461, 0.99814479, ..., 0.9439049 , 0.99189283,\n        0.82635188]])n_steps(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 11.0 15.0array([[ 7.,  7.,  7., ...,  7.,  3.,  7.],\n       [ 7.,  7.,  7., ...,  7., 15.,  7.],\n       [ 7.,  7.,  7., ...,  7.,  3.,  7.],\n       [ 7.,  7., 15., ...,  7., 11., 15.]])energy(chain, draw)float64751.9 752.2 751.9 ... 755.7 756.8array([[751.94493597, 752.16775838, 751.86589668, ..., 757.02411223,\n        757.59305462, 755.76371432],\n       [754.28875075, 752.65123698, 755.84607135, ..., 759.92489206,\n        761.1929198 , 755.68207248],\n       [755.10969541, 756.38178297, 761.18044623, ..., 762.22995695,\n        756.69732237, 756.35418424],\n       [753.32406793, 752.80673891, 752.03157068, ..., 756.22896884,\n        755.6715562 , 756.78221912]])index_in_trajectory(chain, draw)int646 4 -6 -5 -2 4 -5 ... 7 1 4 4 -1 9array([[ 6,  4, -6, ...,  4,  2,  5],\n       [ 4,  7,  0, ..., -6,  9, -2],\n       [ 3,  5,  5, ..., -4, -2,  4],\n       [-3, -4, -3, ...,  4, -1,  9]])lp(chain, draw)float64-750.1 -750.2 ... -751.9 -754.0array([[-750.0635191 , -750.19379111, -751.09897583, ..., -751.87190426,\n        -752.95441117, -754.23172742],\n       [-751.17173283, -750.81535676, -750.81535676, ..., -755.33966632,\n        -755.44844073, -752.33208106],\n       [-754.06157469, -753.90911474, -759.1519945 , ..., -755.85090606,\n        -755.67682047, -752.00709578],\n       [-750.85175653, -751.04878911, -750.6370331 , ..., -751.95509665,\n        -751.89343509, -753.95045712]])energy_error(chain, draw)float640.04379 0.02705 ... -0.1224 0.415array([[ 4.37855069e-02,  2.70547404e-02, -5.88277115e-04, ...,\n         8.64705248e-02,  3.20765941e-01,  4.37823957e-01],\n       [-9.78110746e-02, -4.02245905e-02,  0.00000000e+00, ...,\n         2.05920633e-01,  2.91692817e-01, -2.47079734e-01],\n       [ 1.14647984e+00, -6.82211987e-01,  4.41200505e-01, ...,\n        -3.52379476e-01,  1.56742695e-03, -1.73449488e-01],\n       [ 1.45574755e-02, -4.02199593e-03, -5.80109339e-02, ...,\n        -9.46547980e-03, -1.22382092e-01,  4.15048801e-01]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (8)created_at :2023-12-09T06:01:42.040761arviz_version :0.16.1inference_library :pymcinference_library_version :5.8.1sampling_time :2.652681827545166tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0.dev0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (count_obs: 250)\nCoordinates:\n  * count_obs  (count_obs) int64 0 1 2 3 4 5 6 7 ... 243 244 245 246 247 248 249\nData variables:\n    count      (count_obs) int64 0 0 0 0 1 0 0 0 0 1 0 ... 4 1 1 0 1 0 0 0 0 0 0\nAttributes:\n    created_at:                  2023-12-09T06:01:42.043721\n    arviz_version:               0.16.1\n    inference_library:           pymc\n    inference_library_version:   5.8.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0.dev0xarray.DatasetDimensions:count_obs: 250Coordinates: (1)count_obs(count_obs)int640 1 2 3 4 5 ... 245 246 247 248 249array([  0,   1,   2, ..., 247, 248, 249])Data variables: (1)count(count_obs)int640 0 0 0 1 0 0 0 ... 0 1 0 0 0 0 0 0array([  0,   0,   0,   0,   1,   0,   0,   0,   0,   1,   0,   0,   1,\n         2,   0,   1,   0,   0,   1,   0,   1,   5,   0,   3,  30,   0,\n        13,   0,   0,   0,   0,   0,  11,   5,   0,   1,   1,   7,   0,\n        14,   0,  32,   0,   1,   0,   0,   0,   1,   5,   0,   1,   0,\n        22,   0,  15,   0,   0,   0,   5,   4,   2,   0,   2,  32,   0,\n         0,   1,   0,   0,   0,   7,   0,   0,   0,   0,   0,   0,   0,\n         0,   2,   3,   1,   5,   0,   2,   1,   0,   1, 149,   0,   1,\n         0,   0,   1,   0,   0,   0,   2,   2,  29,   3,   0,   0,   5,\n         0,   0,   0,   0,   0,   1,   7,   1,   0,   2,   0,   2,   0,\n         0,   0,   1,   0,   0,   0,   0,   0,   3,   4,   3,   3,   8,\n         2,   1,   6,   0,   0,   5,   3,  31,   0,   2,   0,   0,   0,\n         0,   0,   0,   6,   9,   0,   0,   0,   0,   0,   2,  15,   1,\n         2,   3,   0,  65,   5,   0,   0,   0,   0,   1,   8,   0,   0,\n         0,   2,   4,   5,   9,   0,   0,   0,   0,  21,   0,   6,   0,\n         0,   0,   0,  16,   0,   0,   4,   2,  10,   0,   0,   0,   2,\n         1,   3,   0,   0,  21,   0,   0,   2,   0,   3,   0,  38,   0,\n         0,   0,   1,   3,   0,   1,   0,   0,   0,   0,   5,   0,   0,\n         2,   0,   0,   0,   1,   4,   0,   0,   2,   3,   0,   0,   0,\n         0,   1,   2,   0,   6,   4,   1,   1,   0,   1,   0,   0,   0,\n         0,   0,   0])Indexes: (1)count_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n      dtype='int64', name='count_obs', length=250))Attributes: (6)created_at :2023-12-09T06:01:42.043721arviz_version :0.16.1inference_library :pymcinference_library_version :5.8.1modeling_interface :bambimodeling_interface_version :0.13.0.dev0\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\ndraw_1 = select_draws(idata_grid, grid, {\"livebait\": 0}, \"count_mean\")\n\n\ndraw_1 = select_draws(idata_grid, grid, {\"livebait\": 0}, \"count_mean\")\ndraw_2 = select_draws(idata_grid, grid, {\"livebait\": 1}, \"count_mean\")\n\ncomparison_mean = (draw_2 - draw_1).mean((\"chain\", \"draw\"))\ncomparison_hdi = az.hdi(draw_2 - draw_1)\n\ncomparison_df = pd.DataFrame(\n    {\n        \"mean\": comparison_mean.values,\n        \"hdi_low\": comparison_hdi.sel(hdi=\"lower\")[\"count_mean\"].values,\n        \"hdi_high\": comparison_hdi.sel(hdi=\"higher\")[\"count_mean\"].values,\n    }\n)\ncomparison_df.head(10)\n\n\n\n\n\n\n\n\nmean\nhdi_low\nhdi_high\n\n\n\n\n0\n0.214363\n0.144309\n0.287735\n\n\n1\n0.053678\n0.029533\n0.077615\n\n\n2\n0.013558\n0.006332\n0.021971\n\n\n3\n0.003454\n0.001132\n0.006040\n\n\n4\n0.512709\n0.369741\n0.661034\n\n\n5\n0.128316\n0.077068\n0.181741\n\n\n6\n0.032392\n0.015553\n0.050690\n\n\n7\n0.008247\n0.003047\n0.014382\n\n\n8\n1.228708\n0.913514\n1.533121\n\n\n9\n0.307342\n0.192380\n0.426808\n\n\n\n\n\n\n\nWe can compare this comparison with bmb.interpret.comparisons.\n\nsummary_df = bmb.interpret.comparisons(\n    fish_model,\n    fish_idata,\n    contrast={\"livebait\": [0, 1]},\n    conditional=conditional\n)\nsummary_df.head(10)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ncamper\npersons\nchild\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0, 1)\n0\n1\n1\n0.214363\n0.144309\n0.287735\n\n\n1\nlivebait\ndiff\n(0, 1)\n0\n1\n2\n0.053678\n0.029533\n0.077615\n\n\n2\nlivebait\ndiff\n(0, 1)\n0\n1\n3\n0.013558\n0.006332\n0.021971\n\n\n3\nlivebait\ndiff\n(0, 1)\n0\n1\n4\n0.003454\n0.001132\n0.006040\n\n\n4\nlivebait\ndiff\n(0, 1)\n0\n2\n1\n0.512709\n0.369741\n0.661034\n\n\n5\nlivebait\ndiff\n(0, 1)\n0\n2\n2\n0.128316\n0.077068\n0.181741\n\n\n6\nlivebait\ndiff\n(0, 1)\n0\n2\n3\n0.032392\n0.015553\n0.050690\n\n\n7\nlivebait\ndiff\n(0, 1)\n0\n2\n4\n0.008247\n0.003047\n0.014382\n\n\n8\nlivebait\ndiff\n(0, 1)\n0\n3\n1\n1.228708\n0.913514\n1.533121\n\n\n9\nlivebait\ndiff\n(0, 1)\n0\n3\n2\n0.307342\n0.192380\n0.426808\n\n\n\n\n\n\n\nAlbeit the other information in the summary_df, the columns estimate, lower_3.0%, upper_97.0% are identical.\n\n\n\nComputing a cross-comparison is useful for when we want to compare contrasts when two (or more) predictors change at the same time. Cross-comparisons are currently not supported in the comparisons function, but we can use select_draws to compute them. For example, imagine we are interested in computing the cross-comparison between the two rows below.\n\nsummary_df.iloc[:2]\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ncamper\npersons\nchild\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0, 1)\n0\n1\n1\n0.214363\n0.144309\n0.287735\n\n\n1\nlivebait\ndiff\n(0, 1)\n0\n1\n2\n0.053678\n0.029533\n0.077615\n\n\n\n\n\n\n\nThe cross-comparison amounts to first computing the comparison for row 0, given below, and can be verified by looking at the estimate in summary_df.\n\ncond_10 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 1,\n    \"livebait\": 0 \n}\n\ncond_11 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 1,\n    \"livebait\": 1\n}\n\ndraws_10 = select_draws(idata_grid, grid, cond_10, \"count_mean\")\ndraws_11 = select_draws(idata_grid, grid, cond_11, \"count_mean\")\n\n(draws_11 - draws_10).mean((\"chain\", \"draw\")).item()\n\n0.2143627093182434\n\n\nNext, we need to compute the comparison for row 1.\n\ncond_20 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 2,\n    \"livebait\": 0\n}\n\ncond_21 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 2,\n    \"livebait\": 1\n}\n\ndraws_20 = select_draws(idata_grid, grid, cond_20, \"count_mean\")\ndraws_21 = select_draws(idata_grid, grid, cond_21, \"count_mean\")\n\n\n(draws_21 - draws_20).mean((\"chain\", \"draw\")).item()\n\n0.053678256991883604\n\n\nNext, we compute the “first level” comparisons (diff_1 and diff_2). Subsequently, we compute the difference between these two differences to obtain the cross-comparison.\n\ndiff_1 = (draws_11 - draws_10)\ndiff_2 = (draws_21 - draws_20)\n\ncross_comparison = (diff_2 - diff_1).mean((\"chain\", \"draw\")).item()\ncross_comparison\n\n-0.16068445232635978\n\n\nTo verify this is correct, we can check by performing the cross-comparison directly on the summary_df.\n\nsummary_df.iloc[1][\"estimate\"] - summary_df.iloc[0][\"estimate\"]\n\n-0.16068445232635978\n\n\n\n\n\n\nIn this notebook, the interpret helper functions data_grid and select_draws were introduced and it was demonstrated how they can be used to compute pairwise grids of data and cross-comparisons. With these functions, it is left to the user to generate their grids of data and quantities of interest allowing for more flexibility and control over the type of data passed to model.predict and the quantities of interest computed.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Tue Dec 05 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\nnumpy : 1.24.2\npandas: 2.1.0\nbambi : 0.13.0.dev0\narviz : 0.16.1\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2023-12-09-interpret-advanced-usage.html#zero-inflated-poisson",
    "href": "posts/2023-12-09-interpret-advanced-usage.html#zero-inflated-poisson",
    "title": "Advanced Interpret Usage in Bambi",
    "section": "",
    "text": "We will adopt the zero inflated Poisson (ZIP) model from the comparisons documentation to demonstrate the helper functions introduced above.\nThe ZIP model will be used to predict how many fish are caught by visitors at a state park using survey data. Many visitors catch zero fish, either because they did not fish at all, or because they were unlucky. We would like to explicitly model this bimodal behavior (zero versus non-zero) using a Zero Inflated Poisson model, and to compare how different inputs of interest \\(w\\) and other covariate values \\(c\\) are associated with the number of fish caught. The dataset contains data on 250 groups that went to a state park to fish. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), if they used a live bait and whether or not they brought a camper to the park (camper).\n\nfish_data = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\ncols = [\"count\", \"livebait\", \"camper\", \"persons\", \"child\"]\nfish_data = fish_data[cols]\nfish_data[\"child\"] = fish_data[\"child\"].astype(np.int8)\nfish_data[\"persons\"] = fish_data[\"persons\"].astype(np.int8)\nfish_data[\"livebait\"] = pd.Categorical(fish_data[\"livebait\"])\nfish_data[\"camper\"] = pd.Categorical(fish_data[\"camper\"])\n\n\nfish_model = bmb.Model(\n    \"count ~ livebait + camper + persons + child\", \n    fish_data, \n    family='zero_inflated_poisson'\n)\n\nfish_idata = fish_model.fit(random_seed=1234)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [count_psi, Intercept, livebait, camper, persons, child]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds."
  },
  {
    "objectID": "posts/2023-12-09-interpret-advanced-usage.html#create-a-grid-of-data",
    "href": "posts/2023-12-09-interpret-advanced-usage.html#create-a-grid-of-data",
    "title": "Advanced Interpret Usage in Bambi",
    "section": "",
    "text": "data_grid allows you to create a pairwise grid, also known as a cross-join or cartesian product, of data using the covariates passed to the conditional and the optional variable parameter. Covariates not passed to conditional, but are terms in the Bambi model, are set to typical values (e.g., mean or mode). If you are coming from R, this function is partially inspired from the data_grid function in {modelr}.\nThere are two ways to create a pairwise grid of data:\n\nuser-provided values are passed as a dictionary to conditional where the keys are the names of the covariates and the values are the values to use in the grid.\na list of covariates where the elements are the names of the covariates to use in the grid. As only the names of the covariates were passed, default values are computed to construct the grid.\n\nAny unspecified covariates, i.e., covariates not passed to conditional but are terms in the Bambi model, are set to their “typical” values such as mean or mode depending on the data type of the covariate.\n\n\nTo construct a pairwise grid of data for specific covariate values, pass a dictionary to conditional. The values of the dictionary can be of type int, float, list, or np.ndarray.\n\nconditional = {\n    \"camper\": np.array([0, 1]),\n    \"persons\": np.arange(1, 5, 1),\n    \"child\": np.array([1, 2, 3]),\n}\nuser_passed_grid = data_grid(fish_model, conditional)\nuser_passed_grid.query(\"camper == 0\")\n\nDefault computed for unspecified variable: livebait\n\n\n\n\n\n\n\n\n\ncamper\npersons\nchild\nlivebait\n\n\n\n\n0\n0\n1\n1\n1.0\n\n\n1\n0\n1\n2\n1.0\n\n\n2\n0\n1\n3\n1.0\n\n\n3\n0\n2\n1\n1.0\n\n\n4\n0\n2\n2\n1.0\n\n\n5\n0\n2\n3\n1.0\n\n\n6\n0\n3\n1\n1.0\n\n\n7\n0\n3\n2\n1.0\n\n\n8\n0\n3\n3\n1.0\n\n\n9\n0\n4\n1\n1.0\n\n\n10\n0\n4\n2\n1.0\n\n\n11\n0\n4\n3\n1.0\n\n\n\n\n\n\n\nSubsetting by camper = 0, it can be seen that a combination of all possible pairs of values from the dictionary (including the unspecified variable livebait) results in a dataframe containing every possible combination of values from the original sets. livebait has been set to 1 as this is the mode of the unspecified categorical variable.\n\n\n\nAlternatively, a list of covariates can be passed to conditional where the elements are the names of the covariates to use in the grid. By doing this, you are telling interpret to compute default values for these covariates. The psuedocode below outlines the logic and functions used to compute these default values:\nif is_numeric_dtype(x) or is_float_dtype(x):\n    values = np.linspace(np.min(x), np.max(x), 50)\n\nelif is_integer_dtype(x):\n    values = np.quantile(x, np.linspace(0, 1, 5))\n\nelif is_categorical_dtype(x) or is_string_dtype(x) or is_object_dtype(x):\n    values = np.unique(x)\n\nconditional = [\"camper\", \"persons\", \"child\"]\ndefault_grid = data_grid(fish_model, conditional)\n\ndefault_grid.shape, user_passed_grid.shape\n\nDefault computed for conditional variable: camper, persons, child\nDefault computed for unspecified variable: livebait\n\n\n((32, 4), (24, 4))\n\n\nNotice how the resulting length is different between the user passed and default grid. This is due to the fact that values for child range from 0 to 3 for the default grid.\n\ndefault_grid.query(\"camper == 0\")\n\n\n\n\n\n\n\n\ncamper\npersons\nchild\nlivebait\n\n\n\n\n0\n0.0\n1\n0\n1.0\n\n\n1\n0.0\n1\n1\n1.0\n\n\n2\n0.0\n1\n2\n1.0\n\n\n3\n0.0\n1\n3\n1.0\n\n\n4\n0.0\n2\n0\n1.0\n\n\n5\n0.0\n2\n1\n1.0\n\n\n6\n0.0\n2\n2\n1.0\n\n\n7\n0.0\n2\n3\n1.0\n\n\n8\n0.0\n3\n0\n1.0\n\n\n9\n0.0\n3\n1\n1.0\n\n\n10\n0.0\n3\n2\n1.0\n\n\n11\n0.0\n3\n3\n1.0\n\n\n12\n0.0\n4\n0\n1.0\n\n\n13\n0.0\n4\n1\n1.0\n\n\n14\n0.0\n4\n2\n1.0\n\n\n15\n0.0\n4\n3\n1.0"
  },
  {
    "objectID": "posts/2023-12-09-interpret-advanced-usage.html#compute-comparisons",
    "href": "posts/2023-12-09-interpret-advanced-usage.html#compute-comparisons",
    "title": "Advanced Interpret Usage in Bambi",
    "section": "",
    "text": "To use data_grid to help generate data in computing comparisons or slopes, additional data is passed to the optional variable parameter. The name variable is an abstraction for the comparisons parameter contrast and slopes parameter wrt. If you have used any of the interpret functions, these parameter names should be familiar and the use of data_grid should be analogous to comparisons, predictions, and slopes.\nvariable can also be passed user-provided data (as a dictionary), or a string indicating the name of the covariate of interest. If the latter, a default value will be computed. Additionally, if an argument is passed for variable, then the effect_type needs to be passed. This is because for comparisons and slopes an epsilon value eps needs to be determined to compute the centered and finite difference, respectively. You can also pass a value for eps as a kwarg.\n\nconditional = {\n    \"camper\": np.array([0, 1]),\n    \"persons\": np.arange(1, 5, 1),\n    \"child\": np.array([1, 2, 3, 4])\n}\nvariable = \"livebait\"\n\ngrid = data_grid(fish_model, conditional, variable, effect_type=\"comparisons\")\n\nDefault computed for contrast variable: livebait\n\n\n\nidata_grid = fish_model.predict(fish_idata, data=grid, inplace=False)\n\n\n\nThe second helper function to aid in more advanced analysis is select_draws. This is a function that selects the posterior or posterior predictive draws from the ArviZ InferenceData object returned by model.predict given a conditional dictionary. The conditional dictionary represents the values that correspond to that draw.\nFor example, if we wanted to select posterior draws where livebait = [0, 1], then all we need to do is pass a dictionary where the key is the name of the covariate and the value is the value that we want to condition on (or select). The resulting InferenceData object will contain the draws that correspond to the data points where livebait = [0, 1]. Additionally, you must pass the InferenceData object returned by model.predict, the data used to generate the predictions, and the name of the data variable data_var you would like to select from the InferenceData posterior group. If you specified to return the posterior predictive samples by passing model.predict(..., kind=\"pps\"), you can use this group instead of the posterior group by passing group=\"posterior_predictive\".\nBelow, it is demonstrated how to compute comparisons for count_mean for the contrast livebait = [0, 1] using the posterior draws.\n\nidata_grid\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:       (chain: 4, draw: 1000, livebait_dim: 1, camper_dim: 1,\n                   count_obs: 64)\nCoordinates:\n  * chain         (chain) int64 0 1 2 3\n  * draw          (draw) int64 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\n  * livebait_dim  (livebait_dim) &lt;U3 '1.0'\n  * camper_dim    (camper_dim) &lt;U3 '1.0'\n  * count_obs     (count_obs) int64 0 1 2 3 4 5 6 7 ... 56 57 58 59 60 61 62 63\nData variables:\n    Intercept     (chain, draw) float64 -2.454 -2.31 -2.91 ... -2.652 -2.887\n    livebait      (chain, draw, livebait_dim) float64 1.629 1.58 ... 1.799 1.967\n    camper        (chain, draw, camper_dim) float64 0.7037 0.7089 ... 0.7128\n    persons       (chain, draw) float64 0.8707 0.8369 0.9457 ... 0.8847 0.912\n    child         (chain, draw) float64 -1.345 -1.412 -1.418 ... -1.293 -1.573\n    count_psi     (chain, draw) float64 0.6311 0.6201 0.6342 ... 0.6768 0.5745\n    count_mean    (chain, draw, count_obs) float64 0.05349 0.2728 ... 0.05777\nAttributes:\n    created_at:                  2023-12-09T06:01:42.032935\n    arviz_version:               0.16.1\n    inference_library:           pymc\n    inference_library_version:   5.8.1\n    sampling_time:               2.652681827545166\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0.dev0xarray.DatasetDimensions:chain: 4draw: 1000livebait_dim: 1camper_dim: 1count_obs: 64Coordinates: (5)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])livebait_dim(livebait_dim)&lt;U3'1.0'array(['1.0'], dtype='&lt;U3')camper_dim(camper_dim)&lt;U3'1.0'array(['1.0'], dtype='&lt;U3')count_obs(count_obs)int640 1 2 3 4 5 6 ... 58 59 60 61 62 63array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63])Data variables: (7)Intercept(chain, draw)float64-2.454 -2.31 ... -2.652 -2.887array([[-2.45369792, -2.31017959, -2.91023957, ..., -2.12445232,\n        -2.13810127, -2.40815127],\n       [-2.21843503, -2.81570517, -2.81570517, ..., -3.23806838,\n        -2.43778314, -2.40094452],\n       [-2.57030767, -2.71197967, -3.13096258, ..., -2.75122519,\n        -2.81845915, -2.77702878],\n       [-2.27327526, -2.76201826, -2.72978474, ..., -2.43555099,\n        -2.65150277, -2.88742403]])livebait(chain, draw, livebait_dim)float641.629 1.58 1.858 ... 1.799 1.967array([[[1.62921077],\n        [1.57971577],\n        [1.85814553],\n        ...,\n        [1.39320044],\n        [1.31938529],\n        [2.0303655 ]],\n\n       [[1.44441014],\n        [2.03180494],\n        [2.03180494],\n        ...,\n        [2.00383874],\n        [2.0219552 ],\n        [1.77554631]],\n\n       [[1.44183823],\n        [2.12560898],\n        [1.84301056],\n        ...,\n        [1.65875902],\n        [1.69372851],\n        [1.78166201]],\n\n       [[1.64438961],\n        [1.88379206],\n        [1.95128032],\n        ...,\n        [1.73718918],\n        [1.79946112],\n        [1.96668979]]])camper(chain, draw, camper_dim)float640.7037 0.7089 ... 0.6445 0.7128array([[[0.70368097],\n        [0.70888183],\n        [0.6432408 ],\n        ...,\n        [0.64457622],\n        [0.68265053],\n        [0.59371655]],\n\n       [[0.73004824],\n        [0.63806292],\n        [0.63806292],\n        ...,\n        [0.92181193],\n        [0.44431118],\n        [0.52257123]],\n\n       [[0.67251782],\n        [0.61970006],\n        [0.80110109],\n        ...,\n        [0.71626836],\n        [0.7386686 ],\n        [0.80126267]],\n\n       [[0.68695406],\n        [0.66147853],\n        [0.7129354 ],\n        ...,\n        [0.62523222],\n        [0.64449304],\n        [0.71282881]]])persons(chain, draw)float640.8707 0.8369 ... 0.8847 0.912array([[0.87073326, 0.83691824, 0.94570525, ..., 0.86127639, 0.85807333,\n        0.7827932 ],\n       [0.85684603, 0.88279463, 0.88279463, ..., 0.93383647, 0.82809407,\n        0.84387704],\n       [0.93868593, 0.83511835, 0.9966305 , ..., 0.92864502, 0.9583216 ,\n        0.9046778 ],\n       [0.82278573, 0.89846761, 0.85979591, ..., 0.85493891, 0.88465389,\n        0.91197904]])child(chain, draw)float64-1.345 -1.412 ... -1.293 -1.573array([[-1.34537519, -1.41232517, -1.41808649, ..., -1.28364874,\n        -1.21311483, -1.343049  ],\n       [-1.36022475, -1.38360737, -1.38360737, ..., -1.34137848,\n        -1.37005354, -1.32761054],\n       [-1.42361978, -1.45390876, -1.59921868, ..., -1.57170719,\n        -1.60156825, -1.49976267],\n       [-1.33972561, -1.29366616, -1.38679881, ..., -1.29405166,\n        -1.29343353, -1.57284431]])count_psi(chain, draw)float640.6311 0.6201 ... 0.6768 0.5745array([[0.63114698, 0.62012886, 0.63423439, ..., 0.57386427, 0.56121415,\n        0.57771985],\n       [0.61291154, 0.607627  , 0.607627  , ..., 0.63015121, 0.54061116,\n        0.54315861],\n       [0.60778057, 0.68015035, 0.57385421, ..., 0.75656914, 0.74951665,\n        0.68320354],\n       [0.61202421, 0.58032736, 0.57834964, ..., 0.65743098, 0.67681655,\n        0.57454481]])count_mean(chain, draw, count_obs)float640.05349 0.2728 ... 0.008082 0.05777array([[[0.05348576, 0.27276925, 0.01392994, ..., 0.50966648,\n         0.02602794, 0.13273855],\n        [0.05582204, 0.27093652, 0.01359692, ..., 0.40216834,\n         0.02018278, 0.09795866],\n        [0.03395834, 0.21773528, 0.00822393, ..., 0.41466193,\n         0.01566191, 0.10042158],\n        ...,\n        [0.07833   , 0.31549128, 0.02169934, ..., 0.61108678,\n         0.04203026, 0.16928611],\n        [0.08264981, 0.30920293, 0.0245693 , ..., 0.70955544,\n         0.05638135, 0.21092947],\n        [0.0513851 , 0.3913936 , 0.013414  , ..., 0.50558281,\n         0.01732754, 0.13198164]],\n\n       [[0.06575538, 0.27876013, 0.01687303, ..., 0.49794444,\n         0.03014001, 0.12777409],\n        [0.03627894, 0.27673   , 0.00909414, ..., 0.46511021,\n         0.01528485, 0.11659041],\n        [0.03627894, 0.27673   , 0.00909414, ..., 0.46511021,\n         0.01528485, 0.11659041],\n...\n        [0.03356447, 0.17630703, 0.00697101, ..., 0.25240022,\n         0.00997967, 0.05242108],\n        [0.03137619, 0.17067787, 0.00632482, ..., 0.25730828,\n         0.00953508, 0.05186824],\n        [0.03431703, 0.20383353, 0.00765898, ..., 0.34140667,\n         0.01282825, 0.07619621]],\n\n       [[0.061408  , 0.31796133, 0.01608383, ..., 0.51172625,\n         0.02588528, 0.13403007],\n        [0.04254398, 0.27987149, 0.01166826, ..., 0.60418447,\n         0.02518935, 0.16570571],\n        [0.03851191, 0.271035  , 0.00962312, ..., 0.45530771,\n         0.01616574, 0.11376952],\n        ...,\n        [0.05643511, 0.32062773, 0.01547212, ..., 0.5853596 ,\n         0.02824695, 0.16048086],\n        [0.04687446, 0.28342116, 0.01285894, ..., 0.57739214,\n         0.02619653, 0.1583944 ],\n        [0.02877382, 0.2056459 , 0.00596925, ..., 0.27844848,\n         0.00808248, 0.05776533]]])Indexes: (5)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))livebait_dimPandasIndexPandasIndex(Index(['1.0'], dtype='object', name='livebait_dim'))camper_dimPandasIndexPandasIndex(Index(['1.0'], dtype='object', name='camper_dim'))count_obsPandasIndexPandasIndex(Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n       36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n       54, 55, 56, 57, 58, 59, 60, 61, 62, 63],\n      dtype='int64', name='count_obs'))Attributes: (8)created_at :2023-12-09T06:01:42.032935arviz_version :0.16.1inference_library :pymcinference_library_version :5.8.1sampling_time :2.652681827545166tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0.dev0\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:                (chain: 4, draw: 1000)\nCoordinates:\n  * chain                  (chain) int64 0 1 2 3\n  * draw                   (draw) int64 0 1 2 3 4 5 ... 994 995 996 997 998 999\nData variables: (12/17)\n    largest_eigval         (chain, draw) float64 nan nan nan nan ... nan nan nan\n    max_energy_error       (chain, draw) float64 0.595 1.645 ... -0.2578 0.4279\n    tree_depth             (chain, draw) int64 3 3 3 3 3 3 3 3 ... 3 3 3 3 3 4 4\n    process_time_diff      (chain, draw) float64 0.000894 0.000888 ... 0.001654\n    diverging              (chain, draw) bool False False False ... False False\n    step_size_bar          (chain, draw) float64 0.4502 0.4502 ... 0.4541 0.4541\n    ...                     ...\n    acceptance_rate        (chain, draw) float64 0.7512 0.5492 ... 0.9919 0.8264\n    n_steps                (chain, draw) float64 7.0 7.0 7.0 ... 7.0 11.0 15.0\n    energy                 (chain, draw) float64 751.9 752.2 ... 755.7 756.8\n    index_in_trajectory    (chain, draw) int64 6 4 -6 -5 -2 4 ... 7 1 4 4 -1 9\n    lp                     (chain, draw) float64 -750.1 -750.2 ... -751.9 -754.0\n    energy_error           (chain, draw) float64 0.04379 0.02705 ... 0.415\nAttributes:\n    created_at:                  2023-12-09T06:01:42.040761\n    arviz_version:               0.16.1\n    inference_library:           pymc\n    inference_library_version:   5.8.1\n    sampling_time:               2.652681827545166\n    tuning_steps:                1000\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0.dev0xarray.DatasetDimensions:chain: 4draw: 1000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (17)largest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])max_energy_error(chain, draw)float640.595 1.645 ... -0.2578 0.4279array([[ 0.595031  ,  1.64520146,  0.21527509, ...,  0.78075972,\n         5.80798519,  0.43782396],\n       [-0.37301973, -0.20402685,  2.5627162 , ...,  0.36741839,\n         3.6828103 , -0.50001872],\n       [ 1.27621281, -0.68221199,  0.44120051, ...,  2.43620146,\n        -0.08386685, -0.22408853],\n       [ 0.14004462,  0.99694174, -0.07864808, ...,  0.22969662,\n        -0.25775022,  0.42794799]])tree_depth(chain, draw)int643 3 3 3 3 3 3 3 ... 3 3 3 3 3 3 4 4array([[3, 3, 3, ..., 3, 2, 3],\n       [3, 3, 3, ..., 3, 4, 3],\n       [3, 3, 3, ..., 3, 2, 3],\n       [3, 3, 4, ..., 3, 4, 4]])process_time_diff(chain, draw)float640.000894 0.000888 ... 0.001654array([[0.000894, 0.000888, 0.000882, ..., 0.00088 , 0.000451, 0.000883],\n       [0.000893, 0.000889, 0.000886, ..., 0.000891, 0.001756, 0.000893],\n       [0.000895, 0.000891, 0.000886, ..., 0.000868, 0.00044 , 0.000865],\n       [0.000894, 0.000895, 0.001761, ..., 0.000882, 0.001246, 0.001654]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])step_size_bar(chain, draw)float640.4502 0.4502 ... 0.4541 0.4541array([[0.45015324, 0.45015324, 0.45015324, ..., 0.45015324, 0.45015324,\n        0.45015324],\n       [0.42774415, 0.42774415, 0.42774415, ..., 0.42774415, 0.42774415,\n        0.42774415],\n       [0.44696235, 0.44696235, 0.44696235, ..., 0.44696235, 0.44696235,\n        0.44696235],\n       [0.45405445, 0.45405445, 0.45405445, ..., 0.45405445, 0.45405445,\n        0.45405445]])perf_counter_start(chain, draw)float641.067e+03 1.067e+03 ... 1.069e+03array([[1067.40390204, 1067.40486017, 1067.40580821, ..., 1068.46292929,\n        1068.46387517, 1068.46438762],\n       [1067.41511467, 1067.41606821, 1067.41704075, ..., 1068.53776558,\n        1068.53871471, 1068.54055937],\n       [1067.46033633, 1067.461301  , 1067.46227354, ..., 1068.56037796,\n        1068.56130604, 1068.56180675],\n       [1067.44745625, 1067.44842704, 1067.44939542, ..., 1068.58570321,\n        1068.58666521, 1068.58797033]])step_size(chain, draw)float640.415 0.415 0.415 ... 0.3918 0.3918array([[0.41500359, 0.41500359, 0.41500359, ..., 0.41500359, 0.41500359,\n        0.41500359],\n       [0.42993239, 0.42993239, 0.42993239, ..., 0.42993239, 0.42993239,\n        0.42993239],\n       [0.55799239, 0.55799239, 0.55799239, ..., 0.55799239, 0.55799239,\n        0.55799239],\n       [0.39177214, 0.39177214, 0.39177214, ..., 0.39177214, 0.39177214,\n        0.39177214]])perf_counter_diff(chain, draw)float640.000894 0.0008885 ... 0.001654array([[0.000894  , 0.00088854, 0.00088317, ..., 0.00087933, 0.00045042,\n        0.00088333],\n       [0.00089242, 0.00088767, 0.00088663, ..., 0.00089146, 0.00178467,\n        0.0008925 ],\n       [0.00089392, 0.00089162, 0.00088608, ..., 0.00086804, 0.00044117,\n        0.00086496],\n       [0.00089838, 0.00089992, 0.00178875, ..., 0.00090033, 0.00124583,\n        0.00165379]])reached_max_treedepth(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])smallest_eigval(chain, draw)float64nan nan nan nan ... nan nan nan nanarray([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])acceptance_rate(chain, draw)float640.7512 0.5492 ... 0.9919 0.8264array([[0.75118358, 0.54917103, 0.90600545, ..., 0.64640509, 0.24651666,\n        0.84703996],\n       [1.        , 1.        , 0.18921988, ..., 0.81098596, 0.34035628,\n        0.99696537],\n       [0.56560841, 0.95846614, 0.85780695, ..., 0.50105863, 0.99947793,\n        1.        ],\n       [0.9683953 , 0.61509461, 0.99814479, ..., 0.9439049 , 0.99189283,\n        0.82635188]])n_steps(chain, draw)float647.0 7.0 7.0 7.0 ... 7.0 11.0 15.0array([[ 7.,  7.,  7., ...,  7.,  3.,  7.],\n       [ 7.,  7.,  7., ...,  7., 15.,  7.],\n       [ 7.,  7.,  7., ...,  7.,  3.,  7.],\n       [ 7.,  7., 15., ...,  7., 11., 15.]])energy(chain, draw)float64751.9 752.2 751.9 ... 755.7 756.8array([[751.94493597, 752.16775838, 751.86589668, ..., 757.02411223,\n        757.59305462, 755.76371432],\n       [754.28875075, 752.65123698, 755.84607135, ..., 759.92489206,\n        761.1929198 , 755.68207248],\n       [755.10969541, 756.38178297, 761.18044623, ..., 762.22995695,\n        756.69732237, 756.35418424],\n       [753.32406793, 752.80673891, 752.03157068, ..., 756.22896884,\n        755.6715562 , 756.78221912]])index_in_trajectory(chain, draw)int646 4 -6 -5 -2 4 -5 ... 7 1 4 4 -1 9array([[ 6,  4, -6, ...,  4,  2,  5],\n       [ 4,  7,  0, ..., -6,  9, -2],\n       [ 3,  5,  5, ..., -4, -2,  4],\n       [-3, -4, -3, ...,  4, -1,  9]])lp(chain, draw)float64-750.1 -750.2 ... -751.9 -754.0array([[-750.0635191 , -750.19379111, -751.09897583, ..., -751.87190426,\n        -752.95441117, -754.23172742],\n       [-751.17173283, -750.81535676, -750.81535676, ..., -755.33966632,\n        -755.44844073, -752.33208106],\n       [-754.06157469, -753.90911474, -759.1519945 , ..., -755.85090606,\n        -755.67682047, -752.00709578],\n       [-750.85175653, -751.04878911, -750.6370331 , ..., -751.95509665,\n        -751.89343509, -753.95045712]])energy_error(chain, draw)float640.04379 0.02705 ... -0.1224 0.415array([[ 4.37855069e-02,  2.70547404e-02, -5.88277115e-04, ...,\n         8.64705248e-02,  3.20765941e-01,  4.37823957e-01],\n       [-9.78110746e-02, -4.02245905e-02,  0.00000000e+00, ...,\n         2.05920633e-01,  2.91692817e-01, -2.47079734e-01],\n       [ 1.14647984e+00, -6.82211987e-01,  4.41200505e-01, ...,\n        -3.52379476e-01,  1.56742695e-03, -1.73449488e-01],\n       [ 1.45574755e-02, -4.02199593e-03, -5.80109339e-02, ...,\n        -9.46547980e-03, -1.22382092e-01,  4.15048801e-01]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (8)created_at :2023-12-09T06:01:42.040761arviz_version :0.16.1inference_library :pymcinference_library_version :5.8.1sampling_time :2.652681827545166tuning_steps :1000modeling_interface :bambimodeling_interface_version :0.13.0.dev0\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt;\nDimensions:    (count_obs: 250)\nCoordinates:\n  * count_obs  (count_obs) int64 0 1 2 3 4 5 6 7 ... 243 244 245 246 247 248 249\nData variables:\n    count      (count_obs) int64 0 0 0 0 1 0 0 0 0 1 0 ... 4 1 1 0 1 0 0 0 0 0 0\nAttributes:\n    created_at:                  2023-12-09T06:01:42.043721\n    arviz_version:               0.16.1\n    inference_library:           pymc\n    inference_library_version:   5.8.1\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.0.dev0xarray.DatasetDimensions:count_obs: 250Coordinates: (1)count_obs(count_obs)int640 1 2 3 4 5 ... 245 246 247 248 249array([  0,   1,   2, ..., 247, 248, 249])Data variables: (1)count(count_obs)int640 0 0 0 1 0 0 0 ... 0 1 0 0 0 0 0 0array([  0,   0,   0,   0,   1,   0,   0,   0,   0,   1,   0,   0,   1,\n         2,   0,   1,   0,   0,   1,   0,   1,   5,   0,   3,  30,   0,\n        13,   0,   0,   0,   0,   0,  11,   5,   0,   1,   1,   7,   0,\n        14,   0,  32,   0,   1,   0,   0,   0,   1,   5,   0,   1,   0,\n        22,   0,  15,   0,   0,   0,   5,   4,   2,   0,   2,  32,   0,\n         0,   1,   0,   0,   0,   7,   0,   0,   0,   0,   0,   0,   0,\n         0,   2,   3,   1,   5,   0,   2,   1,   0,   1, 149,   0,   1,\n         0,   0,   1,   0,   0,   0,   2,   2,  29,   3,   0,   0,   5,\n         0,   0,   0,   0,   0,   1,   7,   1,   0,   2,   0,   2,   0,\n         0,   0,   1,   0,   0,   0,   0,   0,   3,   4,   3,   3,   8,\n         2,   1,   6,   0,   0,   5,   3,  31,   0,   2,   0,   0,   0,\n         0,   0,   0,   6,   9,   0,   0,   0,   0,   0,   2,  15,   1,\n         2,   3,   0,  65,   5,   0,   0,   0,   0,   1,   8,   0,   0,\n         0,   2,   4,   5,   9,   0,   0,   0,   0,  21,   0,   6,   0,\n         0,   0,   0,  16,   0,   0,   4,   2,  10,   0,   0,   0,   2,\n         1,   3,   0,   0,  21,   0,   0,   2,   0,   3,   0,  38,   0,\n         0,   0,   1,   3,   0,   1,   0,   0,   0,   0,   5,   0,   0,\n         2,   0,   0,   0,   1,   4,   0,   0,   2,   3,   0,   0,   0,\n         0,   1,   2,   0,   6,   4,   1,   1,   0,   1,   0,   0,   0,\n         0,   0,   0])Indexes: (1)count_obsPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n      dtype='int64', name='count_obs', length=250))Attributes: (6)created_at :2023-12-09T06:01:42.043721arviz_version :0.16.1inference_library :pymcinference_library_version :5.8.1modeling_interface :bambimodeling_interface_version :0.13.0.dev0\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\ndraw_1 = select_draws(idata_grid, grid, {\"livebait\": 0}, \"count_mean\")\n\n\ndraw_1 = select_draws(idata_grid, grid, {\"livebait\": 0}, \"count_mean\")\ndraw_2 = select_draws(idata_grid, grid, {\"livebait\": 1}, \"count_mean\")\n\ncomparison_mean = (draw_2 - draw_1).mean((\"chain\", \"draw\"))\ncomparison_hdi = az.hdi(draw_2 - draw_1)\n\ncomparison_df = pd.DataFrame(\n    {\n        \"mean\": comparison_mean.values,\n        \"hdi_low\": comparison_hdi.sel(hdi=\"lower\")[\"count_mean\"].values,\n        \"hdi_high\": comparison_hdi.sel(hdi=\"higher\")[\"count_mean\"].values,\n    }\n)\ncomparison_df.head(10)\n\n\n\n\n\n\n\n\nmean\nhdi_low\nhdi_high\n\n\n\n\n0\n0.214363\n0.144309\n0.287735\n\n\n1\n0.053678\n0.029533\n0.077615\n\n\n2\n0.013558\n0.006332\n0.021971\n\n\n3\n0.003454\n0.001132\n0.006040\n\n\n4\n0.512709\n0.369741\n0.661034\n\n\n5\n0.128316\n0.077068\n0.181741\n\n\n6\n0.032392\n0.015553\n0.050690\n\n\n7\n0.008247\n0.003047\n0.014382\n\n\n8\n1.228708\n0.913514\n1.533121\n\n\n9\n0.307342\n0.192380\n0.426808\n\n\n\n\n\n\n\nWe can compare this comparison with bmb.interpret.comparisons.\n\nsummary_df = bmb.interpret.comparisons(\n    fish_model,\n    fish_idata,\n    contrast={\"livebait\": [0, 1]},\n    conditional=conditional\n)\nsummary_df.head(10)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ncamper\npersons\nchild\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0, 1)\n0\n1\n1\n0.214363\n0.144309\n0.287735\n\n\n1\nlivebait\ndiff\n(0, 1)\n0\n1\n2\n0.053678\n0.029533\n0.077615\n\n\n2\nlivebait\ndiff\n(0, 1)\n0\n1\n3\n0.013558\n0.006332\n0.021971\n\n\n3\nlivebait\ndiff\n(0, 1)\n0\n1\n4\n0.003454\n0.001132\n0.006040\n\n\n4\nlivebait\ndiff\n(0, 1)\n0\n2\n1\n0.512709\n0.369741\n0.661034\n\n\n5\nlivebait\ndiff\n(0, 1)\n0\n2\n2\n0.128316\n0.077068\n0.181741\n\n\n6\nlivebait\ndiff\n(0, 1)\n0\n2\n3\n0.032392\n0.015553\n0.050690\n\n\n7\nlivebait\ndiff\n(0, 1)\n0\n2\n4\n0.008247\n0.003047\n0.014382\n\n\n8\nlivebait\ndiff\n(0, 1)\n0\n3\n1\n1.228708\n0.913514\n1.533121\n\n\n9\nlivebait\ndiff\n(0, 1)\n0\n3\n2\n0.307342\n0.192380\n0.426808\n\n\n\n\n\n\n\nAlbeit the other information in the summary_df, the columns estimate, lower_3.0%, upper_97.0% are identical.\n\n\n\nComputing a cross-comparison is useful for when we want to compare contrasts when two (or more) predictors change at the same time. Cross-comparisons are currently not supported in the comparisons function, but we can use select_draws to compute them. For example, imagine we are interested in computing the cross-comparison between the two rows below.\n\nsummary_df.iloc[:2]\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ncamper\npersons\nchild\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\nlivebait\ndiff\n(0, 1)\n0\n1\n1\n0.214363\n0.144309\n0.287735\n\n\n1\nlivebait\ndiff\n(0, 1)\n0\n1\n2\n0.053678\n0.029533\n0.077615\n\n\n\n\n\n\n\nThe cross-comparison amounts to first computing the comparison for row 0, given below, and can be verified by looking at the estimate in summary_df.\n\ncond_10 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 1,\n    \"livebait\": 0 \n}\n\ncond_11 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 1,\n    \"livebait\": 1\n}\n\ndraws_10 = select_draws(idata_grid, grid, cond_10, \"count_mean\")\ndraws_11 = select_draws(idata_grid, grid, cond_11, \"count_mean\")\n\n(draws_11 - draws_10).mean((\"chain\", \"draw\")).item()\n\n0.2143627093182434\n\n\nNext, we need to compute the comparison for row 1.\n\ncond_20 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 2,\n    \"livebait\": 0\n}\n\ncond_21 = {\n    \"camper\": 0,\n    \"persons\": 1,\n    \"child\": 2,\n    \"livebait\": 1\n}\n\ndraws_20 = select_draws(idata_grid, grid, cond_20, \"count_mean\")\ndraws_21 = select_draws(idata_grid, grid, cond_21, \"count_mean\")\n\n\n(draws_21 - draws_20).mean((\"chain\", \"draw\")).item()\n\n0.053678256991883604\n\n\nNext, we compute the “first level” comparisons (diff_1 and diff_2). Subsequently, we compute the difference between these two differences to obtain the cross-comparison.\n\ndiff_1 = (draws_11 - draws_10)\ndiff_2 = (draws_21 - draws_20)\n\ncross_comparison = (diff_2 - diff_1).mean((\"chain\", \"draw\")).item()\ncross_comparison\n\n-0.16068445232635978\n\n\nTo verify this is correct, we can check by performing the cross-comparison directly on the summary_df.\n\nsummary_df.iloc[1][\"estimate\"] - summary_df.iloc[0][\"estimate\"]\n\n-0.16068445232635978"
  },
  {
    "objectID": "posts/2023-12-09-interpret-advanced-usage.html#summary",
    "href": "posts/2023-12-09-interpret-advanced-usage.html#summary",
    "title": "Advanced Interpret Usage in Bambi",
    "section": "",
    "text": "In this notebook, the interpret helper functions data_grid and select_draws were introduced and it was demonstrated how they can be used to compute pairwise grids of data and cross-comparisons. With these functions, it is left to the user to generate their grids of data and quantities of interest allowing for more flexibility and control over the type of data passed to model.predict and the quantities of interest computed.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Tue Dec 05 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\nnumpy : 1.24.2\npandas: 2.1.0\nbambi : 0.13.0.dev0\narviz : 0.16.1\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2023-09-29-ordinal-models-bambi.html",
    "href": "posts/2023-09-29-ordinal-models-bambi.html",
    "title": "Ordinal Models in Bambi",
    "section": "",
    "text": "Code\nimport arviz as az\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport numpy as np\nimport pandas as pd\nimport warnings\n\nimport bambi as bmb\n\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions."
  },
  {
    "objectID": "posts/2023-09-29-ordinal-models-bambi.html#cumulative-model",
    "href": "posts/2023-09-29-ordinal-models-bambi.html#cumulative-model",
    "title": "Ordinal Models in Bambi",
    "section": "Cumulative model",
    "text": "Cumulative model\nA cumulative model assumes that the observed ordinal variable \\(Y\\) originates from the “categorization” of a latent continuous variable \\(Z\\). To model the categorization process, the model assumes that there are \\(K\\) thresholds (or cutpoints) \\(\\tau_k\\) that partition \\(Z\\) into \\(K+1\\) observable, ordered categories of \\(Y\\). The subscript \\(k\\) in \\(\\tau_k\\) is an index that associates that threshold to a particular category \\(k\\). For example, if the response has three categories such as “disagree”, “neither agree nor disagree”, and “agree”, then there are two thresholds \\(\\tau_1\\) and \\(\\tau_2\\) that partition \\(Z\\) into \\(K+1 = 3\\) categories. Additionally, if we assume \\(Z\\) to have a certain distribution (e.g., Normal) with a cumulative distribution function \\(F\\), the probability of \\(Y\\) being equal to category \\(k\\) is\n\\[P(Y = k) = F(\\tau_k) - F(\\tau_{k-1})\\]\nwhere \\(F(\\tau)\\) is a cumulative probability. For example, suppose we are interested in the probability of each category stated above, and have two thresholds \\(\\tau_1 = -1, \\tau_2 = 1\\) for the three categories. Additionally, if we assume \\(Z\\) to be normally distributed with \\(\\sigma = 1\\) and a cumulative distribution function \\(\\Phi\\) then\n\\[P(Y = 1) = \\Phi(\\tau_1) = \\Phi(-1)\\]\n\\[P(Y = 2) = \\Phi(\\tau_2) - \\Phi(\\tau_1) = \\Phi(1) - \\Phi(-1)\\]\n\\[P(Y = 3) = 1 - \\Phi(\\tau_2) = 1 - \\Phi(1)\\]\nBut how to set the values of the thresholds? By default, Bambi uses a Normal distribution with a grid of evenly spaced \\(\\mu\\) that depends on the number of response levels as the prior for the thresholds. Additionally, since the thresholds need to be orderd, Bambi applies a transformation to the values such that the order is preserved. Furthermore, the model specification for ordinal regression typically transforms the cumulative probabilities using the log-cumulative-odds (logit) transformation. Therefore, the learned parameters for the thresholds \\(\\tau\\) will be logits.\nLastly, as each \\(F(\\tau)\\) implies a cumulative probability for each category, the largest response level always has a cumulative probability of 1. Thus, we effectively do not need a parameter for it due to the law of total probability. For example, for three response values, we only need two thresholds as two thresholds partition \\(Z\\) into \\(K+1\\) categories.\n\nThe moral intuition dataset\nTo illustrate an cumulative ordinal model, we will model data from a series of experiments conducted by philsophers (this example comes from Richard McElreath’s Statistical Rethinking). The experiments aim to collect empirical evidence relevant to debates about moral intuition, the forms of reasoning through which people develop judgments about the moral goodness and badness of actions.\nIn the dataset there are 12 columns and 9930 rows, comprising data for 331 unique individuals. The response we are interested in response, is an integer from 1 to 7 indicating how morally permissible the participant found the action to be taken (or not) in the story. The predictors are as follows:\n\naction: a factor with levels 0 and 1 where 1 indicates that the story contained “harm caused by action is morally worse than equivalent harm caused by omission”.\nintention: a factor with levels 0 and 1 where 1 indicates that the story contained “harm intended as the means to a goal is morally worse than equivalent harm foreseen as the side effect of a goal”.\ncontact: a factor with levels 0 and 1 where 1 indicates that the story contained “using physical contact to cause harm to a victim is morally worse than causing equivalent harm to a victim without using physical contact”.\n\n\ntrolly = pd.read_csv(\"https://raw.githubusercontent.com/rmcelreath/rethinking/master/data/Trolley.csv\", sep=\";\")\ntrolly = trolly[[\"response\", \"action\", \"intention\", \"contact\"]]\ntrolly[\"action\"] = pd.Categorical(trolly[\"action\"], ordered=False)\ntrolly[\"intention\"] = pd.Categorical(trolly[\"intention\"], ordered=False)\ntrolly[\"contact\"] = pd.Categorical(trolly[\"contact\"], ordered=False)\ntrolly[\"response\"] = pd.Categorical(trolly[\"response\"], ordered=True)\n\n\n# 7 ordered categories from 1-7\ntrolly.response.unique()\n\n[4, 3, 5, 2, 1, 7, 6]\nCategories (7, int64): [1 &lt; 2 &lt; 3 &lt; 4 &lt; 5 &lt; 6 &lt; 7]\n\n\n\n\nIntercept only model\nBefore we fit a model with predictors, let’s attempt to recover the parameters of an ordinal model using only the thresholds to get a feel for the cumulative family. Traditionally, in Bambi if we wanted to recover the parameters of the likelihood, we would use an intercept only model and write the formula as response ~ 1 where 1 indicates to include the intercept. However, in the case of ordinal regression, the thresholds “take the place” of the intercept. Thus, we can write the formula as response ~ 0 to indicate that we do not want to include an intercept. To fit a cumulative ordinal model, we pass family=\"cumulative\". To compare the thresholds only model, we compute the empirical log-cumulative-odds of the categories directly from the data below and generate a bar plot of the response probabilities.\n\npr_k = trolly.response.value_counts().sort_index().values / trolly.shape[0]\ncum_pr_k = np.cumsum(pr_k)\nlogit_func = lambda x: np.log(x / (1 - x))\ncum_logit = logit_func(cum_pr_k)\ncum_logit\n\n/var/folders/rl/y69t95y51g90tvd6gjzzs59h0000gn/T/ipykernel_22293/1548491577.py:3: RuntimeWarning: invalid value encountered in log\n  logit_func = lambda x: np.log(x / (1 - x))\n\n\narray([-1.91609116, -1.26660559, -0.718634  ,  0.24778573,  0.88986365,\n        1.76938091,         nan])\n\n\n\nplt.figure(figsize=(7, 3))\nplt.bar(np.arange(1, 8), pr_k)\nplt.ylabel(\"Probability\")\nplt.xlabel(\"Response\")\nplt.title(\"Empirical probability of each response category\");\n\n\n\n\n\n\n\n\n\nmodel = bmb.Model(\"response ~ 0\", data=trolly, family=\"cumulative\")\nidata = model.fit(random_seed=1234)\n\nBelow, the components of the model are outputed. Notice how the thresholds are a grid of six values ranging from -2 to 2.\n\nmodel\n\n       Formula: response ~ 0 + action + intention + contact + action:intention + contact:intention\n        Family: cumulative\n          Link: p = logit\n  Observations: 9930\n        Priors: \n    target = p\n        Common-level effects\n            action ~ Normal(mu: 0.0, sigma: 5.045)\n            intention ~ Normal(mu: 0.0, sigma: 5.0111)\n            contact ~ Normal(mu: 0.0, sigma: 6.25)\n            action:intention ~ Normal(mu: 0.0, sigma: 6.7082)\n            contact:intention ~ Normal(mu: 0.0, sigma: 8.3333)\n        \n        Auxiliary parameters\n            threshold ~ Normal(mu: [-2.  -1.2 -0.4  0.4  1.2  2. ], sigma: 1.0, transform: ordered)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\n\naz.summary(idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nresponse_threshold[0]\n-1.917\n0.030\n-1.974\n-1.863\n0.0\n0.0\n4097.0\n3163.0\n1.0\n\n\nresponse_threshold[1]\n-1.267\n0.024\n-1.312\n-1.220\n0.0\n0.0\n5391.0\n3302.0\n1.0\n\n\nresponse_threshold[2]\n-0.719\n0.021\n-0.760\n-0.681\n0.0\n0.0\n5439.0\n3698.0\n1.0\n\n\nresponse_threshold[3]\n0.248\n0.020\n0.211\n0.287\n0.0\n0.0\n5416.0\n3644.0\n1.0\n\n\nresponse_threshold[4]\n0.890\n0.022\n0.847\n0.930\n0.0\n0.0\n4966.0\n3439.0\n1.0\n\n\nresponse_threshold[5]\n1.770\n0.027\n1.721\n1.823\n0.0\n0.0\n4785.0\n3368.0\n1.0\n\n\n\n\n\n\n\nViewing the summary dataframe, we see a total of six response_threshold coefficients. Why six? Remember, we get the last parameter for free. Since there are seven categories, we only need six cutpoints. The index (using zero based indexing) of the response_threshold indicates the category that the threshold is associated with. Comparing to the empirical log-cumulative-odds computation above, the mean of the posterior distribution for each category is close to the empirical value.\nAs the the log cumulative link is used, we need to apply the inverse of the logit function to transform back to cumulative probabilities. Below, we plot the cumulative probabilities for each category.\n\nexpit_func = lambda x: 1 / (1 + np.exp(-x))\ncumprobs = expit_func(idata.posterior.response_threshold).mean((\"chain\", \"draw\"))\ncumprobs = np.append(cumprobs, 1)\n\nplt.figure(figsize=(7, 3))\nplt.plot(sorted(trolly.response.unique()), cumprobs, marker='o')\nplt.ylabel(\"Cumulative probability\")\nplt.xlabel(\"Response category\")\nplt.title(\"Cumulative probabilities of response categories\");\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(7, 3))\nfor i in range(6):\n    outcome = expit_func(idata.posterior.response_threshold).sel(response_threshold_dim=i).to_numpy().flatten()\n    ax.hist(outcome, bins=15, alpha=0.5, label=f\"Category: {i}\")\nax.set_xlabel(\"Probability\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Cumulative Probability by Category\")\nax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\");\n\n\n\n\n\n\n\n\nWe can take the derivative of the cumulative probabilities to get the posterior probabilities for each category. Notice how the posterior probabilities in the barplot below are close to the empirical probabilities in barplot above.\n\n# derivative\nddx = np.diff(cumprobs)\nprobs = np.insert(ddx, 0, cumprobs[0])\n\nplt.figure(figsize=(7, 3))\nplt.bar(sorted(trolly.response.unique()), probs)\nplt.ylabel(\"Probability\")\nplt.xlabel(\"Response category\")\nplt.title(\"Posterior Probability of each response category\");\n\n\n\n\n\n\n\n\nNotice in the plots above, the jump in probability from category 3 to 4. Additionally, the estimates of the coefficients is precise for each category. Now that we have an understanding how the cumulative link function is applied to produce ordered cumulative outcomes, we will add predictors to the model.\n\n\nAdding predictors\nIn the cumulative model described above, adding predictors was explicitly left out. In this section, it is described how predictors are added to ordinal cumulative models. When adding predictor variables, what we would like is for any predictor, as it increases, predictions are moved progressively (increased) through the categories in sequence. A linear regression is formed for \\(Z\\) by adding a predictor term \\(\\eta\\)\n\\[\\eta = \\beta_1 x_1 + \\beta_2 x_2 +, . . ., \\beta_n x_n\\]\nNotice how similar this looks to an ordinary linear model. However, there is no intercept or error term. This is because the intercept is replaced by the threshold \\(\\tau\\) and the error term \\(\\epsilon\\) is added seperately to obtain\n\\[Z = \\eta + \\epsilon\\]\nPutting the predictor term together with the thresholds and cumulative distribution function, we obtain the probability of \\(Y\\) being equal to a category \\(k\\) as\n\\[Pr(Y = k | \\eta) = F(\\tau_k - \\eta) - F(\\tau_{k-1} - \\eta)\\]\nThe same predictor term \\(\\eta\\) is subtracted from each threshold because if we decrease the log-cumulative-odds of every outcome value \\(k\\) below the maximum, this shifts probability mass upwards towards higher outcome values. Thus, positive \\(\\beta\\) values correspond to increasing \\(x\\), which is associated with an increase in the mean response \\(Y\\). The parameters to be estimated from the model are the thresholds \\(\\tau\\) and the predictor terms \\(\\eta\\) coefficients.\nTo add predictors for ordinal models in Bambi, we continue to use the formula interface.\n\nmodel = bmb.Model(\n    \"response ~ 0 + action + intention + contact + action:intention + contact:intention\", \n    data=trolly, \n    family=\"cumulative\"\n)\nidata = model.fit(random_seed=1234)\n\nIn the summary dataframe below, we only select the predictor variables as the thresholds are not of interest at the moment.\nIn the summary dataframe below, we only select the predictor variables as the cutpoints are not of interest at the moment.\n\naz.summary(\n    idata, \n    var_names=[\"action\", \"intention\", \"contact\", \n               \"action:intention\", \"contact:intention\"]\n)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\naction[1]\n-0.466\n0.055\n-0.563\n-0.363\n0.003\n0.002\n412.0\n645.0\n1.00\n\n\nintention[1]\n-0.278\n0.060\n-0.390\n-0.167\n0.003\n0.002\n379.0\n500.0\n1.01\n\n\ncontact[1]\n-0.327\n0.072\n-0.460\n-0.204\n0.003\n0.002\n525.0\n688.0\n1.00\n\n\naction:intention[1, 1]\n-0.450\n0.080\n-0.609\n-0.300\n0.004\n0.003\n396.0\n479.0\n1.00\n\n\ncontact:intention[1, 1]\n-1.278\n0.097\n-1.459\n-1.098\n0.004\n0.003\n557.0\n567.0\n1.00\n\n\n\n\n\n\n\nThe posterior distribution of the slopes are all negative indicating that each of these story features reduces the rating—the acceptability of the story. Below, a forest plot is used to make this insight more clear.\n\naz.plot_forest(\n    idata,\n    combined=True,\n    var_names=[\"action\", \"intention\", \"contact\", \n               \"action:intention\", \"contact:intention\"],\n    figsize=(7, 3),\n    textsize=11\n);\n\n\n\n\n\n\n\n\nAgain, we can plot the cumulative probability of each category. Compared to the same plot above, notice how most of the category probabilities have been shifted to the left. Additionally, there is more uncertainty for category 3, 4, and 5.\n\nfig, ax = plt.subplots(figsize=(7, 3))\nfor i in range(6):\n    outcome = expit_func(idata.posterior.response_threshold).sel(response_threshold_dim=i).to_numpy().flatten()\n    ax.hist(outcome, bins=15, alpha=0.5, label=f\"Category: {i}\")\nax.set_xlabel(\"Probability\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Cumulative Probability by Category\")\nax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\");\n\n\n\n\n\n\n\n\n\n\nPosterior predictive distribution\nTo get a sense of how well the ordinal model fits the data, we can plot samples from the posterior predictive distribution. To plot the samples, a utility function is defined below to assist in the plotting of discrete values.\n\ndef adjust_lightness(color, amount=0.5):\n    import matplotlib.colors as mc\n    import colorsys\n    try:\n        c = mc.cnames[color]\n    except:\n        c = color\n    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n    return colorsys.hls_to_rgb(c[0], c[1] * amount, c[2])\n\ndef plot_ppc_discrete(idata, bins, ax):\n    \n    def add_discrete_bands(x, lower, upper, ax, **kwargs):\n        for i, (l, u) in enumerate(zip(lower, upper)):\n            s = slice(i, i + 2)\n            ax.fill_between(x[s], [l, l], [u, u], **kwargs)\n\n    var_name = list(idata.observed_data.data_vars)[0]\n    y_obs = idata.observed_data[var_name].to_numpy()\n    \n    counts_list = []\n    for draw_values in az.extract(idata, \"posterior_predictive\")[var_name].to_numpy().T:\n        counts, _ = np.histogram(draw_values, bins=bins)\n        counts_list.append(counts)\n    counts_arr = np.stack(counts_list)\n\n    qts_90 = np.quantile(counts_arr, (0.05, 0.95), axis=0)\n    qts_70 = np.quantile(counts_arr, (0.15, 0.85), axis=0)\n    qts_50 = np.quantile(counts_arr, (0.25, 0.75), axis=0)\n    qts_30 = np.quantile(counts_arr, (0.35, 0.65), axis=0)\n    median = np.quantile(counts_arr, 0.5, axis=0)\n\n    colors = [adjust_lightness(\"C0\", x) for x in [1.8, 1.6, 1.4, 1.2, 0.9]]\n\n    add_discrete_bands(bins, qts_90[0], qts_90[1], ax=ax, color=colors[0])\n    add_discrete_bands(bins, qts_70[0], qts_70[1], ax=ax, color=colors[1])\n    add_discrete_bands(bins, qts_50[0], qts_50[1], ax=ax, color=colors[2])\n    add_discrete_bands(bins, qts_30[0], qts_30[1], ax=ax, color=colors[3])\n\n    \n    ax.step(bins[:-1], median, color=colors[4], lw=2, where=\"post\")\n    ax.hist(y_obs, bins=bins, histtype=\"step\", lw=2, color=\"black\", align=\"mid\")\n    handles = [\n        Line2D([], [], label=\"Observed data\", color=\"black\", lw=2),\n        Line2D([], [], label=\"Posterior predictive median\", color=colors[4], lw=2)\n    ]\n    ax.legend(handles=handles)\n    return ax\n\n\nidata_pps = model.predict(idata=idata, kind=\"pps\", inplace=False)\n\nbins = np.arange(7)\nfig, ax = plt.subplots(figsize=(7, 3))\nax = plot_ppc_discrete(idata_pps, bins, ax)\nax.set_xlabel(\"Response category\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Cumulative model - Posterior Predictive Distribution\");"
  },
  {
    "objectID": "posts/2023-09-29-ordinal-models-bambi.html#sequential-model",
    "href": "posts/2023-09-29-ordinal-models-bambi.html#sequential-model",
    "title": "Ordinal Models in Bambi",
    "section": "Sequential Model",
    "text": "Sequential Model\nFor some ordinal variables, the assumption of a single underlying continuous variable (as in cumulative models) may not be appropriate. If the response can be understood as being the result of a sequential process, such that a higher response category is possible only after all lower categories are achieved, then a sequential model may be more appropriate than a cumulative model.\nSequential models assume that for every category \\(k\\) there is a latent continuous variable \\(Z\\) that determines the transition between categories \\(k\\) and \\(k+1\\). Now, a threshold \\(\\tau\\) belongs to each latent process. If there are 3 categories, then there are 3 latent processes. If \\(Z_k\\) is greater than the threshold \\(\\tau_k\\), the sequential process continues, otherwise it stops at category \\(k\\). As with the cumulative model, we assume a distribution for \\(Z_k\\) with a cumulative distribution function \\(F\\).\nAs an example, lets suppose we are interested in modeling the probability a boxer makes it to round 3. This implies that the particular boxer in question survived round 1 \\(Z_1 &gt; \\tau_1\\) , 2 \\(Z_2 &gt; \\tau_2\\), and 3 \\(Z_3 &gt; \\tau_3\\). This can be written as\n\\[Pr(Y = 3) = (1 - P(Z_1 \\leq \\tau_1)) * (1 - P(Z_2 \\leq \\tau_2)) * P(Z_3 \\leq \\tau_3)\\]\nAs in the cumulative model above, if we assume \\(Y\\) to be normally distributed with the thresholds \\(\\tau_1 = -1, \\tau_2 = 0, \\tau_3 = 1\\) and cumulative distribution function \\(\\Phi\\) then\n\\[Pr(Y = 3) = (1 - \\Phi(\\tau_1)) * (1 - \\Phi(\\tau_2)) * \\Phi(\\tau_3)\\]\nTo add predictors to this sequential model, we follow the same specification in the Adding Predictors section above. Thus, the sequential model with predictor terms becomes\n\\[P(Y = k) = F(\\tau_k - \\eta) * \\prod_{j=1}^{k-1}{(1 - F(\\tau_j - \\eta))}\\]\nThus, the probability that \\(Y\\) is equal to category \\(k\\) is equal to the probability that it did not fall in one of the former categories \\(1: k-1\\) multiplied by the probability that the sequential process stopped at \\(k\\) rather than continuing past it.\n\nHuman resources attrition dataset\nTo illustrate an sequential model with a stopping ratio link function, we will use data from the IBM human resources employee attrition and performance dataset. The original dataset contains 1470 rows and 35 columns. However, our goal is to model the total working years of employees using age as a predictor. This data lends itself to a sequential model as the response, total working years, is a sequential process. In order to have 10 years of working experience, it is necessarily true that the employee had 9 years of working experience. Additionally, age is choosen as a predictor as it is positively correlated with total working years.\n\nattrition = pd.read_csv(\"data/hr_employee_attrition.tsv.txt\", sep=\"\\t\")\nattrition = attrition[attrition[\"Attrition\"] == \"No\"]\nattrition[\"YearsAtCompany\"] = pd.Categorical(attrition[\"YearsAtCompany\"], ordered=True)\nattrition[[\"YearsAtCompany\", \"Age\"]].head()\n\n\n\n\n\n\n\n\nYearsAtCompany\nAge\n\n\n\n\n1\n10\n49\n\n\n3\n8\n33\n\n\n4\n2\n27\n\n\n5\n7\n32\n\n\n6\n1\n59\n\n\n\n\n\n\n\nBelow, the empirical probabilities of the response categories are computed. Employees are most likely to stay at the company between 1 and 10 years.\n\npr_k = attrition.YearsAtCompany.value_counts().sort_index().values / attrition.shape[0]\n\nplt.figure(figsize=(7, 3))\nplt.bar(np.arange(0, 36), pr_k)\nplt.xlabel(\"Response category\")\nplt.ylabel(\"Probability\")\nplt.title(\"Empirical probability of each response category\");\n\n\n\n\n\n\n\n\n\n\nDefault prior of thresholds\nBefore we fit the sequential model, it’s worth mentioning that the default priors for the thresholds in a sequential model are different than the cumulative model. In the cumulative model, the default prior for the thresholds is a Normal distribution with a grid of evenly spaced \\(\\mu\\) where an ordered transformation is applied to ensure the ordering of the values. However, in the sequential model, the ordering of the thresholds does not matter. Thus, the default prior for the thresholds is a Normal distribution with a zero \\(\\mu\\) vector of length \\(k - 1\\) where \\(k\\) is the number of response levels. Refer to the getting started docs if you need a refresher on priors in Bambi.\nSubsequently, fitting a sequential model is similar to fitting a cumulative model. The only difference is that we pass family=\"sratio\" to the bambi.Model constructor.\n\nsequence_model = bmb.Model(\n    \"YearsAtCompany ~ 0 + TotalWorkingYears\", \n    data=attrition, \n    family=\"sratio\"\n)\nsequence_idata = sequence_model.fit(random_seed=1234)\n\nOnly 250 samples in chain.\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [YearsAtCompany_threshold, TotalWorkingYears]\n\n\n\n\n\n\n\n    \n      \n      100.00% [2000/2000 02:28&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 250 tune and 250 draw iterations (1_000 + 1_000 draws total) took 148 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n\n\n\nsequence_model\n\n       Formula: YearsAtCompany ~ 0 + TotalWorkingYears\n        Family: sratio\n          Link: p = logit\n  Observations: 1233\n        Priors: \n    target = p\n        Common-level effects\n            TotalWorkingYears ~ Normal(mu: 0.0, sigma: 0.3223)\n        \n        Auxiliary parameters\n            threshold ~ Normal(mu: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n             0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.], sigma: 1.0)\n\n\n\naz.summary(sequence_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nYearsAtCompany_threshold[0]\n-2.525\n0.193\n-2.896\n-2.191\n0.004\n0.003\n2027.0\n812.0\n1.00\n\n\nYearsAtCompany_threshold[1]\n-1.057\n0.110\n-1.265\n-0.850\n0.003\n0.002\n1137.0\n601.0\n1.01\n\n\nYearsAtCompany_threshold[2]\n-1.017\n0.119\n-1.238\n-0.792\n0.004\n0.003\n1009.0\n705.0\n1.00\n\n\nYearsAtCompany_threshold[3]\n-0.760\n0.117\n-0.984\n-0.553\n0.003\n0.002\n1210.0\n833.0\n1.00\n\n\nYearsAtCompany_threshold[4]\n-0.754\n0.126\n-0.969\n-0.493\n0.003\n0.002\n1327.0\n780.0\n1.00\n\n\nYearsAtCompany_threshold[5]\n0.251\n0.111\n0.053\n0.466\n0.003\n0.002\n1207.0\n808.0\n1.01\n\n\nYearsAtCompany_threshold[6]\n-0.500\n0.145\n-0.769\n-0.230\n0.004\n0.003\n1394.0\n526.0\n1.01\n\n\nYearsAtCompany_threshold[7]\n-0.085\n0.137\n-0.355\n0.155\n0.004\n0.003\n1115.0\n835.0\n1.00\n\n\nYearsAtCompany_threshold[8]\n0.025\n0.141\n-0.213\n0.310\n0.004\n0.004\n1081.0\n750.0\n1.00\n\n\nYearsAtCompany_threshold[9]\n0.389\n0.146\n0.128\n0.673\n0.004\n0.003\n1134.0\n865.0\n1.00\n\n\nYearsAtCompany_threshold[10]\n1.267\n0.154\n0.970\n1.540\n0.005\n0.004\n923.0\n737.0\n1.01\n\n\nYearsAtCompany_threshold[11]\n0.436\n0.213\n0.055\n0.826\n0.006\n0.004\n1376.0\n662.0\n1.00\n\n\nYearsAtCompany_threshold[12]\n-0.116\n0.307\n-0.662\n0.457\n0.008\n0.010\n1568.0\n693.0\n1.00\n\n\nYearsAtCompany_threshold[13]\n0.513\n0.250\n0.024\n0.943\n0.006\n0.005\n1537.0\n738.0\n1.00\n\n\nYearsAtCompany_threshold[14]\n0.392\n0.293\n-0.126\n0.958\n0.008\n0.006\n1328.0\n696.0\n1.01\n\n\nYearsAtCompany_threshold[15]\n0.791\n0.277\n0.310\n1.348\n0.008\n0.005\n1405.0\n619.0\n1.00\n\n\nYearsAtCompany_threshold[16]\n0.433\n0.329\n-0.196\n1.017\n0.009\n0.006\n1516.0\n691.0\n1.01\n\n\nYearsAtCompany_threshold[17]\n0.252\n0.365\n-0.397\n0.916\n0.009\n0.009\n1468.0\n841.0\n1.00\n\n\nYearsAtCompany_threshold[18]\n0.791\n0.330\n0.160\n1.394\n0.009\n0.007\n1274.0\n856.0\n1.00\n\n\nYearsAtCompany_threshold[19]\n0.788\n0.347\n0.148\n1.445\n0.010\n0.007\n1306.0\n612.0\n1.01\n\n\nYearsAtCompany_threshold[20]\n2.191\n0.268\n1.684\n2.680\n0.008\n0.006\n1076.0\n720.0\n1.00\n\n\nYearsAtCompany_threshold[21]\n1.842\n0.331\n1.323\n2.516\n0.009\n0.006\n1447.0\n825.0\n1.00\n\n\nYearsAtCompany_threshold[22]\n2.431\n0.369\n1.746\n3.078\n0.011\n0.008\n1230.0\n616.0\n1.00\n\n\nYearsAtCompany_threshold[23]\n0.017\n0.743\n-1.389\n1.340\n0.016\n0.027\n2194.0\n712.0\n1.00\n\n\nYearsAtCompany_threshold[24]\n1.581\n0.571\n0.489\n2.582\n0.015\n0.010\n1671.0\n636.0\n1.00\n\n\nYearsAtCompany_threshold[25]\n1.530\n0.579\n0.491\n2.646\n0.013\n0.010\n2056.0\n616.0\n1.00\n\n\nYearsAtCompany_threshold[26]\n1.727\n0.607\n0.605\n2.844\n0.014\n0.010\n1982.0\n709.0\n1.00\n\n\nYearsAtCompany_threshold[27]\n1.096\n0.732\n-0.351\n2.412\n0.017\n0.016\n1855.0\n791.0\n1.02\n\n\nYearsAtCompany_threshold[28]\n1.156\n0.722\n-0.288\n2.399\n0.016\n0.013\n2073.0\n852.0\n1.00\n\n\nYearsAtCompany_threshold[29]\n0.541\n0.868\n-1.149\n2.239\n0.019\n0.027\n2055.0\n669.0\n1.01\n\n\nYearsAtCompany_threshold[30]\n1.247\n0.808\n-0.275\n2.744\n0.019\n0.015\n1821.0\n653.0\n1.01\n\n\nYearsAtCompany_threshold[31]\n1.385\n0.813\n0.013\n3.021\n0.019\n0.015\n1875.0\n599.0\n1.01\n\n\nYearsAtCompany_threshold[32]\n2.683\n0.710\n1.318\n3.936\n0.017\n0.012\n1698.0\n815.0\n1.01\n\n\nYearsAtCompany_threshold[33]\n0.868\n1.018\n-0.974\n2.944\n0.026\n0.024\n1573.0\n781.0\n1.00\n\n\nYearsAtCompany_threshold[34]\n1.756\n0.853\n0.348\n3.500\n0.021\n0.017\n1640.0\n876.0\n1.01\n\n\nTotalWorkingYears\n0.127\n0.005\n0.118\n0.138\n0.000\n0.000\n482.0\n514.0\n1.01\n\n\n\n\n\n\n\nThe coefficients are still on the logits scale, so we need to apply the inverse of the logit function to transform back to probabilities. Below, we plot the probabilities for each category.\n\nprobs = expit_func(sequence_idata.posterior.YearsAtCompany_threshold).mean((\"chain\", \"draw\"))\nprobs = np.append(probs, 1)\n\nplt.figure(figsize=(7, 3))\nplt.plot(sorted(attrition.YearsAtCompany.unique()), probs, marker='o')\nplt.ylabel(\"Probability\")\nplt.xlabel(\"Response category\");\n\n\n\n\n\n\n\n\nThis plot can seem confusing at first. Remember, the sequential model is a product of probabilities, i.e., the probability that \\(Y\\) is equal to category \\(k\\) is equal to the probability that it did not fall in one of the former categories \\(1: k-1\\) multiplied by the probability that the sequential process stopped at \\(k\\). Thus, the probability of category 5 is the probability that the sequential process did not fall in 0, 1, 2, 3, or 4 multiplied by the probability that the sequential process stopped at 5. This makes sense why the probability of category 36 is 1. There is no category after 36, so once you multiply all of the previous probabilities with the current category, you get 1. This is the reason for the “cumulative-like” shape of the plot. But if the coefficients were truly cumulative, the probability could not decreases as \\(k\\) increases.\n\n\nPosterior predictive samples\nAgain, using the posterior predictive samples, we can visualize the model fit against the observed data. In the case of the sequential model, the model does an alright job of capturing the observed frequencies of the categories. For pedagogical purposes, this fit is sufficient.\n\nidata_pps = model.predict(idata=idata, kind=\"pps\", inplace=False)\n\nbins = np.arange(35)\nfig, ax = plt.subplots(figsize=(7, 3))\nax = plot_ppc_discrete(idata_pps, bins, ax)\nax.set_xlabel(\"Response category\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Sequential model - Posterior Predictive Distribution\");"
  },
  {
    "objectID": "posts/2023-09-29-ordinal-models-bambi.html#summary",
    "href": "posts/2023-09-29-ordinal-models-bambi.html#summary",
    "title": "Ordinal Models in Bambi",
    "section": "Summary",
    "text": "Summary\nThis notebook demonstrated how to fit cumulative and sequential ordinal regression models using Bambi. Cumulative models focus on modeling the cumulative probabilities of an ordinal outcome variable taking on values up to and including a certain category, whereas a sequential model focuses on modeling the probability that an ordinal outcome variable stops at a particular category, rather than continuing to higher categories. To achieve this, both models assume that the reponse variable originates from a categorization of a latent continuous variable \\(Z\\). However, the cumulative model assumes that there are \\(K\\) thresholds \\(\\tau_k\\) that partition \\(Z\\) into \\(K+1\\) observable, ordered categories of \\(Y\\). The sequential model assumes that for every category \\(k\\) there is a latent continuous variable \\(Z\\) that determines the transition between categories \\(k\\) and \\(k+1\\); thus, a threshold \\(\\tau\\) belongs to each latent process.\nCumulative models can be used in situations where the outcome variable is on the Likert scale, and you are interested in understanding the impact of predictors on the probability of reaching or exceeding specific categories. Sequential models are particularly useful when you are interested in understanding the predictors that influence the decision to stop at a specific response level. It’s well-suited for analyzing data where categories represent stages, and the focus is on the transitions between these stages.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Fri Sep 15 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\nbambi     : 0.13.0.dev0\narviz     : 0.15.1\nnumpy     : 1.24.2\npandas    : 2.0.1\nmatplotlib: 3.7.1\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2022-10-14-inference-hmc.html",
    "href": "posts/2022-10-14-inference-hmc.html",
    "title": "Inference - Hamiltonian Monte Carlo from Scratch",
    "section": "",
    "text": "Many MCMC algorithms perform poorly in high dimensions as they rely on a form of random searches based on local perturbations. Hamiltonian Monte Carlo (HMC), however, leverages gradient information to guide the local moves and propose new states. The gradients of the log probability of the posterior evaluated at some state provides information of the geometry of the posterior density function. HMC attempts to avoid the random walk behavior typical of Metropolis-Hastings by using the gradient to propose new positions far from the current one with high acceptance probability. This allows HMC to scale to higher dimensions and in principle more complex posterior geometries.\n\nHamiltonian Mechanics in a Statistical Setting\nHMC gets its name from Hamiltonian mechanics. The field of mechanics can be used to describe simple systems such as a bouncing ball, a pendulum or an oscillating spring in which energy changes from kinetic to potential and back again over time. Consider a skateboarder riding in an empty pool. We can characterize the skateboarder in terms of their position \\(\\theta \\in \\mathbb{R}^D\\) denoted \\(q\\) and momentum \\(v \\in \\mathbb{R}^D\\), denoted \\(p\\). The set of all possible values for (\\(q, p\\)) the skateboarder can take on is called the phase space. The Hamiltonian function is a description of the total energy of a physical system and is defined as:\n\\[\\mathcal{H}(q, p) = \\mathcal{E}(q) + \\mathcal{K}(p)\\]\nwhere \\(\\mathcal{E}(q)\\) is the potential energy, and \\(\\mathcal{K}(p)\\) is the kinetic energy. \\(\\mathcal{H}(q, p)\\) is the total energy. However, since we are operating in a statistical, not a physical, setting, the potential energy is a log probability density function (logpdf) such as \\(p(q, D)\\):\n\\[\\mathcal{E}(q) = -log\\tilde{p}(q)\\]\nand kinetic energy is:\n\\[\\mathcal{K} = \\frac{1}{2}p^T\\sum^{-1}p\\]\nwhere \\(\\sum\\) is a positive-definite matrix, known as the inverse mass matrix. Why is kinetic energy this way? We are free to choose the kinetic energy, and if we choose it to be Gaussian, and drop the normalization constant, we get the \\(\\mathcal{K}\\) above.\nTo run the physics simulation, one must solve the continuous time differential equations, known as Hamilton’s equations:\n\\[\\frac{dq}{dt} = \\frac{\\partial \\mathcal{H}}{\\partial p} = \\frac{\\partial \\mathcal{K}}{\\partial p}\\]\n\\[\\frac{dp}{dt} = -\\frac{\\partial \\mathcal{H}}{\\partial q} = -\\frac{\\partial \\mathcal{E}}{\\partial q}\\]\n\nConservation of Energy\nSince we are running a physics simulation, total energy must be conserved. Intuitively, a satellite in orbit around a planet will “want” to continue in a straight line due to its momentum, but will get pulled in towards the planet due to gravity, and if these forces cancel out, the orbit is stable. If the satellite beings spiraling towards the planet, its kinetic energy will increase but its potential energy will decrease. In our statistical setting, if total energy is not conserved, then this means there were divergences and our numerical approximation went bad.\n\n\n\nLeapfrog Integrator\nTo simulate the differential equations above, we must first discretize \\(t\\), and go back and forth updating \\(q\\) and \\(p\\). However, this “back and forth” is not so straightforward. It turns out, one way to simulate Hamiltonian dynamics, is with a method called the leapfrog integrator.\nThis integrator first performs a half update of the momentum \\(p\\), then a full update of the position \\(q\\), and then finally another half update of the momentum.\n\\[\\underbrace{v_{t+1/2} = v_t - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_t)}{\\partial q}}_{\\text{half update}}\\]\n\\[\\underbrace{q_{t+1} = q_t + \\eta \\frac{\\partial \\mathcal{K}(p_{t+1/2})}{\\partial p}}_{\\text{full update}}\\]\n\\[\\underbrace{v_{t+1} = v_{t+1/2} - \\frac{\\eta}{2} \\frac{\\partial \\mathcal{E}(q_{t+1})}{\\partial q}}_{\\text{half update}}\\]\nThe leapfrog integrator has two important parameters: (1) path length, and (2) step size \\(\\eta\\). In the simululation, the path length represents how “long” you travel before collecting another sample. The step size indicates the size each step is in the path length and determines how fine grained the simulation is. For example, in the drawing below, imagine path length \\(=1\\) for both simulations. However, the left simulation has a step size \\(=4\\) whereas the right simulation has a step size \\(=2\\). These parameters are important in determining how the simulator collects samples of the geometry of the posterior.\n\n\n\nleapfrog\n\n\n\n\nMain Idea\nHMC says the log posterior is like a “bowl” (the empty pool in the figure below), with the highest posterior probability at the center of the valley. If we give the skateboarder a flick, this momentum will simulate its path. It must obey physics, gliding along until we stop the clock and take a sample. If the log posterior is flat, then not much information is in the likelihood & prior and the skateboarder will glide before the gradient makes it turn around. However, if the geometry of the log posterior is not flat, like the pool below, the gradient will make the skateboarder turn around and back into the valley. Since HMC runs a physics simulation, certain things must be conserved, i.e., the total energy of the system.\n\n\n\nskateboarder\n\n\nHMC needs a few things to run: 1. A function or callable that returns the negative log probability of the data at the current position \\(q\\) 2. A means of returning the gradient of the negative log probability at the current position \\(q\\) 3. An integrator for simulating the Hamiltonian equations in discrete time with two parameters: - step size - path length\nThe algorithm for running HMC is to: set the initial position \\(q\\) to \\(q'_0 = q_{t-1}\\), and sample a new random momentum \\(p' \\sim \\mathcal{N}(0, \\sum)\\). Then, initialize a random trajectory in the phase space, starting at (\\(q'_0, p'_0\\)), followed for \\(L\\) leapfrog steps until we get to the new proposed state (\\(q^*, p^*\\)) \\(=\\) (\\(q'_L, p'_L\\)). With the new proposed state, we compute the MH acceptance probability. This process is ran \\(n\\) times, according the number of samples the user wants to collect.\n\n\nHamiltonian Monte Carlo - Multivariate Normal\nThe HMC implementation below is heavily inspired by Colin Carroll’s implementation. I recommend reading his blog plosts on HMC for an in-depth, but intuitive, explanation.\n\n\nCode\ndef log_probs_to_img(dist, extent=None, num=100):\n\n    if extent is None:\n        extent = (-3, 3, -3, 3)\n\n    X, Y = torch.meshgrid(\n        torch.linspace(*extent[:2], num), torch.linspace(*extent[2:], num)\n        )\n\n    Z = torch.dstack((X, Y))\n    log_probs = torch.exp(dist.log_prob(Z))\n    \n    return X, Y, log_probs, extent\n\n\ndef plot_hmc(X, Y, Z, samples, positions, momentum, extent):\n\n    collected_samples = np.vstack([tens.detach().numpy() for tens in samples])\n    leap_q = [[tens.detach().numpy() for tens in q] for q in positions]\n    leap_p = [[tens.detach().numpy() for tens in p] for p in momentum]\n\n    final_q = np.concatenate(leap_q)\n    final_p = np.concatenate(leap_p)\n\n    steps = slice(None, None, 20)\n\n    plt.figure(figsize=(10, 6))\n    plt.contourf(X, Y, Z, extent=extent, levels=20)\n    plt.quiver(\n        final_q[steps, 0], final_q[steps, 1], final_p[steps, 0], final_p[steps, 1], \n        headwidth=5, scale=80, headlength=7\n        )\n    plt.plot(final_q[:, 0], final_q[:, 1], linestyle='-', lw=2.5, color='black')\n    plt.scatter(\n        collected_samples[:, 0], collected_samples[:, 1],\n        color='red', alpha=0.75\n        )\n\n    plt.title('Hamiltonian Monte Carlo')\n    plt.show()\n\n\n\ndef leapfrog(q, p, dist, path_len, step_size):\n    \n    output = -dist.log_prob(q)\n    output.backward()\n    p -= step_size * q.grad / 2\n    q.grad.zero_()\n\n    leap_q = []\n    leap_p = []\n    for _ in range(int(path_len / step_size) - 1):\n\n        q.grad.zero_()\n        with torch.no_grad():\n            q += step_size * p\n        output = -dist.log_prob(q)\n        output.backward()\n        p -= step_size * q.grad\n\n        leap_q.append(q.clone())\n        leap_p.append(p.clone())\n        \n    output = -dist.log_prob(q)\n    output.backward()\n    \n    with torch.no_grad():\n        q += step_size * p\n    \n    p -= step_size * q.grad / 2\n\n    return q, -p, leap_q, leap_p\n\n\ndef hamiltonian_monte_carlo(\n    n_samples, \n    dist, \n    initial_position,\n    path_len=1, \n    step_size=0.1\n    ):\n    \"\"\"\n\n    This HMC implementation is inspired by Colin Carroll's blog post:\n    https://colindcarroll.com/2019/04/11/hamiltonian-monte-carlo-from-scratch/\n\n    Parameters:\n    ----------\n    n_samples : int\n        Number of samples to return\n    dist : object\n        PyTorch distribution object that can be called\n    initial_position : np.array\n        A place to start sampling from.\n    path_len : float\n        How long each integration path is. Smaller is faster and more correlated.\n    step_size : float\n        How long each integration step is. Smaller is slower and more accurate.\n\n    Returns:\n    -------\n    param_samples: list\n        list of parameter samples (position q)\n    all_leap_q: list\n        list of all the positions (q) when leapfrog integrator is ran\n    all_leap_p: list\n        list of all the momentum (p) values when leapfrog integrator is ran\n    \"\"\"    \n\n    samples = [initial_position]\n    param_samples = []\n    all_leap_q = []\n    all_leap_p = []\n\n    momentum = torch.distributions.Normal(0, 1)\n\n    size = (n_samples,) + initial_position.shape[:1]\n    for idx, p0 in tqdm(enumerate(momentum.sample(size)), total=size[0]):\n\n        q0 = samples[-1]\n        q_new, p_new, leap_q, leap_p = leapfrog(\n            q=q0,\n            p=p0,\n            dist=dist,\n            path_len=2 * np.random.rand() * path_len,\n            step_size=step_size\n            )\n\n        all_leap_q.append(leap_q)\n        all_leap_p.append(leap_p)\n\n        # Metropolis acceptance criterion\n        start_log_p = torch.sum(momentum.log_prob(p0)) - dist.log_prob(samples[-1])\n        new_log_p = torch.sum(momentum.log_prob(p_new)) - dist.log_prob(q_new)\n        p_accept = min(1, torch.exp(new_log_p - start_log_p))\n\n        if torch.rand(1) &lt; p_accept:\n            param_samples.append(q_new.clone())\n        else:\n            param_samples.append(q0.clone())\n        \n    return param_samples, all_leap_q, all_leap_p\n\n\ndef main(args):\n\n    mvn = dist.MultivariateNormal(torch.zeros(2), torch.eye(2))\n    init_pos = torch.randn(2, requires_grad=True)\n\n    X, Y, Z, extent = log_probs_to_img(mvn, (-3, 3, -3, 3), num=200)\n\n    samples, leap_q, leap_p = hamiltonian_monte_carlo(\n        n_samples=args.n_samples,\n        dist=mvn,\n        initial_position=init_pos,\n        path_len=args.path_len,\n        step_size=args.step_size\n        )\n\n    plot_hmc(X, Y, Z, samples, leap_q, leap_p, extent)\n\n\nparser = argparse.ArgumentParser(description='HMC')\nparser.add_argument('--n_samples', type=int, default=10)\nparser.add_argument('--path_len', type=int, default=1)\nparser.add_argument('--step_size', type=float, default=0.01)\nargs = parser.parse_args(\"\")\n\nmain(args)\n\n100%|██████████| 10/10 [00:00&lt;00:00, 62.54it/s]"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html",
    "href": "posts/2022-07-12-bmcp-ch-3.html",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport arviz as az\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions import constraints\nfrom pyro.infer import Predictive, TracePredictive, NUTS, MCMC\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom pyro.infer.mcmc.util import summary\nfrom palmerpenguins import load_penguins\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (7, 4)"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#linear-penguins",
    "href": "posts/2022-07-12-bmcp-ch-3.html#linear-penguins",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "3.2.1 Linear Penguins",
    "text": "3.2.1 Linear Penguins\n\nadelie_flipper_length = torch.from_numpy(penguins.loc[adelie_mask, 'flipper_length_mm'].values)\nadelie_flipper_length -= adelie_flipper_length.mean()\nadelie_mass = torch.from_numpy(penguins.loc[adelie_mask, 'body_mass_g'].values)\n\n\ndef linear_model(flipper_length, mass=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(2000.))\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 4000.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 4000.))\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * flipper_length)\n\n    with pyro.plate('plate'):   \n        preds = pyro.sample('mass', dist.Normal(mu, sigma), obs=mass)  \n\n\npyro.render_model(\n    linear_model, \n    model_args=(adelie_flipper_length, adelie_mass),\n    render_distributions=True\n    )\n\n\n\n\n\n\n\n\n\nMCMC\n\nkernel = NUTS(linear_model, adapt_step_size=True)\nmcmc_simple = MCMC(kernel, num_samples=500, warmup_steps=300)\nmcmc_simple.run(flipper_length=adelie_flipper_length, mass=adelie_mass)\n\nSample: 100%|██████████| 800/800 [00:37, 21.16it/s, step size=9.30e-01, acc. prob=0.911] \n\n\n\nmcmc_simple.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0   3706.50     33.96   3706.28   3650.73   3755.13    421.93      1.01\n    beta_1     32.37      5.06     32.27     25.29     42.32    401.68      1.00\n     sigma    408.44     23.59    407.82    366.63    443.52    353.65      1.00\n\nNumber of divergences: 0\n\n\n\n# trace plots are interesting for only only chain\naz.plot_trace(az.from_pyro(mcmc_simple))\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\nPosterior Predictive Distribution\n\n# posterior samples\nmcmc_samples = mcmc_simple.get_samples(num_samples=1000)\n# pred. dist. conditioned on posterior samples\npredictive = Predictive(linear_model, mcmc_samples)\npredictive_samples = predictive(flipper_length=adelie_flipper_length, mass=None) \n\nfor k, v in predictive_samples.items():\n    print(f'{k}: {tuple(v.shape)}')\n\nmass: (1000, 146)\nmu: (1000, 1, 146)\n\n\n\ndef mcmc_fit(predictive):\n    mass = predictive['mass']\n    mass_mu = mass.mean(axis=0)\n    mass_std = mass.std(axis=0)\n\n    mass_df = pd.DataFrame({\n        'feat': adelie_flipper_length,\n        'mean': mass_mu,\n        'high': mass_mu + mass_std,\n        'low': mass_mu - mass_std}\n    )\n\n    return mass_df.sort_values(by=['feat'])\n\n\nmass_df = mcmc_fit(predictive=predictive_samples)\n\n\nplt.scatter(adelie_flipper_length.numpy(), adelie_mass.numpy(), alpha=0.5)\nplt.plot(mass_df['feat'], mass_df['mean'], color='black')\nplt.fill_between(\n    mass_df['feat'], mass_df['high'], mass_df['low'], alpha=0.2, color='grey')\nplt.xlabel('adelie_flipper_length')\nplt.ylabel('mass')\nplt.title('$\\mu =  \\\\beta_0 + \\\\beta_1X_1$')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSVI\nNOT WORKING: I believe it has to do with the autoguide. Solution could be to implement the guide by hand\npyro.get_param_store() is comprised of learned parameters that will be used in the Predictive stage. Instead of providing samples, the guide parameter is used to construct the posterior predictive distribution"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#multiple-linear-regression",
    "href": "posts/2022-07-12-bmcp-ch-3.html#multiple-linear-regression",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "3.3 Multiple Linear Regression",
    "text": "3.3 Multiple Linear Regression\n\nsex_obs = torch.from_numpy(penguins.loc[adelie_mask, 'sex'].replace({'male': 0, 'female': 1}).values)\n\nsns.scatterplot(\n    x=adelie_flipper_length, y=adelie_mass_obs, hue=sex_obs, alpha=0.5)\nplt.xlabel('adelie_flipper_length')\nplt.ylabel('mass')\nplt.show()\n\n\n\n\n\n\n\n\n\ndef linear_model(flipper_length, sex, mass=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(2000.))\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 3000.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 3000.))\n    beta_2 = pyro.sample('beta_2', dist.Normal(0., 3000.))\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * flipper_length + beta_2 * sex)\n\n    with pyro.plate('plate'):   \n        preds = pyro.sample('mass', dist.Normal(mu, sigma), obs=mass)  \n\n\npyro.render_model(\n    linear_model, \n    model_args=(adelie_flipper_length, sex_obs, adelie_mass),\n    render_distributions=True\n    )\n\n\n\n\n\n\n\n\n\nMCMC\n\nkernel = NUTS(linear_model, adapt_step_size=True)\nmcmc = MCMC(kernel, num_samples=500, warmup_steps=300, num_chains=1)\nmcmc.run(flipper_length=adelie_flipper_length, sex=sex_obs, mass=adelie_mass)\nmcmc.summary()\n\nSample: 100%|██████████| 800/800 [00:53, 15.07it/s, step size=5.06e-01, acc. prob=0.926] \n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0   4004.68     37.17   4003.62   3947.60   4064.92    329.83      1.00\n    beta_1     16.42      3.98     16.41      9.50     22.23    234.91      1.00\n    beta_2   -597.92     54.41   -594.84   -684.72   -514.38    313.84      1.00\n     sigma    297.48     18.46    296.21    271.08    332.22    352.02      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\naz.plot_trace(az.from_pyro(mcmc))\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nmcmc_samples = mcmc.get_samples(num_samples=1000)\npredictive = Predictive(linear_model, mcmc_samples)\npredictive_samples = predictive(flipper_length=adelie_flipper_length, sex=sex_obs, mass=None) \n\n\nmass_mu = predictive_samples['mass'].numpy().mean(axis=0)\nmass_std = predictive_samples['mass'].numpy().std(axis=0)\n\npredictions = pd.DataFrame({\n    'sex': sex_obs,\n    'flipper': adelie_flipper_length,\n    'mass_mu': mass_mu,\n    'mass_std': mass_std,\n    'high': mass_mu + mass_std,\n    'low': mass_mu - mass_std\n})\n\npredictions = predictions.sort_values(by=['flipper'])\n\n\nmale = predictions[predictions['sex'] == 0]\nfemale = predictions[predictions['sex'] == 1]\n\n\nsns.scatterplot(\n    x=adelie_flipper_length, y=adelie_mass_obs, hue=sex_obs, alpha=0.5)\nplt.plot(male['flipper'], male['mass_mu'])\nplt.plot(female['flipper'], female['mass_mu'])\nplt.fill_between(\n    male['flipper'], male['high'], male['low'], alpha=0.2, color='grey')\nplt.fill_between(\n    female['flipper'], female['high'], female['low'], alpha=0.2, color='grey')\nplt.xlabel('adelie_flipper_length')\nplt.ylabel('mass')\nplt.title('Predictions $\\pm 1 \\sigma$ ')\nplt.show()\n\n\n\n\n\n\n\n\n\nmcmc_multiple_az = az.from_pyro(mcmc)\nmcmc_simple_az = az.from_pyro(mcmc_simple)\n\naz.plot_forest([mcmc_simple_az, mcmc_multiple_az], var_names=['sigma'])\n# manually specify to avoid confusion\nplt.legend(['flipper_x_sex $\\sigma$', 'flipper_only $\\sigma$'])\nplt.show()\n\n\n\n\n\n\n\n\n\n\nCounterfactuals\n\nbill_length_obs = torch.from_numpy(penguins.loc[adelie_mask, 'bill_length_mm'].values)\nbill_length_obs -= bill_length_obs.mean()\n\n\ndef linear_model_counterfactual(flipper_length, sex, bill_length, mass=None):\n\n    sigma = pyro.sample('sigma', dist.HalfNormal(2000.))\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 3000.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 3000.))\n    beta_2 = pyro.sample('beta_2', dist.Normal(0., 3000.))\n    beta_3 = pyro.sample('beta_3', dist.Normal(0., 3000.))\n    mu = pyro.deterministic(\n        'mu', beta_0 + beta_1 * flipper_length + beta_2 * sex + beta_3 * bill_length)\n\n    with pyro.plate('plate'):   \n        preds = pyro.sample('mass', dist.Normal(mu, sigma), obs=mass)  \n\n\npyro.render_model(\n    linear_model_counterfactual, \n    model_args=(adelie_flipper_length, bill_length_obs, sex_obs, adelie_mass),\n    render_distributions=True\n    )\n\n\nkernel = NUTS(linear_model_counterfactual, adapt_step_size=True)\nmcmc = MCMC(kernel, num_samples=500, warmup_steps=300, num_chains=1)\nmcmc.run(adelie_flipper_length, bill_length_obs, sex_obs, adelie_mass)\nmcmc.summary()\n\nSample: 100%|██████████| 800/800 [01:08, 11.60it/s, step size=4.50e-01, acc. prob=0.913] \n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0   3978.38     41.39   3978.53   3900.43   4039.73    201.51      1.00\n    beta_1     14.99      4.32     15.03      7.73     20.89    326.37      1.01\n    beta_2     21.17     12.07     21.02      1.69     40.13    258.26      1.00\n    beta_3   -541.23     64.61   -540.35   -646.93   -431.72    196.03      1.00\n     sigma    295.87     16.45    294.49    269.92    323.25    452.90      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\nmcmc_samples = mcmc.get_samples(num_samples=1000)\n\n\nmean_flipper_length = penguins.loc[adelie_mask, 'flipper_length_mm'].mean()\ncounterfactual_flipper_lengths = torch.linspace(mean_flipper_length - 20, mean_flipper_length + 20, 21)\ncounterfactual_flipper_lengths -= counterfactual_flipper_lengths.mean()\n\nsex_indicator = torch.zeros_like(counterfactual_flipper_lengths)\nmean_bill_length = torch.ones_like(counterfactual_flipper_lengths) * bill_length_obs.mean()\n\n\ncounterfactual_samples = Predictive(\n    linear_model_counterfactual,\n    mcmc_samples)(counterfactual_flipper_lengths, sex_indicator, mean_bill_length, None)\n\nmass_mu = counterfactual_samples['mass'].numpy().mean(axis=0)\nmass_std = counterfactual_samples['mass'].numpy().std(axis=0)\n\n\nplt.plot(counterfactual_flipper_lengths, mass_mu, color='blue')\nplt.fill_between(\n    x=counterfactual_flipper_lengths , \n    y1=mass_mu + mass_std, \n    y2=mass_mu - mass_std,\n    color='grey',\n    alpha=0.5)\nplt.ylabel('counterfactual expected mass')\nplt.xlabel('manipulated flipper length (centered)')\nplt.title('Counterfactual plot');"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#generalized-linear-models",
    "href": "posts/2022-07-12-bmcp-ch-3.html#generalized-linear-models",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "Generalized Linear Models",
    "text": "Generalized Linear Models\n\nLogistic Regression\n\nspecies_filter = penguins['species'].isin(['Adelie', 'Chinstrap'])\n\n\nspecies_filter = penguins['species'].isin(['Adelie', 'Chinstrap'])\nbill_length_obs = torch.from_numpy(penguins.loc[species_filter, 'bill_length_mm'].values.reshape(-1, 1))\nbill_length_obs = torch.tensor(bill_length_obs, dtype=torch.float)\n\nspecies = pd.Categorical(penguins.loc[species_filter, 'species'])\nspecies_codes = torch.from_numpy(species.codes).to(torch.float64)\nspecies_codes = torch.tensor(species_codes, dtype=torch.float)\n\n\ndef logistic_model(bill_length, species=None):\n\n    N, P = bill_length.shape\n\n    beta_0 = pyro.sample('beta_0', dist.Normal(0., 10.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 10.).expand([P]))#.unsqueeze(-1)\n    mu = beta_0 + torch.matmul(beta_1, bill_length.T) \n    theta = pyro.deterministic('theta', torch.sigmoid(mu))\n    db = pyro.deterministic('db', -beta_0 / beta_1)\n\n    with pyro.plate('plate'):\n        y1 = pyro.sample('y1', dist.Bernoulli(theta), obs=species)\n\n\n# we don't observe theta or db\npyro.render_model(\n    logistic_model, \n    model_args=(bill_length_obs, species_codes),\n    render_distributions=True)\n\n\n\n\n\n\n\n\n\n\nMCMC\n\nkernel = NUTS(logistic_model, adapt_step_size=True)\nmcmc_logistic = MCMC(kernel, num_samples=500, warmup_steps=300)\nmcmc_logistic.run(bill_length=bill_length_obs, species=species_codes)\nmcmc_logistic.summary()\n\nSample: 100%|██████████| 800/800 [00:11, 70.35it/s, step size=9.29e-02, acc. prob=0.857]\n\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n    beta_0    -33.38      4.12    -33.01    -41.20    -27.87     55.78      1.00\n beta_1[0]      0.75      0.10      0.75      0.61      0.91     55.19      1.00\n\nNumber of divergences: 0\n\n\n\n\n\n\naz.plot_trace(az.from_pyro(mcmc_logistic))\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n# latent variables\nmcmc_samples = mcmc_logistic.get_samples(num_samples=1000)\n# posterior samples \npredictive = Predictive(logistic_model, mcmc_samples)\n# posterior predictive\npredictive_samples = predictive(bill_length_obs, None) \n\n\nprob_mu = predictive_samples['theta'].numpy().mean(axis=0).flatten()\nprob_std = predictive_samples['theta'].numpy().std(axis=0).flatten()\ndb_mu = predictive_samples['db'].numpy().mean()\ndb_std = predictive_samples['db'].numpy().std()\n\npredictions = pd.DataFrame({\n    'bill_length': bill_length_obs.flatten(),\n    'prob_mu': prob_mu,\n    'prob_std': prob_std,\n    'high': prob_mu + prob_std,\n    'low': prob_mu - prob_std\n})\n\npredictions = predictions.sort_values(by=['bill_length'])\n\n\nfor i, (label, marker) in enumerate(zip(species.categories, (\".\", \"s\"))):\n    _filter = (species.codes == i) ## size\n    x = bill_length_obs[_filter] ## x_obs\n    y = np.random.normal(i, 0.02, size=_filter.sum()) ## small amount of noise (jitter)\n    plt.scatter(bill_length_obs[_filter], y, marker=marker, label=label, alpha=.8)\n\nplt.plot(predictions['bill_length'], predictions['prob_mu'], color='black')\nplt.fill_between(\n    predictions['bill_length'], predictions['high'], predictions['low'],\n    alpha=0.25, color='grey')\nplt.axvline(\n    x=predictive_samples['db'].numpy().mean(), linestyle='--', color='black')\nplt.xlabel('bill length')\nplt.ylabel('Probability')\nplt.title('Logistic Model - MCMC');\n\n\n\n\n\n\n\n\n\n\nSVI\n\ndef logistic_guide(bill_length, species=None):\n\n    N, P = bill_length.shape\n    \n    beta_0_loc = pyro.param('beta_0_loc', torch.tensor(0.))\n    beta_0_scale = pyro.param('beta_0_scale', torch.tensor(0.1), constraint=constraints.positive)\n    beta_0 = pyro.sample('beta_0', dist.Normal(beta_0_loc, beta_0_scale))\n\n    beta_1_loc = pyro.param('beta_1_loc', torch.tensor(0.1))\n    beta_1_scale = pyro.param('beta_1_scale', torch.tensor(0.1), constraint=constraints.positive)\n    beta_1 = pyro.sample('beta_1', dist.Normal(beta_1_loc, beta_1_scale).expand([P]))\n\n\npyro.render_model(\n    logistic_guide, \n    model_args=(bill_length_obs, species_codes), \n    render_params=True\n    )\n\n\n\n\n\n\n\n\n\npyro.clear_param_store()\n\noptim = Adam({\"lr\": 0.2})\nsvi = SVI(logistic_model, logistic_guide, optim, Trace_ELBO())\n\nelbo_loss = []\nfor i in range(500):\n    loss = svi.step(bill_length_obs, species_codes)\n    elbo_loss.append(loss)\n\nplt.plot(np.arange(1, 501), elbo_loss)\nplt.ylabel('ELBO Loss')\nplt.xlabel('Iterations')\nplt.title(f'iter: 1000, loss: {elbo_loss[-1]:.4f}');\n\n\n\n\n\n\n\n\n\nfor name, value in pyro.get_param_store().items():\n    print(name, pyro.param(name))\n\nbeta_0_loc tensor(-13.7759, requires_grad=True)\nbeta_0_scale tensor(0.1191, grad_fn=&lt;AddBackward0&gt;)\nbeta_1_loc tensor(0.3193, requires_grad=True)\nbeta_1_scale tensor(0.0044, grad_fn=&lt;AddBackward0&gt;)\n\n\n\npredictive = Predictive(logistic_model, guide=logistic_guide, num_samples=1000)\nposterior_svi_samples = predictive(bill_length_obs, None)\n\n\nprob_mu = posterior_svi_samples['theta'].numpy().mean(axis=0).flatten()\nprob_std = posterior_svi_samples['theta'].numpy().std(axis=0).flatten()\n\npredictions = pd.DataFrame({\n    'bill_length': bill_length_obs.flatten(),\n    'prob_mu': prob_mu,\n    'prob_std': prob_std,\n    'high': prob_mu + prob_std,\n    'low': prob_mu - prob_std\n})\n\npredictions = predictions.sort_values(by=['bill_length'])\n\n\nfor i, (label, marker) in enumerate(zip(species.categories, (\".\", \"s\"))):\n    _filter = (species.codes == i) ## size\n    x = bill_length_obs[_filter] ## x_obs\n    y = np.random.normal(i, 0.02, size=_filter.sum()) ## add small amount of noise for plotting\n    plt.scatter(bill_length_obs[_filter], y, marker=marker, label=label, alpha=.8)\n\nplt.plot(predictions['bill_length'], predictions['prob_mu'], color='black')\nplt.fill_between(\n    predictions['bill_length'], predictions['high'], predictions['low'],\n    alpha=0.25, color='grey')\n# plt.axvline(\n#     x=predictive_samples['db'].numpy().mean(), linestyle='--', color='black')\nplt.xlabel('bill length')\nplt.ylabel('Probability')\nplt.title('Logistic Model - SVI');\n\n\n\n\n\n\n\n\n\n\nCode 3.24\nUsing body mass and flipper length as covariates\nWhen creating a multidimensional distribution in pyro, there is the added functionality of .to_event(1). This method implies that “these dimensions should be treated as a single event”. - see discussion here\n\nX = penguins.loc[species_filter, ['bill_length_mm', 'body_mass_g']]\n\nbill_length_mu = X['bill_length_mm'].mean()\nbill_length_std = X['bill_length_mm'].std()\nbody_mass_g_mu = X['body_mass_g'].mean()\nbody_mass_g_std = X['body_mass_g'].std()\n\nX['bill_length_mm'] = (X['bill_length_mm'] - bill_length_mu) / bill_length_std\nX['body_mass_g'] = (X['body_mass_g'] - body_mass_g_mu) / body_mass_g_std\nX = torch.from_numpy(X.values).to(torch.float)\n\nintercept = torch.ones_like(X[:, 0][..., None])\nX = torch.hstack((intercept, X))\n\nspecies_codes = species_codes.to(torch.float)\n\n\ndef multiple_logistic_model(data_matrix, species=None):\n\n    N, K = data_matrix.size()\n\n    # w = pyro.sample('coef', dist.MultivariateNormal(\n    #     loc=torch.ones(K), covariance_matrix=torch.eye(K)\n    # ))\n\n    w = pyro.sample('coef', dist.Normal(0., 10.).expand([K]))    \n    mu = torch.matmul(w, data_matrix.T)\n    theta = pyro.deterministic('theta', torch.sigmoid(mu))\n    db = pyro.deterministic('db', -w[0] / w[2] - w[1] / w[2] * data_matrix[:, 1])\n    \n    with pyro.plate('output', N):\n        y1 = pyro.sample('obs', dist.Bernoulli(logits=theta), obs=species)\n\n\npyro.render_model(\n    multiple_logistic_model, \n    model_args=(X, species_codes),\n    render_distributions=True\n    )\n\n\n\n\n\n\n\n\n\nkernel = NUTS(model=multiple_logistic_model, adapt_step_size=True)\nmcmc_mult_logistic = MCMC(kernel, num_samples=800, warmup_steps=500)\nmcmc_mult_logistic.run(X, species_codes)\n\nSample: 100%|██████████| 1300/1300 [00:05, 222.11it/s, step size=2.76e-01, acc. prob=0.935]\n\n\n\nmcmc_mult_logistic.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   coef[0]     -6.54      3.13     -6.00    -11.19     -1.35    154.09      1.00\n   coef[1]     15.50      5.52     15.11      6.80     23.63    151.94      1.01\n   coef[2]     -4.98      2.82     -4.58     -9.15     -0.30    203.16      1.00\n\nNumber of divergences: 0\n\n\n\nmcmc_mult_samples = mcmc_mult_logistic.get_samples(num_samples=1000)\npost_predictive = Predictive(multiple_logistic_model, mcmc_mult_samples)\npredictive_samples = post_predictive(X, None)\n\nfor k, v in predictive_samples.items():\n    print(f'{k}: {tuple(v.shape)}')\n\nobs: (1000, 214)\ntheta: (1000, 1, 214)\ndb: (1000, 1, 214)\n\n\n\ninf_data = az.from_pyro(\n    mcmc_mult_logistic,\n    posterior_predictive=mcmc_mult_samples\n)\n\n\naz.plot_trace(inf_data, compact=False, var_names=['coef'])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nprob_mu = predictive_samples['theta'].mean(axis=0)[0]\nprob_std = predictive_samples['theta'].std(axis=0)[0]\ndb_mu = predictive_samples['db'].mean(axis=0)[0]\ndb_std = predictive_samples['db'].std(axis=0)[0]\n\npredictions = pd.DataFrame({\n    'bill_length': X[:, 1].numpy(),\n    'mass': X[:, 2].numpy(),\n    'species': species_codes.numpy(),\n    'prob_mu': prob_mu,\n    'prob_std': prob_std,\n    'high': prob_mu + prob_std,\n    'low': prob_mu - prob_std,\n    'db_mu': db_mu,\n    'db_high': db_mu + db_std,\n    'db_low': db_mu - db_std\n})\n\npredictions = predictions.sort_values(by=['bill_length', 'mass'])\n\n\nsns.scatterplot(data=predictions, x='bill_length', y='mass', hue='species')\nsns.lineplot(data=predictions, x='bill_length', y='db_mu', color='black');\nplt.fill_between(\n    x=predictions['bill_length'], \n    y1=predictions['db_high'], \n    y2=predictions['db_low'],\n    color='grey', alpha=0.5\n    )\nplt.ylim(bottom=-10, top=10)\n\n\n\n\n\n\n\n\n\n# not working\naz.plot_separation(inf_data, y='obs')\n\n\n\nInterpreting log odds\n\ncounts = penguins['species'].value_counts()\nadelie_count = counts['Adelie']\nchinstrap_count = counts['Chinstrap']\nadelie_probs = adelie_count / (adelie_count + chinstrap_count)\n\nprint(f'prior probability of adelie     = {adelie_probs:.4f}')\nprint(f'odds of adelie                  = {(adelie_probs / (1 - adelie_probs)):.4f}')\nprint(f'log odds (logit) of adelie      = {np.log(adelie_probs / (1 - adelie_probs)):.4f}')\n\nprior probability of adelie     = 0.6822\nodds of adelie                  = 2.1471\nlog odds (logit) of adelie      = 0.7641\n\n\n\nbeta_0 = inf_data['posterior']['coef'].to_numpy()[0][:, 0].mean()\nbeta_1 = inf_data['posterior']['coef'].to_numpy()[0][:, 1].mean()\nbeta_2 = inf_data['posterior']['coef'].to_numpy()[0][:, 2].mean()\n\nbill_length = 0\nval_1 = beta_0 + beta_1*bill_length + beta_2*0\nval_2 = beta_0 + beta_1*(bill_length+0.5) + beta_2*0\n\nval_1_probs = 1 / (1 + np.exp(-val_1))\nval_2_probs = 1 / (1 + np.exp(-val_2))\n\nprint(f'''\nincreasing bill length by 0.5 stddev (while holding body mass constant) \nincreases class probability from {val_1_probs:.2f} to {val_2_probs:.2f}\n''')\n\n\nincreasing bill length by 0.5 stddev (while holding body mass constant) \nincreases class probability from 0.19 to 0.55"
  },
  {
    "objectID": "posts/2022-07-12-bmcp-ch-3.html#picking-priors-in-regression-models",
    "href": "posts/2022-07-12-bmcp-ch-3.html#picking-priors-in-regression-models",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 3",
    "section": "Picking priors in regression models",
    "text": "Picking priors in regression models\n\nx = torch.arange(-2, 3, 1)\ny = torch.tensor([50, 44, 50, 47, 56])\n\nplt.scatter(x, y)\nplt.title('Attractiveness of parent and sex ratio');\n\n\n\n\n\n\n\n\n\ndef model_uninformative_prior_sex_ratio(x, obs=None):\n\n    sigma = pyro.sample('sigma', dist.Exponential(0.5))\n    beta_0 = pyro.sample('beta_0', dist.Normal(50., 20.))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 20.))\n\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * x)\n    ratio = pyro.sample('ratio', dist.Normal(mu, sigma), obs=obs)\n\ndef model_informative_prior_sex_ratio(x, obs=None):\n\n    sigma = pyro.sample('sigma', dist.Exponential(0.5))\n    beta_0 = pyro.sample('beta_0', dist.Normal(50., 0.5))\n    beta_1 = pyro.sample('beta_1', dist.Normal(0., 0.5))\n\n    mu = pyro.deterministic('mu', beta_0 + beta_1 * x)\n    ratio = pyro.sample('ratio', dist.Normal(mu, sigma), obs=obs)\n\n\nsex_ratio_uninform_prior_mcmc = MCMC(NUTS(\n    model_uninformative_prior_sex_ratio), num_samples=500, warmup_steps=300)\nsex_ratio_uninform_prior_mcmc.run(x, y)\n\nsex_ratio_inform_prior_mcmc = MCMC(NUTS(\n    model_informative_prior_sex_ratio), num_samples=500, warmup_steps=300)\nsex_ratio_inform_prior_mcmc.run(x, y)\n\nSample: 100%|██████████| 800/800 [00:04, 185.18it/s, step size=5.95e-01, acc. prob=0.923]\nSample: 100%|██████████| 800/800 [00:02, 298.60it/s, step size=1.04e+00, acc. prob=0.879]\n\n\n\n# uninformative prior\nuninform_prior_predictive = Predictive(\n    model_uninformative_prior_sex_ratio, \n    None, \n    num_samples=500)(x, None)\n\nuninform_post_samples = sex_ratio_uninform_prior_mcmc.get_samples(500)\n\nuninform_post_predictive = Predictive(\n    model_uninformative_prior_sex_ratio, \n    posterior_samples=uninform_post_samples)(x, None)\n\n# informative prior\ninform_prior_predictive = Predictive(\n    model_informative_prior_sex_ratio, \n    None, \n    num_samples=500)(x, None)\n\ninform_post_samples = sex_ratio_inform_prior_mcmc.get_samples(500)\n\ninform_post_predictive = Predictive(\n    model_informative_prior_sex_ratio, \n    posterior_samples=inform_post_samples)(x, None)\n\n\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, 7))\n\nax[0].scatter(x, y, color='black')\nax[0].plot(x, uninform_prior_predictive['ratio'].T, color='grey', alpha=0.1)\nax[0].plot(x, uninform_prior_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean prior sample')\nax[0].set_ylabel('ratio (in %)')\nax[0].set_ylim(bottom=0, top=100)\nax[0].legend()\nax[0].set_title('Uninformative prior samples')\n\nax[1].scatter(x, y, color='black')\nax[1].plot(x, uninform_post_predictive['ratio'].T, color='grey', alpha=0.1)\nax[1].plot(x, uninform_post_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean post. sample')\nax[1].set_ylabel('ratio (in %)')\nax[1].set_ylim(bottom=0, top=100)\nax[1].legend()\nax[1].set_title('Uninformative posterior samples');\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, 7))\n\nax[0].scatter(x, y, color='black')\nax[0].plot(x, inform_prior_predictive['ratio'].T, color='grey', alpha=0.1)\nax[0].plot(x, inform_prior_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean prior sample')\nax[0].set_ylabel('ratio (in %)')\nax[0].set_ylim(bottom=0, top=100)\nax[0].legend()\nax[0].set_title('Informative prior samples')\n\nax[1].scatter(x, y, color='black')\nax[1].plot(x, inform_post_predictive['ratio'].T, color='grey', alpha=0.1)\nax[1].plot(x, inform_post_predictive['ratio'].mean(axis=0), color='blue', alpha=0.75, label='mean post. sample')\nax[1].set_ylabel('ratio (in %)')\nax[1].set_ylim(bottom=0, top=100)\nax[1].legend()\nax[1].set_title('Informative posterior samples');"
  },
  {
    "objectID": "posts/2023-09-29-zip-models-bambi.html",
    "href": "posts/2023-09-29-zip-models-bambi.html",
    "title": "Zero Inflated Models in Bambi",
    "section": "",
    "text": "Code\nimport arviz as az\nimport matplotlib.pyplot as plt\nfrom matplotlib.lines import Line2D\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport seaborn as sns\nimport warnings\n\nimport bambi as bmb\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions."
  },
  {
    "objectID": "posts/2023-09-29-zip-models-bambi.html#zero-inflated-outcomes",
    "href": "posts/2023-09-29-zip-models-bambi.html#zero-inflated-outcomes",
    "title": "Zero Inflated Models in Bambi",
    "section": "Zero inflated outcomes",
    "text": "Zero inflated outcomes\nSometimes, an observation is not generated from a single process, but from a mixture of processes. Whenever there is a mixture of processes generating an observation, a mixture model may be more appropriate. A mixture model uses more than one probability distribution to model the data. Count data are more susceptible to needing a mixture model as it is common to have a large number of zeros and values greater than zero. A zero means “nothing happened”, and this can be either because the rate of events is low, or because the process that generates the events was never “triggered”. For example, in health service utilization data (the number of times a patient used a service during a given time period), a large number of zeros represents patients with no utilization during the time period. However, some patients do use a service which is a result of some “triggered process”.\nThere are two popular classes of models for modeling zero-inflated data: (1) ZIP, and (2) hurdle Poisson. First, the ZIP model is described and how to implement it in Bambi is outlined. Subsequently, the hurdle Poisson model and how to implement it is outlined thereafter."
  },
  {
    "objectID": "posts/2023-09-29-zip-models-bambi.html#zero-inflated-poisson",
    "href": "posts/2023-09-29-zip-models-bambi.html#zero-inflated-poisson",
    "title": "Zero Inflated Models in Bambi",
    "section": "Zero inflated poisson",
    "text": "Zero inflated poisson\nTo model zero-inflated outcomes, the ZIP model uses a distribution that mixes two data generating processes. The first process generates zeros, and the second process uses a Poisson distribution to generate counts (of which some may be zero). The result of this mixture is a distribution that can be described as\n\\[P(Y=0) = (1 - \\psi) + \\psi e^{-\\mu}\\]\n\\[P(Y=y_i) = \\psi \\frac{e^{-\\mu} \\mu_{i}^y}{y_{i}!} \\ \\text{for} \\ y_i = 1, 2, 3,...,n\\]\nwhere \\(y_i\\) is the outcome, \\(\\mu\\) is the mean of the Poisson process where \\(\\mu \\ge 0\\), and \\(\\psi\\) is the probability of the Poisson process where \\(0 \\lt \\psi \\lt 1\\). To understand how these two processes are “mixed”, let’s simulate some data using the two process equations above (taken from the PyMC docs).\n\nx = np.arange(0, 22)\npsis = [0.7, 0.4]\nmus = [10, 4]\nplt.figure(figsize=(7, 3))\nfor psi, mu in zip(psis, mus):\n    pmf = stats.poisson.pmf(x, mu)\n    pmf[0] = (1 - psi) + pmf[0] # 1.) generate zeros\n    pmf[1:] =  psi * pmf[1:] # 2.) generate counts\n    pmf /= pmf.sum() # normalize to get probabilities\n    plt.plot(x, pmf, '-o', label='$\\\\psi$ = {}, $\\\\mu$ = {}'.format(psi, mu))\n\nplt.title(\"Zero Inflated Poisson Process\")\nplt.xlabel('x', fontsize=12)\nplt.ylabel('f(x)', fontsize=12)\nplt.legend(loc=1)\nplt.show()\n\n\n\n\n\n\n\n\nNotice how the blue line, corresponding to a higher \\(\\psi\\) and \\(\\mu\\), has a higher rate of counts and less zeros. Additionally, the inline comments above describe the first and second process generating the data.\n\nZIP regression model\nThe equations above only describe the ZIP distribution. However, predictors can be added to make this a regression model. Suppose we have a response variable \\(Y\\), which represents the number of events that occur during a time period, and \\(p\\) predictors \\(X_1, X_2, ..., X_p\\). We can model the parameters of the ZIP distribution as a linear combination of the predictors.\n\\[Y_i \\sim \\text{ZIPoisson}(\\mu_i, \\psi_i)\\]\n\\[g(\\mu_i) = \\beta_0 + \\beta_1 X_{1i}+,...,+\\beta_p X_{pi}\\]\n\\[h(\\psi_i) = \\alpha_0 + \\alpha_1 X_{1i}+,...,+\\alpha_p X_{pi}\\]\nwhere \\(g\\) and \\(h\\) are the link functions for each parameter. Bambi, by default, uses the log link for \\(g\\) and the logit link for \\(h\\). Notice how there are two linear models and two link functions: one for each parameter in the \\(\\text{ZIPoisson}\\). The parameters of the linear model differ, because any predictor such as \\(X\\) may be associated differently with each part of the mixture. Actually, you don’t even need to use the same predictors in both linear models—but this beyond the scope of this notebook.\n\nThe fish dataset\nTo demonstrate the ZIP regression model, we model and predict how many fish are caught by visitors at a state park using survey data. Many visitors catch zero fish, either because they did not fish at all, or because they were unlucky. The dataset contains data on 250 groups that went to a state park to fish. Each group was questioned about how many fish they caught (count), how many children were in the group (child), how many people were in the group (persons), if they used a live bait (livebait) and whether or not they brought a camper to the park (camper).\n\nfish_data = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\ncols = [\"count\", \"livebait\", \"camper\", \"persons\", \"child\"]\nfish_data = fish_data[cols]\nfish_data[\"livebait\"] = pd.Categorical(fish_data[\"livebait\"])\nfish_data[\"camper\"] = pd.Categorical(fish_data[\"camper\"])\nfish_data = fish_data[fish_data[\"count\"] &lt; 60] # remove outliers\n\n\nfish_data.head()\n\n\n\n\n\n\n\n\ncount\nlivebait\ncamper\npersons\nchild\n\n\n\n\n0\n0.0\n0.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n1.0\n1.0\n1.0\n0.0\n\n\n2\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n1.0\n1.0\n2.0\n1.0\n\n\n4\n1.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n# Excess zeros, and skewed count\nplt.figure(figsize=(7, 3))\nsns.histplot(fish_data[\"count\"], discrete=True)\nplt.xlabel(\"Number of Fish Caught\");\n\n\n\n\n\n\n\n\nTo fit a ZIP regression model, we pass family=zero_inflated_poisson to the bmb.Model constructor.\n\nzip_model = bmb.Model(\n    \"count ~ livebait + camper + persons + child\", \n    fish_data, \n    family='zero_inflated_poisson'\n)\n\nzip_idata = zip_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [count_psi, Intercept, livebait, camper, persons, child]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:03&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 4 seconds.\n\n\nLets take a look at the model components. Why is there only one linear model and link function defined for \\(\\mu\\). Where is the linear model and link function for \\(\\psi\\)? By default, the “main” (or first) formula is defined for the parent parameter; in this case \\(\\mu\\). Since we didn’t pass an additional formula for the non-parent parameter \\(\\psi\\), \\(\\psi\\) was never modeled as a function of the predictors as explained above. If we want to model both \\(\\mu\\) and \\(\\psi\\) as a function of the predictor, we need to expicitly pass two formulas.\n\nzip_model\n\n       Formula: count ~ livebait + camper + persons + child\n        Family: zero_inflated_poisson\n          Link: mu = log\n  Observations: 248\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 0.0, sigma: 9.5283)\n            livebait ~ Normal(mu: 0.0, sigma: 7.2685)\n            camper ~ Normal(mu: 0.0, sigma: 5.0733)\n            persons ~ Normal(mu: 0.0, sigma: 2.2583)\n            child ~ Normal(mu: 0.0, sigma: 2.9419)\n        \n        Auxiliary parameters\n            psi ~ Beta(alpha: 2.0, beta: 2.0)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\n\nformula = bmb.Formula(\n    \"count ~ livebait + camper + persons + child\", # parent parameter mu\n    \"psi ~ livebait + camper + persons + child\"    # non-parent parameter psi\n)\n\nzip_model = bmb.Model(\n    formula, \n    fish_data, \n    family='zero_inflated_poisson'\n)\n\nzip_idata = zip_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, livebait, camper, persons, child, psi_Intercept, psi_livebait, psi_camper, psi_persons, psi_child]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:05&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 6 seconds.\n\n\n\nzip_model\n\n       Formula: count ~ livebait + camper + persons + child\n                psi ~ livebait + camper + persons + child\n        Family: zero_inflated_poisson\n          Link: mu = log\n                psi = logit\n  Observations: 248\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 0.0, sigma: 9.5283)\n            livebait ~ Normal(mu: 0.0, sigma: 7.2685)\n            camper ~ Normal(mu: 0.0, sigma: 5.0733)\n            persons ~ Normal(mu: 0.0, sigma: 2.2583)\n            child ~ Normal(mu: 0.0, sigma: 2.9419)\n    target = psi\n        Common-level effects\n            psi_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_livebait ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_camper ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_persons ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_child ~ Normal(mu: 0.0, sigma: 1.0)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\nNow, both \\(\\mu\\) and \\(\\psi\\) are defined as a function of a linear combination of the predictors. Additionally, we can see that the log and logit link functions are defined for \\(\\mu\\) and \\(\\psi\\), respectively.\n\nzip_model.graph()\n\n\n\n\n\n\n\n\nSince each parameter has a different link function, and each parameter has a different meaning, we must be careful on how the coefficients are interpreted. Coefficients without the substring “psi” correspond to the \\(\\mu\\) parameter (the mean of the Poisson process) and are on the log scale. Coefficients with the substring “psi” correspond to the \\(\\psi\\) parameter (this can be thought of as the log-odds of non-zero data) and are on the logit scale. Interpreting these coefficients can be easier with the interpret sub-package. Below, we will show how to use this sub-package to interpret the coefficients conditional on a set of the predictors.\n\naz.summary(\n    zip_idata, \n    var_names=[\"Intercept\", \"livebait\", \"camper\", \"persons\", \"child\"], \n    filter_vars=\"like\"\n)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n-1.573\n0.310\n-2.130\n-0.956\n0.005\n0.004\n3593.0\n3173.0\n1.0\n\n\nlivebait[1.0]\n1.609\n0.272\n1.143\n2.169\n0.004\n0.003\n4158.0\n3085.0\n1.0\n\n\ncamper[1.0]\n0.262\n0.095\n0.085\n0.440\n0.001\n0.001\n5032.0\n2816.0\n1.0\n\n\npersons\n0.615\n0.045\n0.527\n0.697\n0.001\n0.000\n4864.0\n2709.0\n1.0\n\n\nchild\n-0.795\n0.094\n-0.972\n-0.625\n0.002\n0.001\n3910.0\n3232.0\n1.0\n\n\npsi_Intercept\n-1.443\n0.817\n-2.941\n0.124\n0.013\n0.009\n4253.0\n3018.0\n1.0\n\n\npsi_livebait[1.0]\n-0.188\n0.677\n-1.490\n1.052\n0.010\n0.011\n4470.0\n2776.0\n1.0\n\n\npsi_camper[1.0]\n0.841\n0.323\n0.222\n1.437\n0.004\n0.003\n6002.0\n3114.0\n1.0\n\n\npsi_persons\n0.912\n0.193\n0.571\n1.288\n0.003\n0.002\n4145.0\n3169.0\n1.0\n\n\npsi_child\n-1.890\n0.305\n-2.502\n-1.353\n0.005\n0.003\n4022.0\n2883.0\n1.0\n\n\n\n\n\n\n\n\n\nInterpret model parameters\nSince we have fit a distributional model, we can leverage the plot_predictions() function in the interpret sub-package to visualize how the \\(\\text{ZIPoisson}\\) parameters \\(\\mu\\) and \\(\\psi\\) vary as a covariate changes.\n\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n\nbmb.interpret.plot_predictions(\n    zip_model,\n    zip_idata,\n    covariates=\"persons\",\n    ax=ax[0]\n)\nax[0].set_ylabel(\"mu (fish count)\")\nax[0].set_title(\"$\\\\mu$ as a function of persons\")\n\nbmb.interpret.plot_predictions(\n    zip_model,\n    zip_idata,\n    covariates=\"persons\",\n    target=\"psi\",\n    ax=ax[1]\n)\nax[1].set_title(\"$\\\\psi$ as a function of persons\");\n\n\n\n\n\n\n\n\nInterpreting the left plot (the \\(\\mu\\) parameter) as the number of people in a group fishing increases, so does the number of fish caught. The right plot (the \\(\\psi\\) parameter) shows that as the number of people in a group fishing increases, the probability of the Poisson process increases. One interpretation of this is that as the number of people in a group increases, the probability of catching no fish decreases.\n\n\nPosterior predictive distribution\nLastly, lets plot the posterior predictive distribution against the observed data to see how well the model fits the data. To plot the samples, a utility function is defined below to assist in the plotting of discrete values.\n\ndef adjust_lightness(color, amount=0.5):\n    import matplotlib.colors as mc\n    import colorsys\n    try:\n        c = mc.cnames[color]\n    except:\n        c = color\n    c = colorsys.rgb_to_hls(*mc.to_rgb(c))\n    return colorsys.hls_to_rgb(c[0], c[1] * amount, c[2])\n\ndef plot_ppc_discrete(idata, bins, ax):\n    \n    def add_discrete_bands(x, lower, upper, ax, **kwargs):\n        for i, (l, u) in enumerate(zip(lower, upper)):\n            s = slice(i, i + 2)\n            ax.fill_between(x[s], [l, l], [u, u], **kwargs)\n\n    var_name = list(idata.observed_data.data_vars)[0]\n    y_obs = idata.observed_data[var_name].to_numpy()\n    \n    counts_list = []\n    for draw_values in az.extract(idata, \"posterior_predictive\")[var_name].to_numpy().T:\n        counts, _ = np.histogram(draw_values, bins=bins)\n        counts_list.append(counts)\n    counts_arr = np.stack(counts_list)\n\n    qts_90 = np.quantile(counts_arr, (0.05, 0.95), axis=0)\n    qts_70 = np.quantile(counts_arr, (0.15, 0.85), axis=0)\n    qts_50 = np.quantile(counts_arr, (0.25, 0.75), axis=0)\n    qts_30 = np.quantile(counts_arr, (0.35, 0.65), axis=0)\n    median = np.quantile(counts_arr, 0.5, axis=0)\n\n    colors = [adjust_lightness(\"C0\", x) for x in [1.8, 1.6, 1.4, 1.2, 0.9]]\n\n    add_discrete_bands(bins, qts_90[0], qts_90[1], ax=ax, color=colors[0])\n    add_discrete_bands(bins, qts_70[0], qts_70[1], ax=ax, color=colors[1])\n    add_discrete_bands(bins, qts_50[0], qts_50[1], ax=ax, color=colors[2])\n    add_discrete_bands(bins, qts_30[0], qts_30[1], ax=ax, color=colors[3])\n\n    \n    ax.step(bins[:-1], median, color=colors[4], lw=2, where=\"post\")\n    ax.hist(y_obs, bins=bins, histtype=\"step\", lw=2, color=\"black\", align=\"mid\")\n    handles = [\n        Line2D([], [], label=\"Observed data\", color=\"black\", lw=2),\n        Line2D([], [], label=\"Posterior predictive median\", color=colors[4], lw=2)\n    ]\n    ax.legend(handles=handles)\n    return ax\n\n\nzip_pps = zip_model.predict(idata=zip_idata, kind=\"pps\", inplace=False)\n\nbins = np.arange(39)\nfig, ax = plt.subplots(figsize=(7, 3))\nax = plot_ppc_discrete(zip_pps, bins, ax)\nax.set_xlabel(\"Number of Fish Caught\")\nax.set_ylabel(\"Count\")\nax.set_title(\"ZIP model - Posterior Predictive Distribution\");\n\n\n\n\n\n\n\n\nThe model captures the number of zeros accurately. However, the model seems to slightly underestimate the counts 1 and 2. Nonetheless, the plot shows that the model captures the overall distribution of counts reasonably well."
  },
  {
    "objectID": "posts/2023-09-29-zip-models-bambi.html#hurdle-poisson",
    "href": "posts/2023-09-29-zip-models-bambi.html#hurdle-poisson",
    "title": "Zero Inflated Models in Bambi",
    "section": "Hurdle poisson",
    "text": "Hurdle poisson\nBoth ZIP and hurdle models both use two processes to generate data. The two models differ in their conceptualization of how the zeros are generated. In \\(\\text{ZIPoisson}\\), the zeroes can come from any of the processes, while in the hurdle Poisson they come only from one of the processes. Thus, a hurdle model assumes zero and positive values are generated from two independent processes. In the hurdle model, there are two components: (1) a “structural” process such as a binary model for modeling whether the response variable is zero or not, and (2) a process using a truncated model such as a truncated Poisson for modeling the counts. The result of these two components is a distribution that can be described as\n\\[P(Y=0) = 1 - \\psi\\]\n\\[P(Y=y_i) = \\psi \\frac{e^{-\\mu_i}\\mu_{i}^{y_i} / y_i!}{1 - e^{-\\mu_i}} \\ \\text{for} \\ y_i = 1, 2, 3,...,n\\]\nwhere \\(y_i\\) is the outcome, \\(\\mu\\) is the mean of the Poisson process where \\(\\mu \\ge 0\\), and \\(\\psi\\) is the probability of the Poisson process where \\(0 \\lt \\psi \\lt 1\\). The numerator of the second equation is the Poisson probability mass function, and the denominator is one minus the Poisson cumulative distribution function. This is a lot to digest. Again, let’s simulate some data to understand how data is generated from this process.\n\nx = np.arange(0, 22)\npsis = [0.7, 0.4]\nmus = [10, 4]\n\nplt.figure(figsize=(7, 3))\nfor psi, mu in zip(psis, mus):\n    pmf = stats.poisson.pmf(x, mu) # pmf evaluated at x given mu\n    cdf = stats.poisson.cdf(0, mu) # cdf evaluated at 0 given mu\n    pmf[0] = 1 - psi # 1.) generate zeros\n    pmf[1:] =  (psi * pmf[1:]) / (1 - cdf) # 2.) generate counts\n    pmf /= pmf.sum() # normalize to get probabilities\n    plt.plot(x, pmf, '-o', label='$\\\\psi$ = {}, $\\\\mu$ = {}'.format(psi, mu))\n\nplt.title(\"Hurdle Poisson Process\")\nplt.xlabel('x', fontsize=12)\nplt.ylabel('f(x)', fontsize=12)\nplt.legend(loc=1)\nplt.show()\n\n\n\n\n\n\n\n\nThe differences between the ZIP and hurdle models are subtle. Notice how in the code for the hurdle Poisson process, the zero counts are generate by (1 - psi) versus (1 - psi) + pmf[0] for the ZIP process. Additionally, the positive observations are generated by the process (psi * pmf[1:]) / (1 - cdf) where the numerator is a vector of probabilities for positive counts scaled by \\(\\psi\\) and the denominator uses the Poisson cumulative distribution function to evaluate the probability a count is greater than 0.\n\nHurdle regression model\nTo add predictors in the hurdle model, we follow the same specification as in the ZIP regression model section since both models have the same structure. The only difference is that the hurdle model uses a truncated Poisson distribution instead of a ZIP distribution. Right away, we will model both the parent and non-parent parameter as a function of the predictors.\n\nhurdle_formula = bmb.Formula(\n    \"count ~ livebait + camper + persons + child\", # parent parameter mu\n    \"psi ~ livebait + camper + persons + child\"    # non-parent parameter psi\n)\n\nhurdle_model = bmb.Model(\n    hurdle_formula, \n    fish_data, \n    family='hurdle_poisson'\n)\n\nhurdle_idata = hurdle_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, livebait, camper, persons, child, psi_Intercept, psi_livebait, psi_camper, psi_persons, psi_child]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:06&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 6 seconds.\n\n\n\nhurdle_model\n\n       Formula: count ~ livebait + camper + persons + child\n                psi ~ livebait + camper + persons + child\n        Family: hurdle_poisson\n          Link: mu = log\n                psi = logit\n  Observations: 248\n        Priors: \n    target = mu\n        Common-level effects\n            Intercept ~ Normal(mu: 0.0, sigma: 9.5283)\n            livebait ~ Normal(mu: 0.0, sigma: 7.2685)\n            camper ~ Normal(mu: 0.0, sigma: 5.0733)\n            persons ~ Normal(mu: 0.0, sigma: 2.2583)\n            child ~ Normal(mu: 0.0, sigma: 2.9419)\n    target = psi\n        Common-level effects\n            psi_Intercept ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_livebait ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_camper ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_persons ~ Normal(mu: 0.0, sigma: 1.0)\n            psi_child ~ Normal(mu: 0.0, sigma: 1.0)\n------\n* To see a plot of the priors call the .plot_priors() method.\n* To see a summary or plot of the posterior pass the object returned by .fit() to az.summary() or az.plot_trace()\n\n\n\nhurdle_model.graph()\n\n\n\n\n\n\n\n\nAs the same link functions are used for ZIP and Hurdle model, the coefficients can be interpreted in a similar manner.\n\naz.summary(\n    hurdle_idata,\n    var_names=[\"Intercept\", \"livebait\", \"camper\", \"persons\", \"child\"], \n    filter_vars=\"like\"\n)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n-1.615\n0.363\n-2.278\n-0.915\n0.006\n0.005\n3832.0\n2121.0\n1.0\n\n\nlivebait[1.0]\n1.661\n0.329\n1.031\n2.273\n0.005\n0.004\n4149.0\n1871.0\n1.0\n\n\ncamper[1.0]\n0.271\n0.100\n0.073\n0.449\n0.001\n0.001\n6843.0\n2934.0\n1.0\n\n\npersons\n0.610\n0.045\n0.533\n0.700\n0.001\n0.000\n4848.0\n3196.0\n1.0\n\n\nchild\n-0.791\n0.094\n-0.970\n-0.618\n0.001\n0.001\n4371.0\n3006.0\n1.0\n\n\npsi_Intercept\n-2.780\n0.583\n-3.906\n-1.715\n0.008\n0.006\n4929.0\n3258.0\n1.0\n\n\npsi_livebait[1.0]\n0.764\n0.427\n-0.067\n1.557\n0.006\n0.005\n5721.0\n2779.0\n1.0\n\n\npsi_camper[1.0]\n0.849\n0.298\n0.283\n1.378\n0.004\n0.003\n5523.0\n2855.0\n1.0\n\n\npsi_persons\n1.040\n0.183\n0.719\n1.396\n0.003\n0.002\n3852.0\n3007.0\n1.0\n\n\npsi_child\n-2.003\n0.282\n-2.555\n-1.517\n0.004\n0.003\n4021.0\n3183.0\n1.0\n\n\n\n\n\n\n\n\nPosterior predictive samples\nAs with the ZIP model above, we plot the posterior predictive distribution against the observed data to see how well the model fits the data.\n\nhurdle_pps = hurdle_model.predict(idata=hurdle_idata, kind=\"pps\", inplace=False)\n\nbins = np.arange(39)\nfig, ax = plt.subplots(figsize=(7, 3))\nax = plot_ppc_discrete(hurdle_pps, bins, ax)\nax.set_xlabel(\"Number of Fish Caught\")\nax.set_ylabel(\"Count\")\nax.set_title(\"Hurdle Model - Posterior Predictive Distribution\");\n\n\n\n\n\n\n\n\nThe plot looks similar to the ZIP model above. Nonetheless, the plot shows that the model captures the overall distribution of counts reasonably well."
  },
  {
    "objectID": "posts/2023-09-29-zip-models-bambi.html#summary",
    "href": "posts/2023-09-29-zip-models-bambi.html#summary",
    "title": "Zero Inflated Models in Bambi",
    "section": "Summary",
    "text": "Summary\nIn this notebook, two classes of models (ZIP and hurdle Poisson) for modeling zero-inflated data were presented and implemented in Bambi. The difference of the data generating process between the two models differ in how zeros are generated. The ZIP model uses a distribution that mixes two data generating processes. The first process generates zeros, and the second process uses a Poisson distribution to generate counts (of which some may be zero). The hurdle Poisson also uses two data generating processes, but doesn’t “mix” them. A process is used for generating zeros such as a binary model for modeling whether the response variable is zero or not, and a second process for modeling the counts. These two proceses are independent of each other.\nThe dataset used to demonstrate the two models had a large number of zeros. These zeros appeared because the group doesn’t fish, or because they fished, but caught zero fish. Because zeros could be generated due to two different reasons, the ZIP model, which allows zeros to be generated from a mixture of processes, seems to be more appropriate for this datset.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Mon Sep 25 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\nseaborn   : 0.12.2\nnumpy     : 1.24.2\nscipy     : 1.11.2\nbambi     : 0.13.0.dev0\nmatplotlib: 3.7.1\narviz     : 0.16.1\npandas    : 2.1.0\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html",
    "href": "posts/2023-03-26-numpyro-log-joint.html",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "",
    "text": "In probabilistic programming languages (PPLs), one needs to compute the joint probability (often unnormalized) of values and observed variables under a generative model to perform approximate inference. However, given a model in the form of a Python function, how does one translate this function (model) into a log joint probability? The objective of this blog is to better understand how modern PPLs, in particular NumPyro, performs this translation in a dynamic way, i.e., the functions for performing this translation can handle a variety of models defined by the user."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#objective",
    "href": "posts/2023-03-26-numpyro-log-joint.html#objective",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "",
    "text": "In probabilistic programming languages (PPLs), one needs to compute the joint probability (often unnormalized) of values and observed variables under a generative model to perform approximate inference. However, given a model in the form of a Python function, how does one translate this function (model) into a log joint probability? The objective of this blog is to better understand how modern PPLs, in particular NumPyro, performs this translation in a dynamic way, i.e., the functions for performing this translation can handle a variety of models defined by the user."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#the-model",
    "href": "posts/2023-03-26-numpyro-log-joint.html#the-model",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "The Model",
    "text": "The Model\nThe example zero_inflated_poisson.py from the NumPyro docs will be used. In this example, the authors model and predict how many fish are caught by visitors in a state park. Many groups of visitors catch zero fish, either because they did not fish at all or because they were unlucky. They explicitly model this bimodal behavior (zero versus non-zero) and ascertain which variables contribute to each behavior. The authors answer this question by fitting a zero-inflated poisson regression model. We will use NUTs as the inference method to understand the model translation.\n\nWorkflow\n\nDefine model using NumPyro primitives\nConstruct a kernel for inference and feed model into kernel\nPerform inference using MCMC\n\n\n\nCode\nimport argparse\nimport os\nimport random\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\nimport jax.numpy as jnp\nfrom jax.random import PRNGKey\nimport jax.scipy as jsp\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer import MCMC, NUTS, SVI, Predictive, Trace_ELBO, autoguide\nfrom numpyro.infer import util\n\nmatplotlib.use(\"Agg\")  # noqa: E402\n\n\n\n\nCode\ndef set_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n\n\n\ndef model(X, Y):\n    D_X = X.shape[1]\n    b1 = numpyro.sample(\"b1\", dist.Normal(0.0, 1.0).expand([D_X]).to_event(1))\n    b2 = numpyro.sample(\"b2\", dist.Normal(0.0, 1.0).expand([D_X]).to_event(1))\n\n    q = jsp.special.expit(jnp.dot(X, b1[:, None])).reshape(-1)\n    lam = jnp.exp(jnp.dot(X, b2[:, None]).reshape(-1))\n\n    with numpyro.plate(\"obs\", X.shape[0]):\n        numpyro.sample(\"Y\", dist.ZeroInflatedPoisson(gate=q, rate=lam), obs=Y)\n\n\ndef run_mcmc(model, args, X, Y):\n    kernel = NUTS(model)\n    mcmc = MCMC(\n        kernel,\n        num_warmup=args.num_warmup,\n        num_samples=args.num_samples,\n        num_chains=args.num_chains,\n        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n    )\n    mcmc.run(PRNGKey(1), X, Y)\n    mcmc.print_summary()\n    return mcmc.get_samples()\n\n\n\nCode\ndef main(args):\n    set_seed(args.seed)\n\n    # prepare dataset\n    df = pd.read_stata(\"http://www.stata-press.com/data/r11/fish.dta\")\n    df[\"intercept\"] = 1\n    cols = [\"livebait\", \"camper\", \"persons\", \"child\", \"intercept\"]\n\n    mask = np.random.randn(len(df)) &lt; args.train_size\n    df_train = df[mask]\n    df_test = df[~mask]\n    X_train = jnp.asarray(df_train[cols].values)\n    y_train = jnp.asarray(df_train[\"count\"].values)\n    X_test = jnp.asarray(df_test[cols].values)\n    y_test = jnp.asarray(df_test[\"count\"].values)\n\n    print(\"run MCMC.\")\n    posterior_samples = run_mcmc(model, args, X_train, y_train)\n\n    predictive = Predictive(model, posterior_samples=posterior_samples)\n    predictions = predictive(PRNGKey(1), X=X_test, Y=None)\n    mcmc_predictions = jnp.rint(predictions[\"Y\"].mean(0))\n\n    print(\n        \"MCMC RMSE: \",\n        mean_squared_error(np.asarray(y_test), np.asarray(mcmc_predictions), squared=False),\n    )\n\n\n\n\nCode\nparser = argparse.ArgumentParser(\"Zero-Inflated Poisson Regression\")\nparser.add_argument(\"--seed\", nargs=\"?\", default=42, type=int)\nparser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=2000, type=int)\nparser.add_argument(\"--num-warmup\", nargs=\"?\", default=1000, type=int)\nparser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int)\nparser.add_argument(\"--num-data\", nargs=\"?\", default=100, type=int)\nparser.add_argument(\"--maxiter\", nargs=\"?\", default=5000, type=int)\nparser.add_argument(\"--train-size\", nargs=\"?\", default=0.8, type=float)\nparser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".')\nargs = parser.parse_args(\"\")\n\nnumpyro.set_platform(args.device)\nnumpyro.set_host_device_count(args.num_chains)\n\nmain(args)"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#initializing-the-kernel",
    "href": "posts/2023-03-26-numpyro-log-joint.html#initializing-the-kernel",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "Initializing the kernel",
    "text": "Initializing the kernel\nFirst, we initialize the NUTS kernel with the model. The word kernel is used in a wide range of fields ranging from probabilistic programming, statistics, and deep learning. In PPLs, the name kernel is typically used to define the interface with the sampling algorithm. In this case, we have initialized a NUTS kernel with our model, and this kernel will allow us to interface our model with the underlying HMC sampling variant NUTS.\nBut, the sampling algorithm can’t simply interface with a Python function. Our model, in the form of a Python function, needs to be translated into a joint log density function and used as input into the sampler. Here, this is where NumPyro performs a series of steps to perform this translation.\nWhen we “feed” the model into the NUTS class an initialize_model utility function is called. This function calls various helper functions such as get_potential_fn and find_valid_initial_params to return a tuple of (init_params_info, potential_fn, postprocess_fn, model_trace). Here, we are interested in initialize_model and get_potential_fn.\nThe graph of function calls looks like: initialize model \\(\\leftrightarrow\\) get potential fn \\(\\leftrightarrow\\) potential energy \\(\\leftrightarrow\\) log density where each function being called is also returning an object. Below, the sequential order of functions calls are described."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#initialize_model",
    "href": "posts/2023-03-26-numpyro-log-joint.html#initialize_model",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "initialize_model",
    "text": "initialize_model\ninitialize_model is a function that returns a tuple of objects and values used as input into the HMC algorithm. At a high level, our model and data are passed into the initialize_model function to intialize the model to some values using the observed data and numpyro.sample statements. This initialization allows us to perform inference with NUTS. Below, the various helper functions that are called within this function are described as these helpers constitute where the majority of our interest lies regarding translating a model into a log joint probability."
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#get_potential_fn",
    "href": "posts/2023-03-26-numpyro-log-joint.html#get_potential_fn",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "get_potential_fn",
    "text": "get_potential_fn\nInside of intialize_model, the function get_potential_fn is called. Given a model with Pyro primitives, this Python function returns another function which, given unconstrained parameters, evaluates the potential energy (negative log joint density). In addition, this returns a function to transform unconstrained values at sample sites to constrained values within their respective support.\nThe interesting parts here are the evaluation of potential energy and the returns a function. First, we focus on the function potential_energy to evaluate the potential energy. Later, we then return to the potential_fn object.\n\n\nCode\ndef get_potential_fn(\n    model,\n    inv_transforms,\n    *,\n    enum=False,\n    replay_model=False,\n    dynamic_args=False,\n    model_args=(),\n    model_kwargs=None,\n):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Given a model with Pyro primitives, returns a\n    function which, given unconstrained parameters, evaluates the potential\n    energy (negative log joint density). In addition, this returns a\n    function to transform unconstrained values at sample sites to constrained\n    values within their respective support.\n\n    :param model: Python callable containing Pyro primitives.\n    :param dict inv_transforms: dictionary of transforms keyed by names.\n    :param bool enum: whether to enumerate over discrete latent sites.\n    :param bool replay_model: whether we need to replay model in\n        `postprocess_fn` to obtain `deterministic` sites.\n    :param bool dynamic_args: if `True`, the `potential_fn` and\n        `constraints_fn` are themselves dependent on model arguments.\n        When provided a `*model_args, **model_kwargs`, they return\n        `potential_fn` and `constraints_fn` callables, respectively.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :return: tuple of (`potential_fn`, `postprocess_fn`). The latter is used\n        to constrain unconstrained samples (e.g. those returned by HMC)\n        to values that lie within the site's support, and return values at\n        `deterministic` sites in the model.\n    \"\"\"\n    if dynamic_args:\n        potential_fn = partial(\n            _partial_args_kwargs, partial(potential_energy, model, enum=enum)\n        )\n        if replay_model:\n            # XXX: we seed to sample discrete sites (but not collect them)\n            model_ = seed(model.fn, 0) if enum else model\n            postprocess_fn = partial(\n                _partial_args_kwargs,\n                partial(constrain_fn, model, return_deterministic=True),\n            )\n        else:\n            postprocess_fn = partial(\n                _drop_args_kwargs, partial(transform_fn, inv_transforms)\n            )\n    else:\n        model_kwargs = {} if model_kwargs is None else model_kwargs\n        potential_fn = partial(\n            potential_energy, model, model_args, model_kwargs, enum=enum\n        )\n        if replay_model:\n            model_ = seed(model.fn, 0) if enum else model\n            postprocess_fn = partial(\n                constrain_fn,\n                model_,\n                model_args,\n                model_kwargs,\n                return_deterministic=True,\n            )\n        else:\n            postprocess_fn = partial(transform_fn, inv_transforms)\n\n    print(f\"potential_fn: {potential_fn}\")\n    return potential_fn, postprocess_fn"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#potential_energy",
    "href": "posts/2023-03-26-numpyro-log-joint.html#potential_energy",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "potential_energy",
    "text": "potential_energy\nComputes potential energy (negative joint log density) of a model given unconstrained parameters. Under the hood, NumPyro will transform these unconstrained parameters to the values belonging to the supports of the corresponding priors in the model. To compute the potential energy, this function calls a log_density function that computes the log of joint density for the model given the latent values (parameters).\n\n\nCode\ndef potential_energy(model, model_args, model_kwargs, params, enum=False):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Computes potential energy of a model given unconstrained params.\n    Under the hood, we will transform these unconstrained parameters to the values\n    belong to the supports of the corresponding priors in `model`.\n\n    :param model: a callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: unconstrained parameters of `model`.\n    :param bool enum: whether to enumerate over discrete latent sites.\n    :return: potential energy given unconstrained parameters.\n    \"\"\"\n    if enum:\n        from numpyro.contrib.funsor import log_density as log_density_\n    else:\n        log_density_ = log_density\n\n    substituted_model = substitute(\n        model, substitute_fn=partial(_unconstrain_reparam, params)\n    )\n    # no param is needed for log_density computation because we already substitute\n    log_joint, model_trace = log_density_(\n        substituted_model, model_args, model_kwargs, {}\n    )\n    print(f\"-log_joint: {log_joint}\")\n    return -log_joint\n\n\nGiven our NumPyro model, data (model_args), and initialized parameters (using numpyro.sample), the potential energy (negative log joint density) is the following output:\n-log_joint: Traced&lt;ConcreteArray([ 15.773586   15.796449   15.68768    17.106882   16.307858   15.10714\n  16.277817   16.401972   17.409088   15.488239   17.44108    20.011204\n  15.796449   16.401972   17.409088   15.488239   32.46398    16.306961\n  18.321064   15.776845   16.432753   41.972443   16.90719    17.324219\n  15.487675   15.68768    15.10714    15.68768    16.96474    61.152782\n  16.319      32.46398    56.631355   16.733091   44.390583   15.796449\n  31.771408   15.9031725  17.999393   15.929144   15.796449   15.776845\n  25.234818   15.487675   32.46398    15.776845   22.136528   16.745249\n  15.796449   15.796449   61.152782   17.459694   15.776845   39.30664\n  31.771408   17.106882   15.796449   15.776845   16.836826   16.90372\n  15.565512   15.266311   15.796449   15.487675   25.503807   66.416145\n  42.01054    15.68768    16.438005   35.528217   16.401972  275.8356\n  15.488239   46.813717   18.31475    42.01054    15.929144   16.733091\n  15.929144   18.632296   16.553946   22.139755   16.879503   16.253452\n  15.929144   16.68712    16.90719    17.409088   16.306961   17.900412\n  72.883484   20.984446   17.080605   15.68768    15.266311   17.459694\n  15.487675   17.409088  221.04407    15.487675   15.68768    15.68768\n  15.903938   17.608683   17.233418   16.945618   17.102604   16.230682\n  16.401972   20.437622   16.307858   15.776845   66.416145   22.85551\n  17.459694   15.266729   18.263693   16.733091   15.68768    16.69443\n  17.767635   16.892221   16.277817   16.699389   15.796449   15.776845\n  15.68768    17.459694   18.93738    16.401972   41.471767   15.796449\n  82.16476    16.664154   15.68768    17.409088   17.106882   15.488239\n  15.266729   17.917303   26.629543   21.383934   18.279554   15.929144\n  16.90719    38.06461    16.673416   15.487675   16.253452   15.776845\n  16.306961   41.61213    15.9031725  15.488239   19.056816   30.152964\n  18.068584   15.796449   15.773586   16.216064   17.080605   16.798647\n  16.733091   16.307858   38.06461    15.487675   16.69443    66.416145\n  16.733091   39.110504   15.266729   15.796449   16.401972   18.379236\n  15.929144   16.276924   15.929144   16.306961  360.43808    15.929144\n  20.011204   15.776845   15.796449   15.487675   39.291206   15.68768\n  15.488239   30.152964   16.879503   15.929144   16.306961   16.69443\n  16.401972   16.553946   15.796449   16.673416   17.080605  379.3062\n  16.745249   17.896193   16.90719    15.266729   15.776845   15.776845 ], \n  dtype=float32)"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#log_density",
    "href": "posts/2023-03-26-numpyro-log-joint.html#log_density",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "log_density",
    "text": "log_density\nThe log_density function first uses the effect handler substitute to return a callable which substitutes all primitive calls in fn with values from data whose key matches the site name. If the site name is not present in data, then there is no side effect. After substitute, another effect handler trace is used to record inputs, distributions, and outputs of numpyro.sample statements in the model, and NumPyro primitive calls, generally speaking.\n\n\nCode\ndef log_density(model, model_args, model_kwargs, params):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n    latent values ``params``.\n\n    :param model: Python callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: dictionary of current parameter values keyed by site\n        name.\n    :return: log of joint density and a corresponding model trace\n    \"\"\"\n    \n    model = substitute(model, data=params)\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n\n\nThe effect handlers allow us to effectively loop through each site in the model trace to compute the joint log probability density. In the for loop, if the site type == sample grab that sites value(s) (the samples from the numpyro.sample statement) and evaluate the log probability of the value(s) for that sites fn (dist.Normal(), dist.MultivariateNormal(), etc.) with site['fn'].log_prob(&lt;some value&gt;) The output snippet below shows the site b1 defined in the model and the value sampled using the numpyro.sample statement.\nsite: {'type': 'sample', 'name': 'b1', 'fn': &lt;numpyro.distributions.distribution.Independent object at 0x13a9ceb20&gt;, 'args': (), 'kwargs': {'rng_key': None, 'sample_shape': ()}, 'value': Traced&lt;ConcreteArray([ 1.2406311  -0.5222316  -1.2795658   1.800642   -0.43796206], dtype=float32)&gt;with&lt;JVPTrace(level=2/0)&gt; with\n  primal = Array([ 1.2406311 , -0.5222316 , -1.2795658 ,  1.800642  , -0.43796206],      dtype=float32)\n  tangent = Traced&lt;ShapedArray(float32[5])&gt;with&lt;JaxprTrace(level=1/0)&gt; with\n    pval = (ShapedArray(float32[5]), None)\n    recipe = LambdaBinding(), 'scale': None, 'is_observed': False, 'intermediates': [], 'cond_indep_stack': [], 'infer': {}} \n\nvalue: Traced&lt;ConcreteArray([ 1.2406311  -0.5222316  -1.2795658   1.800642   -0.43796206], dtype=float32)&gt;with&lt;JVPTrace(level=2/0)&gt; with\n  primal = Array([ 1.2406311 , -0.5222316 , -1.2795658 ,  1.800642  , -0.43796206],      dtype=float32)\n  tangent = Traced&lt;ShapedArray(float32[5])&gt;with&lt;JaxprTrace(level=1/0)&gt; with\n    pval = (ShapedArray(float32[5]), None)\n    recipe = LambdaBinding(), \nSubsequently, we can also see the fn of this sample site defined in our model:\nsite fn: &lt;numpyro.distributions.distribution.Independent object at 0x13a9ceb20&gt;\nwhere the fn is an Independent Normal distribution because we called the .to_event(1) method in our model. Next, the log probability for the sample site value is computed by calling the .log_prob() method. For example, the log probability of the sampled values for site b1 is:\nlog prob.    = Traced&lt;ConcreteArray(-8.036343574523926, dtype=float32)\nSubsequently, we sum over the log probability for that site. Lastly, the variable log_joint is created for the log joint probability density, and the current site log probability is added to the log joint probability. After looping through each sample site, the log joint then represents the log joint probability density for the model given the latent values (parameters).\nlog joint         = Traced&lt;ConcreteArray([ -15.773586   -15.796449   -15.68768    -17.106882   -16.307858\n  -15.10714    -16.277817   -16.401972   -17.409088   -15.488239\n  -17.44108    -20.011204   -15.796449   -16.401972   -17.409088\n  -15.488239   -32.46398    -16.306961   -18.321064   -15.776845\n  -16.432753   -41.972443   -16.90719    -17.324219   -15.487675\n  -15.68768    -15.10714    -15.68768    -16.96474    -61.152782\n  -16.319      -32.46398    -56.631355   -16.733091   -44.390583\n  -15.796449   -31.771408   -15.9031725  -17.999393   -15.929144\n  -15.796449   -15.776845   -25.234818   -15.487675   -32.46398\n  -15.776845   -22.136528   -16.745249   -15.796449   -15.796449\n  -61.152782   -17.459694   -15.776845   -39.30664    -31.771408\n  -17.106882   -15.796449   -15.776845   -16.836826   -16.90372\n  -15.565512   -15.266311   -15.796449   -15.487675   -25.503807\n  -66.416145   -42.01054    -15.68768    -16.438005   -35.528217\n  -16.401972  -275.8356     -15.488239   -46.813717   -18.31475\n  -42.01054    -15.929144   -16.733091   -15.929144   -18.632296\n  -16.553946   -22.139755   -16.879503   -16.253452   -15.929144\n  -16.68712    -16.90719    -17.409088   -16.306961   -17.900412\n  -72.883484   -20.984446   -17.080605   -15.68768    -15.266311\n  -17.459694   -15.487675   -17.409088  -221.04407    -15.487675\n  -15.68768    -15.68768    -15.903938   -17.608683   -17.233418\n  -16.945618   -17.102604   -16.230682   -16.401972   -20.437622\n  -16.307858   -15.776845   -66.416145   -22.85551    -17.459694\n  -15.266729   -18.263693   -16.733091   -15.68768    -16.69443\n  -17.767635   -16.892221   -16.277817   -16.699389   -15.796449\n  -15.776845   -15.68768    -17.459694   -18.93738    -16.401972\n  -41.471767   -15.796449   -82.16476    -16.664154   -15.68768\n  -17.409088   -17.106882   -15.488239   -15.266729   -17.917303\n  -26.629543   -21.383934   -18.279554   -15.929144   -16.90719\n  -38.06461    -16.673416   -15.487675   -16.253452   -15.776845\n  -16.306961   -41.61213    -15.9031725  -15.488239   -19.056816\n  -30.152964   -18.068584   -15.796449   -15.773586   -16.216064\n  -17.080605   -16.798647   -16.733091   -16.307858   -38.06461\n  -15.487675   -16.69443    -66.416145   -16.733091   -39.110504\n  -15.266729   -15.796449   -16.401972   -18.379236   -15.929144\n  -16.276924   -15.929144   -16.306961  -360.43808    -15.929144\n  -20.011204   -15.776845   -15.796449   -15.487675   -39.291206\n  -15.68768    -15.488239   -30.152964   -16.879503   -15.929144\n  -16.306961   -16.69443    -16.401972   -16.553946   -15.796449\n  -16.673416   -17.080605  -379.3062     -16.745249   -17.896193\n  -16.90719    -15.266729   -15.776845   -15.776845 ], dtype=float32)\nAnd voila, this output is the log joint probability density (and placing a negative sign in front of this array gives you the potential energy) for the model given the latent values. The values in the output above represent the log joint probability of the initialized latent valus, i.e., no inference has been ran yet.\n\n\nCode\ndef log_density(model, model_args, model_kwargs, params):\n    \"\"\"\n    (EXPERIMENTAL INTERFACE) Computes log of joint density for the model given\n    latent values ``params``.\n\n    :param model: Python callable containing NumPyro primitives.\n    :param tuple model_args: args provided to the model.\n    :param dict model_kwargs: kwargs provided to the model.\n    :param dict params: dictionary of current parameter values keyed by site\n        name.\n    :return: log of joint density and a corresponding model trace\n    \"\"\"\n    \n    model = substitute(model, data=params)\n    model_trace = trace(model).get_trace(*model_args, **model_kwargs)\n    log_joint = jnp.zeros(())\n    print(\"---- inside log_density -----\")\n    print('\\n')\n    for site in model_trace.values():\n        print(f\"site: {site}\", '\\n')\n        if site[\"type\"] == \"sample\":\n            value = site[\"value\"]\n            print(f\"value: {value}, \\n\")\n            intermediates = site[\"intermediates\"]\n            print(f\"intermediates: {intermediates}, \\n\")\n            scale = site[\"scale\"]\n            print(f\"site fn: {site['fn']}, \\n\")\n            if intermediates:\n                log_prob = site[\"fn\"].log_prob(value, intermediates)\n            else:\n                guide_shape = jnp.shape(value)\n                model_shape = tuple(\n                    site[\"fn\"].shape()\n                )  # TensorShape from tfp needs casting to tuple\n                try:\n                    broadcast_shapes(guide_shape, model_shape)\n                except ValueError:\n                    raise ValueError(\n                        \"Model and guide shapes disagree at site: '{}': {} vs {}\".format(\n                            site[\"name\"], model_shape, guide_shape\n                        )\n                    )\n                log_prob = site[\"fn\"].log_prob(value)\n\n            if (scale is not None) and (not is_identically_one(scale)):\n                log_prob = scale * log_prob\n\n            # print(f\"before sum log prob.    = {log_prob}, \\n\")\n            # log_prob = jnp.sum(log_prob)\n            # print(f\"after sum log prob.     = {log_prob}, \\n\")\n            log_joint = log_joint + log_prob\n            print(f\"log joint               = {log_joint}, \\n\")\n    return log_joint, model_trace"
  },
  {
    "objectID": "posts/2023-03-26-numpyro-log-joint.html#returning-to-get_potential_fn",
    "href": "posts/2023-03-26-numpyro-log-joint.html#returning-to-get_potential_fn",
    "title": "Translating a Model into a Log Joint Probability",
    "section": "Returning to get_potential_fn",
    "text": "Returning to get_potential_fn\nHowever, log_density and potential_energy compute the log probability of the current latent value, not a function. Therefore, we return to the Python function get_potential_fn that returns the log joint probability as a function potential_fn, i.e., a function that will evaluate the potential energy given the model args defined by our model, i.e., X_train and y_train and the latent values using the log_density function described above. The potential_fn is then “fed” into the HMC algorithm to perform inference.\nWhat’s great about get_potential_fn is that the log joint probability density function can be accessed externally given our NumPyro model, and passed into other sampling libraries such as Blackjax."
  },
  {
    "objectID": "posts/2023-11-28-constrained-bayesopt.html",
    "href": "posts/2023-11-28-constrained-bayesopt.html",
    "title": "Outcome Constraints in Bayesian Optimization",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport torch\nimport numpy as np\n\nfrom botorch.acquisition import qLogExpectedImprovement\nfrom botorch.fit import fit_gpytorch_model\nfrom botorch.models import SingleTaskGP\nfrom botorch.optim import optimize_acqf\nfrom gpytorch.mlls import ExactMarginalLogLikelihood\nfrom torch.distributions import Normal\n\nplt.style.use(\"https://raw.githubusercontent.com/GStechschulte/filterjax/main/docs/styles.mplstyle\")"
  },
  {
    "objectID": "posts/2023-11-28-constrained-bayesopt.html#probability-of-feasibility",
    "href": "posts/2023-11-28-constrained-bayesopt.html#probability-of-feasibility",
    "title": "Outcome Constraints in Bayesian Optimization",
    "section": "Probability of feasibility",
    "text": "Probability of feasibility\nIn BoTorch it is common to use a Gaussian Process (GP) to model the objective function. The output of the GP is a Gaussian distribution over the predicted values for a given set of input points. It provides not just a single point estimate but a probabilistic prediction that accounts for uncertainty in the predictions. Thus, for each point in the search space, we have a corresponding Gaussian distribution representing the belief of the objective value at that point.\nIntuitively, if we have defined an outcome constraint, we can compute the probability that \\(f(x)\\) is feasible by taking the cumulative distribution function (CDF) of the predictive distribution and computing the area between the lower bound and the upper bound. For example, imagine a GP has made a prediction given an input \\(x\\). This predictive distribution of the outcome \\(y\\) is shown below. The prediction is normally distributed around \\(0.0\\) with plausible predictions ranging from \\(-3\\) to \\(3\\). Additionally, there is an outcome constraint of the form\n\\[0 &lt; f(x)\\]\nThe probability that the prediction is feasible (above \\(0\\)) is computed using the CDF of the predictive distribution. In this example, the probability of feasibility is \\(0.5\\). As will be shown below, this probability can then multiplied by the policy score to get the constrained policy score.\n\n\nCode\nxs = torch.linspace(-3, 3, 100)\n\nnormal = Normal(torch.tensor([0.0]), torch.tensor([1.0]))\nxs_eval = torch.exp(normal.log_prob(xs)) / torch.exp(normal.log_prob(xs)).sum()\n\ncdf = normal.cdf(xs)\nabove = torch.where(cdf &gt; 0.5)\nprob_feasibility = normal.cdf(torch.zeros(1)).item()\n\nplt.figure(figsize=(7, 3))\nplt.plot(xs, xs_eval, label=\"Predictive distribution\")\nplt.fill_between(xs, xs_eval, where=xs &gt; 0, alpha=0.5, label=\"Feasible region\")\nplt.xlabel('Predicted outcome $y$')\nplt.ylabel('Probability')\nplt.title(f\"Area under the shaded curve: {prob_feasibility:.2f}\")\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/2023-11-28-constrained-bayesopt.html#constrained-policy",
    "href": "posts/2023-11-28-constrained-bayesopt.html#constrained-policy",
    "title": "Outcome Constraints in Bayesian Optimization",
    "section": "Constrained Policy",
    "text": "Constrained Policy\nWith the probability of feasibility computed, we can scale the policy, e.g. expected improvement (EI), score of each unseen point in the search space by the probability the point is feasible.\n\nIf the data point is likely to satisfy the constraints, then its EI score will be multiplied by a large number (a high probability of feasibility), thus keeping the EI score high.\nIf the data point is unlikely to satisfy the constraints, then its EI score will be multiplied by a small number (a small probability of feasibility), thus keeping the EI score small.\n\nTo implement inequality outcome constraints, acquisition functions that utilize Monte-Carlo (MC) sampling are used as this allows us to directly pass a list of constraint callables. These are any acquisition functions that inherit from SampleReducingMCAcqquisitionFunction."
  },
  {
    "objectID": "posts/2023-11-28-constrained-bayesopt.html#implementation",
    "href": "posts/2023-11-28-constrained-bayesopt.html#implementation",
    "title": "Outcome Constraints in Bayesian Optimization",
    "section": "Implementation",
    "text": "Implementation\nTo implement inequality outcome constraints, only a list of constraint callables which map a Tensor of posterior samples of dimension sample_shape x batch-shape x q x m-dim to a sample_shape x batch-shape x q-dim Tensor. The associated constraints are considered satisfied if the output is less than zero. In the example below, we aim to minimize the Forrester function subject to the following constraint that \\[f(x) &lt; 0\\]\nNote: Since we are minimizing, the objective function is inverted, and thus the inequality is also inverted.\n\ndef objective_fn(x):\n    return -((x + 1) ** 2) * torch.sin(2 * x + 2) / 5 + 1 + x / 3\n\n\n\nCode\nlb, ub = -5, 5\nbounds = torch.tensor([[lb], [ub]], dtype=torch.float)\nxs = torch.linspace(lb, ub, 100).unsqueeze(1)\nys = -objective_fn(xs)\n\nn = 5\ntrain_x = bounds[0] + (bounds[1] - bounds[0]) * torch.rand(n, 1, dtype=torch.double)\ntrain_y = -objective_fn(train_x)\n\nplt.figure(figsize=(7, 3))\nplt.plot(xs, ys, label=\"Objective\")\nplt.scatter(train_x, train_y, color=\"black\", label=\"Observations\")\nplt.axhline(y=0, color=\"k\", linestyle=\"--\", label=\"Upper Bound\")\nplt.legend();\n\n\n\n\n\n\n\n\n\nThe Bayesian optimization loop below uses the qLogExpectedImprovement policy. To impose the desired inequality outcome constraint \\(f(x) &lt; 0\\), a list of callables [lambda Z: Z.squeeze(-1) - upper] is passed to constraints. This callable subtracts the posterior samples \\(Z\\) by upper which is \\(0.0\\). If the result of this is less than zero, then the constraint is satisfied.\nNote that \\(Z\\) here would be passing in all outcomes if a multi-task GP had been defined, so you want to index into \\(Z\\) appropriately and make separate callables for each outcome, e.g. constraints=[lambda Z: Z[..., constraint_outcome_idx]]. However, in this example, there is only one outcome, so we can just use Z.squeeze(-1) to select the correct (and only) outcome dimension.\n\nn_iterations = 20\nupper = 0.\nfor iteration in range(n_iterations):\n    print(f\"iter: {iteration}\")\n\n    model = SingleTaskGP(train_x, train_y, )\n    mll = ExactMarginalLogLikelihood(model.likelihood, model)\n    fit_gpytorch_model(mll)\n\n    logEI = qLogExpectedImprovement(\n        model, \n        best_f=train_y[train_y &lt; 0].max(), \n        constraints=[\n            lambda Z: Z.squeeze(-1) - upper\n            # if there was also a lower bound\n            # lambda Z: lower - Z.squeeze(-1),\n        ]\n    )\n\n    new_x, _ = optimize_acqf(\n        acq_function=logEI,\n        bounds=bounds,\n        q=1,  \n        num_restarts=5,\n        raw_samples=20,\n    )\n\n    new_y = -objective_fn(new_x)\n\n    train_x = torch.cat([train_x, new_x])\n    train_y = torch.cat([train_y, new_y])\n\nWe can then evaluate the policy on unseen data and plot the proposed samples (queries).\n\nwith torch.no_grad():\n    acquisition_score = logEI(xs.unsqueeze(1))\n\n\nfig, ax = plt.subplots(\n    2, 1, \n    figsize=(8, 4), \n    sharex=True, \n    gridspec_kw={\"height_ratios\": [2, 1]}\n)\n\nax[0].scatter(train_x[:5], -train_y[:5], color=\"black\", label=\"observed samples\")\nax[0].scatter(train_x[5:], -train_y[5:], color=\"red\", label=\"proposal samples\")\nax[0].plot(xs, -ys, label=\"objective function\")\nax[0].legend()\n\nax[1].plot(xs, acquisition_score, label=\"acquisition score\")\nax[1].legend();\n\n\n\n\n\n\n\n\nThe objective function has been flipped back to its original form to visually evaluate the optimization loop. Notice how the majority of the proposed points that minimize the objective function are near \\(0\\). Points that are below \\(0\\) happen due to the fact that we are using a probabilistic surrogate model to compute the probability of feasibility. The predictions of this model are not perfect, and thus, it is possible that the optimized policy score informs the next query to be a point below \\(0.0\\). Nonetheless, the minimizing points are found near \\(-4.2\\), \\(-2.5\\), and \\(-1.5\\)."
  },
  {
    "objectID": "posts/2024-03-29-alternative_samplers.html",
    "href": "posts/2024-03-29-alternative_samplers.html",
    "title": "Using Alternative Samplers in Bambi",
    "section": "",
    "text": "This blog post is a copy of the alternative samplers documentation I wrote for Bambi. The original post can be found here.\nIn Bambi, the sampler used is automatically selected given the type of variables used in the model. For inference, Bambi supports both MCMC and variational inference. By default, Bambi uses PyMC’s implementation of the adaptive Hamiltonian Monte Carlo (HMC) algorithm for sampling. Also known as the No-U-Turn Sampler (NUTS). This sampler is a good choice for many models. However, it is not the only sampling method, nor is PyMC the only library implementing NUTS.\nTo this extent, Bambi supports multiple backends for MCMC sampling such as NumPyro and Blackjax. This notebook will cover how to use such alternatives in Bambi.\nNote: Bambi utilizes bayeux to access a variety of sampling backends. Thus, you will need to install the optional dependencies in the Bambi pyproject.toml file to use these backends.\n\nimport arviz as az\nimport bambi as bmb\nimport bayeux as bx\nimport numpy as np\nimport pandas as pd\n\nWARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n\n\n\n\nBambi leverages bayeux to access different sampling backends. In short, bayeux lets you write a probabilistic model in JAX and immediately have access to state-of-the-art inference methods.\nSince the underlying Bambi model is a PyMC model, this PyMC model can be “given” to bayeux. Then, we can choose from a variety of MCMC methods to perform inference.\nTo demonstrate the available backends, we will fist simulate data and build a model.\n\nnum_samples = 100\nnum_features = 1\nnoise_std = 1.0\nrandom_seed = 42\n\nnp.random.seed(random_seed)\n\ncoefficients = np.random.randn(num_features)\nX = np.random.randn(num_samples, num_features)\nerror = np.random.normal(scale=noise_std, size=num_samples)\ny = X @ coefficients + error\n\ndata = pd.DataFrame({\"y\": y, \"x\": X.flatten()})\n\n\nmodel = bmb.Model(\"y ~ x\", data)\nmodel.build()\n\nWe can call bmb.inference_methods.names that returns a nested dictionary of the backends and list of inference methods.\n\nmethods = bmb.inference_methods.names\nmethods\n\n{'pymc': {'mcmc': ['mcmc'], 'vi': ['vi']},\n 'bayeux': {'mcmc': ['tfp_hmc',\n   'tfp_nuts',\n   'tfp_snaper_hmc',\n   'blackjax_hmc',\n   'blackjax_chees_hmc',\n   'blackjax_meads_hmc',\n   'blackjax_nuts',\n   'blackjax_hmc_pathfinder',\n   'blackjax_nuts_pathfinder',\n   'flowmc_rqspline_hmc',\n   'flowmc_rqspline_mala',\n   'flowmc_realnvp_hmc',\n   'flowmc_realnvp_mala',\n   'numpyro_hmc',\n   'numpyro_nuts']}}\n\n\nWith the PyMC backend, we have access to their implementation of the NUTS sampler and mean-field variational inference.\n\nmethods[\"pymc\"]\n\n{'mcmc': ['mcmc'], 'vi': ['vi']}\n\n\nbayeux lets us have access to Tensorflow probability, Blackjax, FlowMC, and NumPyro backends.\n\nmethods[\"bayeux\"]\n\n{'mcmc': ['tfp_hmc',\n  'tfp_nuts',\n  'tfp_snaper_hmc',\n  'blackjax_hmc',\n  'blackjax_chees_hmc',\n  'blackjax_meads_hmc',\n  'blackjax_nuts',\n  'blackjax_hmc_pathfinder',\n  'blackjax_nuts_pathfinder',\n  'flowmc_rqspline_hmc',\n  'flowmc_rqspline_mala',\n  'flowmc_realnvp_hmc',\n  'flowmc_realnvp_mala',\n  'numpyro_hmc',\n  'numpyro_nuts']}\n\n\nThe values of the MCMC and VI keys in the dictionary are the names of the argument you would pass to inference_method in model.fit. This is shown in the section below.\n\n\n\nBy default, Bambi uses the PyMC NUTS implementation. To use a different backend, pass the name of the bayeux MCMC method to the inference_method parameter of the fit method.\n\n\n\nblackjax_nuts_idata = model.fit(inference_method=\"blackjax_nuts\")\nblackjax_nuts_idata\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 100kB\nDimensions:    (chain: 8, draw: 500)\nCoordinates:\n  * chain      (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw       (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    Intercept  (chain, draw) float64 32kB 0.04421 0.1077 ... 0.0259 0.06753\n    x          (chain, draw) float64 32kB 0.1353 0.232 0.5141 ... 0.2195 0.5014\n    y_sigma    (chain, draw) float64 32kB 0.9443 0.9102 0.922 ... 0.9597 0.9249\nAttributes:\n    created_at:                  2024-04-13T05:34:49.761913+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 500Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (3)Intercept(chain, draw)float640.04421 0.1077 ... 0.0259 0.06753array([[ 0.04420881,  0.10774889, -0.01477631, ...,  0.05546068,\n        -0.01057083,  0.09897323],\n       [ 0.00694954,  0.10744512, -0.017276  , ...,  0.19618115,\n         0.06402486, -0.01106827],\n       [ 0.16110577, -0.07458938,  0.04475104, ...,  0.16745381,\n        -0.00406837,  0.07311051],\n       ...,\n       [ 0.09943931, -0.03684845,  0.09735818, ..., -0.06556524,\n         0.11011645,  0.08414361],\n       [-0.07703379,  0.02738655,  0.02285994, ...,  0.14379745,\n        -0.10339471, -0.02836366],\n       [ 0.04903997, -0.03220716, -0.02720002, ...,  0.17203999,\n         0.02589751,  0.06752773]])x(chain, draw)float640.1353 0.232 ... 0.2195 0.5014array([[0.13526029, 0.23196226, 0.51413147, ..., 0.23278954, 0.32745043,\n        0.37862773],\n       [0.40584773, 0.51513052, 0.2268538 , ..., 0.41687492, 0.30601076,\n        0.2634667 ],\n       [0.41543724, 0.44571834, 0.23530532, ..., 0.6172463 , 0.29822452,\n        0.45765768],\n       ...,\n       [0.49946851, 0.29694244, 0.44142996, ..., 0.26425056, 0.46471836,\n        0.32217591],\n       [0.41877449, 0.33327679, 0.4045056 , ..., 0.66448843, 0.24280931,\n        0.50115044],\n       [0.51180277, 0.42393989, 0.56394504, ..., 0.29234944, 0.21949889,\n        0.5013853 ]])y_sigma(chain, draw)float640.9443 0.9102 ... 0.9597 0.9249array([[0.94428889, 0.91016104, 0.92196855, ..., 0.83634906, 0.79627853,\n        1.08163408],\n       [0.87025311, 0.85044922, 0.91347637, ..., 1.0028945 , 0.77749843,\n        0.87518191],\n       [0.94615571, 0.84280628, 1.05011189, ..., 1.0255364 , 0.96478417,\n        0.9140493 ],\n       ...,\n       [0.87146472, 1.04641364, 0.86900166, ..., 0.91303204, 0.95041789,\n        0.96797332],\n       [0.94906021, 0.99194229, 0.84058257, ..., 0.99087914, 0.96639345,\n        0.99059172],\n       [0.91025793, 0.8993632 , 1.03222263, ..., 0.9717563 , 0.95967178,\n        0.92491709]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (4)created_at :2024-04-13T05:34:49.761913+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 200kB\nDimensions:          (chain: 8, draw: 500)\nCoordinates:\n  * chain            (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 32kB 0.8137 1.0 1.0 ... 0.9094 0.9834\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB 142.0 142.5 ... 143.0 141.5\n    lp               (chain, draw) float64 32kB -141.8 -140.8 ... -140.3 -140.3\n    n_steps          (chain, draw) int64 32kB 3 3 7 7 7 3 3 7 ... 7 3 7 5 7 3 7\n    step_size        (chain, draw) float64 32kB 0.7326 0.7326 ... 0.7643 0.7643\n    tree_depth       (chain, draw) int64 32kB 2 2 3 3 3 2 2 3 ... 3 2 3 3 3 2 3\nAttributes:\n    created_at:                  2024-04-13T05:34:49.763427+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 500Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (7)acceptance_rate(chain, draw)float640.8137 1.0 1.0 ... 0.9094 0.9834array([[0.81368466, 1.        , 1.        , ..., 0.95726715, 0.95332204,\n        1.        ],\n       [0.98623155, 0.76947694, 1.        , ..., 0.6501189 , 0.6980205 ,\n        1.        ],\n       [0.98539058, 0.82802334, 0.96559601, ..., 0.72850635, 1.        ,\n        0.86563511],\n       ...,\n       [0.79238494, 0.97989654, 0.94005541, ..., 0.98283263, 0.99321313,\n        0.92314755],\n       [0.95823733, 0.94198648, 0.91853339, ..., 0.68699656, 0.972578  ,\n        0.74390253],\n       [0.99181102, 0.97429544, 0.78790853, ..., 1.        , 0.90941548,\n        0.98341956]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64142.0 142.5 141.8 ... 143.0 141.5array([[141.967537  , 142.54106626, 141.78398912, ..., 142.07442022,\n        143.72755872, 142.21633731],\n       [142.4254147 , 142.09857076, 141.94644772, ..., 142.39410249,\n        146.51261269, 143.65723574],\n       [141.4414682 , 142.96701481, 142.39395521, ..., 144.02150538,\n        142.66721068, 140.85225446],\n       ...,\n       [142.45649528, 142.75638401, 142.59874467, ..., 141.35824722,\n        140.94450824, 141.2808887 ],\n       [140.3251331 , 141.16320788, 140.88902952, ..., 144.52035375,\n        144.09031991, 143.92351871],\n       [142.65283365, 141.01504212, 142.60582761, ..., 143.46419056,\n        142.97607812, 141.46662296]])lp(chain, draw)float64-141.8 -140.8 ... -140.3 -140.3array([[-141.78590569, -140.75355135, -140.61320047, ..., -141.63502666,\n        -142.12600187, -141.57227603],\n       [-139.88014501, -141.79255751, -140.226333  , ..., -141.301519  ,\n        -143.3329595 , -140.20584575],\n       [-140.39038429, -141.56925705, -141.27509741, ..., -143.36048355,\n        -139.58615368, -139.84922801],\n       ...,\n       [-140.99893216, -140.82540718, -140.38825538, ..., -140.12098164,\n        -140.10850196, -139.71074945],\n       [-140.09932106, -139.69086444, -140.49414807, ..., -143.90263595,\n        -140.69641315, -140.7183776 ],\n       [-140.46042516, -139.8366111 , -142.15416918, ..., -140.96117584,\n        -140.27772734, -140.27024162]])n_steps(chain, draw)int643 3 7 7 7 3 3 7 ... 7 7 3 7 5 7 3 7array([[ 3,  3,  7, ...,  7,  7,  7],\n       [ 3,  3,  3, ...,  3,  7,  3],\n       [11,  3,  3, ...,  3,  3,  3],\n       ...,\n       [ 7,  7,  7, ...,  7,  7,  3],\n       [ 3,  3,  3, ...,  7,  7,  3],\n       [ 7,  3,  3, ...,  7,  3,  7]])step_size(chain, draw)float640.7326 0.7326 ... 0.7643 0.7643array([[0.73264667, 0.73264667, 0.73264667, ..., 0.73264667, 0.73264667,\n        0.73264667],\n       [0.84139296, 0.84139296, 0.84139296, ..., 0.84139296, 0.84139296,\n        0.84139296],\n       [0.90832794, 0.90832794, 0.90832794, ..., 0.90832794, 0.90832794,\n        0.90832794],\n       ...,\n       [0.75868138, 0.75868138, 0.75868138, ..., 0.75868138, 0.75868138,\n        0.75868138],\n       [0.83356209, 0.83356209, 0.83356209, ..., 0.83356209, 0.83356209,\n        0.83356209],\n       [0.76429536, 0.76429536, 0.76429536, ..., 0.76429536, 0.76429536,\n        0.76429536]])tree_depth(chain, draw)int642 2 3 3 3 2 2 3 ... 3 3 2 3 3 3 2 3array([[2, 2, 3, ..., 3, 3, 3],\n       [2, 2, 2, ..., 2, 3, 2],\n       [4, 2, 2, ..., 2, 2, 2],\n       ...,\n       [3, 3, 3, ..., 3, 3, 2],\n       [2, 2, 2, ..., 3, 3, 2],\n       [3, 2, 2, ..., 3, 2, 3]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (4)created_at :2024-04-13T05:34:49.763427+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\nDifferent backends have different naming conventions for the parameters specific to that MCMC method. Thus, to specify backend-specific parameters, pass your own kwargs to the fit method.\nThe following can be performend to identify the kwargs specific to each method.\n\nbmb.inference_methods.get_kwargs(\"blackjax_nuts\")\n\n{&lt;function blackjax.adaptation.window_adaptation.window_adaptation(algorithm: Union[blackjax.mcmc.hmc.hmc, blackjax.mcmc.nuts.nuts], logdensity_fn: Callable, is_mass_matrix_diagonal: bool = True, initial_step_size: float = 1.0, target_acceptance_rate: float = 0.8, progress_bar: bool = False, **extra_parameters) -&gt; blackjax.base.AdaptationAlgorithm&gt;: {'logdensity_fn': &lt;function bayeux._src.shared.constrain.&lt;locals&gt;.wrap_log_density.&lt;locals&gt;.wrapped(args)&gt;,\n  'is_mass_matrix_diagonal': True,\n  'initial_step_size': 1.0,\n  'target_acceptance_rate': 0.8,\n  'progress_bar': False,\n  'algorithm': blackjax.mcmc.nuts.nuts},\n 'adapt.run': {'num_steps': 500},\n blackjax.mcmc.nuts.nuts: {'max_num_doublings': 10,\n  'divergence_threshold': 1000,\n  'integrator': &lt;function blackjax.mcmc.integrators.generate_euclidean_integrator.&lt;locals&gt;.euclidean_integrator(logdensity_fn: Callable, kinetic_energy_fn: blackjax.mcmc.metrics.KineticEnergy) -&gt; Callable[[blackjax.mcmc.integrators.IntegratorState, float], blackjax.mcmc.integrators.IntegratorState]&gt;,\n  'logdensity_fn': &lt;function bayeux._src.shared.constrain.&lt;locals&gt;.wrap_log_density.&lt;locals&gt;.wrapped(args)&gt;,\n  'step_size': 0.5},\n 'extra_parameters': {'chain_method': 'vectorized',\n  'num_chains': 8,\n  'num_draws': 500,\n  'num_adapt_draws': 500,\n  'return_pytree': False}}\n\n\nNow, we can identify the kwargs we would like to change and pass to the fit method.\n\nkwargs = {\n        \"adapt.run\": {\"num_steps\": 500},\n        \"num_chains\": 4,\n        \"num_draws\": 250,\n        \"num_adapt_draws\": 250\n}\n\nblackjax_nuts_idata = model.fit(inference_method=\"blackjax_nuts\", **kwargs)\nblackjax_nuts_idata\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 26kB\nDimensions:    (chain: 4, draw: 250)\nCoordinates:\n  * chain      (chain) int64 32B 0 1 2 3\n  * draw       (draw) int64 2kB 0 1 2 3 4 5 6 7 ... 243 244 245 246 247 248 249\nData variables:\n    Intercept  (chain, draw) float64 8kB 0.112 -0.08016 ... -0.04784 -0.04427\n    x          (chain, draw) float64 8kB 0.4311 0.3077 0.2568 ... 0.557 0.4814\n    y_sigma    (chain, draw) float64 8kB 0.9461 0.9216 0.9021 ... 0.9574 0.9414\nAttributes:\n    created_at:                  2024-04-13T05:36:20.439151+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 4draw: 250Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 245 246 247 248 249array([  0,   1,   2, ..., 247, 248, 249])Data variables: (3)Intercept(chain, draw)float640.112 -0.08016 ... -0.04427array([[ 1.12014970e-01, -8.01554356e-02, -5.13623666e-02,\n         3.14326884e-02, -1.15157203e-02,  8.29955919e-02,\n         1.93598964e-02,  3.52348779e-02,  1.16428197e-01,\n         8.72568457e-02,  8.72568457e-02, -2.82423636e-02,\n        -1.84281659e-01, -7.19613600e-02,  1.91168564e-01,\n        -1.23364451e-01, -1.19337859e-01, -3.08974539e-02,\n        -7.18298964e-02,  1.19861654e-01, -1.00735526e-01,\n         6.32700448e-02, -5.72727355e-03,  1.17338655e-01,\n        -3.83727222e-02,  1.84746578e-01,  7.26989694e-02,\n         1.94564654e-01,  1.44827239e-01, -8.50718497e-02,\n         1.87666705e-01, -1.60104555e-01, -4.99203302e-02,\n         3.19979141e-03, -3.94543945e-03,  8.66672888e-04,\n         4.33390733e-02,  5.22653528e-02, -4.77497368e-02,\n        -1.88745071e-02,  5.03627424e-02,  8.24434767e-02,\n        -3.79889140e-03,  7.70856139e-03, -4.77259521e-02,\n        -1.90736318e-02,  6.76733158e-02,  5.14461069e-02,\n        -4.56113715e-02,  1.60543248e-01,  7.32836299e-02,\n         2.28842579e-03,  3.05194139e-02,  4.27895103e-02,\n        -2.51634507e-02, -4.60161935e-02,  2.44964388e-01,\n         2.76318153e-01,  4.33818171e-02,  1.46208904e-01,\n...\n        -5.68838115e-02, -6.14275201e-02, -5.96425618e-02,\n         6.11356758e-02,  6.42661723e-03,  5.41912583e-02,\n        -1.76976244e-01, -4.62930404e-02,  9.61963932e-02,\n        -1.40433636e-01,  2.05056910e-01,  1.82385197e-01,\n         1.21005125e-01, -9.65523825e-02,  9.11450646e-02,\n         1.49525640e-02, -8.32289763e-02,  3.24479331e-02,\n         1.09007071e-02, -6.92830705e-02,  6.64926592e-02,\n         3.23060974e-02, -1.73437807e-01, -1.25619389e-03,\n         8.89183729e-02,  1.02309051e-01,  4.12736086e-02,\n         1.03893380e-01,  6.89267255e-02,  1.37649597e-01,\n        -7.63849028e-02,  7.69987215e-02, -1.14605433e-01,\n        -1.59066163e-01,  2.02049201e-01,  1.67222994e-01,\n        -5.02468032e-02, -1.17601875e-01, -1.67595598e-03,\n        -1.97669449e-01,  4.36079372e-02,  8.41929183e-02,\n         1.31836071e-01,  1.65427331e-01,  1.26585460e-01,\n        -7.27516393e-02,  5.41849189e-02,  2.21844869e-02,\n         5.94315594e-02,  5.94315594e-02,  7.45566691e-02,\n        -9.33357688e-03, -4.93686976e-02, -3.43353187e-02,\n         6.89221401e-02, -3.19652375e-02, -4.78438315e-02,\n        -4.42738699e-02]])x(chain, draw)float640.4311 0.3077 ... 0.557 0.4814array([[0.43113703, 0.30774729, 0.25682166, 0.31641395, 0.51546729,\n        0.31100976, 0.44043531, 0.38164497, 0.42080014, 0.30069041,\n        0.30069041, 0.53305267, 0.11871418, 0.22919114, 0.22043383,\n        0.32368544, 0.28827739, 0.44216387, 0.38292596, 0.38328387,\n        0.31277052, 0.28380182, 0.39125062, 0.5436668 , 0.19914823,\n        0.23381157, 0.3952613 , 0.48281672, 0.27598205, 0.46597795,\n        0.48635971, 0.21363092, 0.39350997, 0.42601567, 0.3035345 ,\n        0.26553072, 0.44019149, 0.34397815, 0.23609522, 0.53683168,\n        0.45841485, 0.23891478, 0.54442998, 0.16697332, 0.19146859,\n        0.22799538, 0.39366724, 0.37134365, 0.34501806, 0.37506017,\n        0.28311981, 0.16254121, 0.61289656, 0.13063232, 0.03017502,\n        0.18434623, 0.36065819, 0.52235008, 0.24458848, 0.14313226,\n        0.22279879, 0.44892021, 0.3952106 , 0.34290512, 0.42439318,\n        0.23102895, 0.19110882, 0.25093658, 0.37681057, 0.36135287,\n        0.30745033, 0.27562781, 0.31724922, 0.45716849, 0.47116505,\n        0.43884602, 0.43553571, 0.29161261, 0.41998198, 0.40796597,\n        0.30405689, 0.31259796, 0.20570747, 0.39392466, 0.31348596,\n        0.4214938 , 0.52463068, 0.16792862, 0.28029374, 0.16153929,\n        0.16724633, 0.08144633, 0.19192458, 0.34938819, 0.13305379,\n        0.13881198, 0.37849938, 0.37084368, 0.27404992, 0.46209003,\n...\n        0.36307521, 0.28227501, 0.38551525, 0.23261809, 0.36514994,\n        0.35783934, 0.3823261 , 0.30670976, 0.32886498, 0.37068029,\n        0.34013729, 0.52474148, 0.639228  , 0.09371894, 0.42023135,\n        0.36733482, 0.40599032, 0.24382963, 0.40221825, 0.3047073 ,\n        0.24407962, 0.30213837, 0.44363912, 0.57883031, 0.48781764,\n        0.48525391, 0.29198732, 0.37745175, 0.31777746, 0.28262044,\n        0.18702892, 0.51867304, 0.52340339, 0.24125419, 0.23332597,\n        0.46851727, 0.46079104, 0.32301517, 0.35635714, 0.33111389,\n        0.37437903, 0.28657985, 0.43734974, 0.35478284, 0.30887643,\n        0.49867288, 0.2525673 , 0.33079942, 0.02800324, 0.31465776,\n        0.4585882 , 0.28368126, 0.4896697 , 0.44762422, 0.41453835,\n        0.246885  , 0.37482138, 0.40059614, 0.27591068, 0.21900013,\n        0.47128275, 0.21132567, 0.39900367, 0.2329504 , 0.39579287,\n        0.37344961, 0.34516518, 0.32227915, 0.35271413, 0.37687565,\n        0.31151605, 0.37301695, 0.26012957, 0.30024098, 0.2745939 ,\n        0.25698144, 0.37064686, 0.43608796, 0.29833848, 0.4057974 ,\n        0.37998817, 0.3505483 , 0.3385325 , 0.29122156, 0.46273061,\n        0.26565498, 0.27156778, 0.37355743, 0.30409736, 0.34103447,\n        0.33149781, 0.33149781, 0.43853323, 0.27834059, 0.36111445,\n        0.40162141, 0.42356887, 0.55111442, 0.55695307, 0.48135813]])y_sigma(chain, draw)float640.9461 0.9216 ... 0.9574 0.9414array([[0.94608021, 0.92160426, 0.90211804, 0.8784906 , 1.00443151,\n        0.96727798, 0.87524178, 0.96817024, 0.86457884, 1.0262922 ,\n        1.0262922 , 0.84391933, 0.98129848, 1.00988048, 0.99554283,\n        0.88587989, 1.00586764, 0.92997466, 0.94168997, 0.99374899,\n        0.92810392, 0.96878376, 0.90682728, 0.94883541, 1.01957489,\n        1.02278733, 0.90160336, 0.94497765, 0.87888132, 0.92205875,\n        0.9138956 , 0.96519328, 1.06316311, 0.84402636, 0.83729644,\n        0.89811997, 0.97144791, 0.98208145, 0.91289233, 0.96673035,\n        0.95542624, 0.91245841, 0.96527727, 0.92783747, 1.03786087,\n        0.94764661, 1.00547045, 0.85588467, 0.98223118, 0.8674327 ,\n        0.94037555, 0.91725845, 0.99391199, 0.92434293, 0.9638643 ,\n        1.08815478, 1.01399545, 1.02349856, 0.92934388, 0.96598116,\n        0.96311436, 0.93945143, 0.89124759, 0.98455184, 0.89591612,\n        1.006701  , 0.95597051, 1.00027136, 0.91409196, 0.97378494,\n        0.87137146, 0.87160277, 1.04749666, 0.8805835 , 0.8819731 ,\n        0.88645983, 1.00263402, 0.88708112, 0.99995189, 1.01743406,\n        0.87473936, 0.9076109 , 1.02202715, 0.88250374, 1.06665137,\n        0.84538309, 0.84109731, 1.0524254 , 0.97522117, 0.94564838,\n        1.05965236, 0.97217503, 0.96459187, 0.9413301 , 1.00422163,\n        1.00733854, 0.95474848, 0.94441562, 0.89236671, 0.96775448,\n...\n        0.9959878 , 0.93569281, 0.96401675, 0.88786078, 1.14540889,\n        0.8224594 , 0.84935646, 1.03698789, 1.00625543, 0.88735547,\n        0.99331278, 1.00797432, 0.94295773, 0.98513086, 1.0195952 ,\n        0.88995881, 0.84278984, 1.02888997, 0.96128787, 0.91245996,\n        0.97871983, 0.89146682, 0.98259937, 0.95369473, 1.02821356,\n        0.83242344, 1.06338194, 0.82728423, 1.06433136, 0.85249613,\n        0.92553966, 0.96450458, 1.05280513, 0.90353168, 0.84823849,\n        1.03949674, 0.92214448, 0.9231072 , 0.87897527, 0.95304901,\n        0.91455056, 0.97220005, 0.91253068, 0.92932491, 0.85741327,\n        1.05336522, 1.05774423, 1.20149457, 0.99443219, 0.89727566,\n        0.97462237, 0.9137672 , 0.99391023, 0.98467151, 0.83221799,\n        1.06702143, 0.85499338, 0.94884501, 0.94337727, 0.96101538,\n        0.87323245, 1.02556183, 0.95388553, 0.97263382, 0.98591673,\n        0.96502309, 0.85746496, 0.84968585, 0.99422795, 0.89441428,\n        1.04297339, 1.05277335, 0.85709214, 0.87885518, 1.03047245,\n        1.05704007, 0.91158198, 0.91662192, 0.90469643, 0.96868723,\n        0.91674164, 0.93328151, 0.91403954, 1.22344839, 0.98301442,\n        0.97414525, 0.97886513, 0.91856841, 0.95869794, 0.92022874,\n        0.92707182, 0.92707182, 0.95625616, 0.93941143, 0.93244475,\n        0.93337728, 0.97659533, 0.97438746, 0.95742502, 0.94139699]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n      dtype='int64', name='draw', length=250))Attributes: (4)created_at :2024-04-13T05:36:20.439151+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 51kB\nDimensions:          (chain: 4, draw: 250)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 2kB 0 1 2 3 4 5 6 ... 244 245 246 247 248 249\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.8489 0.9674 ... 0.983 1.0\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 141.2 140.7 140.5 ... 141.9 141.4\n    lp               (chain, draw) float64 8kB -139.9 -140.0 ... -141.6 -140.3\n    n_steps          (chain, draw) int64 8kB 3 7 7 3 3 3 7 3 ... 3 3 3 3 3 3 3 3\n    step_size        (chain, draw) float64 8kB 0.8923 0.8923 ... 0.9726 0.9726\n    tree_depth       (chain, draw) int64 8kB 2 3 3 2 2 2 3 2 ... 2 2 2 2 2 2 2 2\nAttributes:\n    created_at:                  2024-04-13T05:36:20.441267+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 4draw: 250Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 245 246 247 248 249array([  0,   1,   2, ..., 247, 248, 249])Data variables: (7)acceptance_rate(chain, draw)float640.8489 0.9674 0.9733 ... 0.983 1.0array([[0.84888924, 0.9673831 , 0.9733441 , 0.89329671, 0.86130558,\n        0.70132658, 0.96381014, 1.        , 0.92972425, 0.99393407,\n        0.42991297, 0.91174018, 0.87340076, 1.        , 0.9589841 ,\n        0.87037497, 1.        , 1.        , 0.99496101, 0.88726023,\n        0.9885737 , 0.90697071, 0.99084364, 0.80724362, 0.96962189,\n        0.9833609 , 0.97786349, 0.86134776, 0.90956489, 0.90854708,\n        0.98462356, 0.89669834, 0.90547798, 0.98699424, 0.97484471,\n        1.        , 0.88021684, 0.97615242, 0.96073465, 0.91237892,\n        0.9845141 , 0.94372409, 0.92777163, 0.97342742, 0.95057722,\n        1.        , 0.9612599 , 0.98843436, 1.        , 0.8602412 ,\n        0.99473406, 0.82449416, 0.87100299, 0.89980582, 0.91056175,\n        1.        , 0.9853551 , 0.6339886 , 0.79539193, 0.79552437,\n        0.98590884, 0.7882424 , 0.95636624, 1.        , 0.91644093,\n        0.61071389, 0.9079233 , 1.        , 0.94421384, 0.89397927,\n        0.97441365, 0.74609534, 0.87623779, 0.65668758, 1.        ,\n        0.99754243, 1.        , 1.        , 0.91660863, 0.99472955,\n        0.7589626 , 1.        , 0.79027932, 0.96149457, 0.86777121,\n        0.98246178, 0.78602939, 0.98883522, 0.99371046, 0.87781383,\n        0.90548202, 0.91736797, 0.93188359, 1.        , 0.56722994,\n        0.92168357, 1.        , 0.99974158, 0.85855488, 0.9130902 ,\n...\n        0.933183  , 0.9759323 , 0.80154439, 0.58682022, 0.89919977,\n        0.96192085, 1.        , 1.        , 1.        , 0.92846989,\n        0.73777125, 0.78583409, 0.84084899, 0.99937813, 0.68385273,\n        0.90734862, 0.87463518, 0.86085163, 0.96549202, 1.        ,\n        0.73037943, 0.94028211, 1.        , 0.82836097, 0.98809743,\n        0.81764963, 1.        , 1.        , 0.98798519, 0.98349015,\n        0.67753562, 0.95280466, 0.90283301, 0.93024677, 0.93151669,\n        1.        , 0.96818253, 0.99781367, 0.91713181, 0.96014857,\n        1.        , 0.75023019, 0.96045113, 0.94362692, 0.7532761 ,\n        0.83880141, 0.87182051, 0.74171229, 0.94450692, 1.        ,\n        0.94232357, 0.86828235, 0.92197311, 1.        , 0.68644282,\n        0.97359373, 0.90638899, 1.        , 0.76594222, 0.91762793,\n        0.95915589, 0.87177042, 1.        , 0.82258746, 0.81200426,\n        1.        , 0.93555039, 0.8848828 , 1.        , 0.87053966,\n        0.85639141, 0.97637026, 0.98528981, 1.        , 0.85350081,\n        0.95414825, 1.        , 1.        , 0.98446748, 0.80153679,\n        1.        , 0.62828104, 1.        , 0.36846189, 0.86386322,\n        0.64099386, 1.        , 0.84170202, 0.95361984, 0.9205592 ,\n        0.98298569, 0.66323733, 0.85359733, 0.98170805, 0.99093396,\n        0.97552769, 0.95590249, 0.84734008, 0.98296291, 1.        ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False]])energy(chain, draw)float64141.2 140.7 140.5 ... 141.9 141.4array([[141.20161744, 140.70598666, 140.49148926, 141.20239103,\n        141.19010811, 144.18660929, 140.33281704, 139.96308322,\n        140.57893835, 141.38941515, 148.64059094, 142.93289825,\n        146.04299163, 144.14605027, 142.67642036, 144.14576568,\n        141.7660787 , 141.06947956, 140.19356762, 141.08246659,\n        141.00591894, 141.35859185, 139.93979496, 141.51824106,\n        142.56997862, 142.56991682, 142.58758086, 141.72338033,\n        142.8969296 , 142.42932381, 141.68924271, 143.93789916,\n        143.12463584, 142.28934616, 141.66880823, 140.75972524,\n        141.01378042, 140.13124997, 140.51835247, 141.61498126,\n        141.31832015, 140.66043205, 142.12540545, 141.64700326,\n        142.85807033, 141.70816164, 140.81111539, 140.31405788,\n        140.29502047, 142.17134901, 141.34942234, 141.96753663,\n        144.45306366, 144.30037464, 144.92560947, 145.88202322,\n        145.57638211, 149.53691678, 146.82447002, 143.92894787,\n        142.945291  , 143.62675737, 140.2703418 , 140.10974452,\n        140.59959866, 143.312126  , 143.4798215 , 142.32917667,\n        141.59167508, 140.91590845, 141.55742637, 142.42319463,\n        142.17191796, 145.8462414 , 143.50482308, 143.38838779,\n        140.93972277, 140.42443743, 141.12402307, 141.13791   ,\n...\n        141.93825369, 144.56393892, 142.73608027, 144.07625796,\n        142.0531902 , 141.41700837, 141.51465137, 142.62280631,\n        143.79817982, 142.76116047, 143.85988869, 142.11378708,\n        141.76697822, 141.60731201, 141.3540055 , 139.81189634,\n        140.0125599 , 140.24700481, 139.84972186, 141.52789778,\n        141.49476174, 140.70968672, 141.66856485, 142.71296198,\n        143.85619075, 145.52274885, 149.21963527, 147.57321087,\n        140.39448216, 141.853325  , 143.43444642, 141.78434647,\n        145.09952198, 142.36990061, 143.10356634, 140.37811064,\n        141.38857059, 141.31196017, 140.94430523, 142.22846388,\n        141.29718806, 141.2077227 , 142.47077178, 141.54519327,\n        140.3278173 , 140.95240769, 140.60694954, 140.58587491,\n        140.9740406 , 141.73032551, 142.70897439, 141.88098807,\n        141.65131353, 142.52300291, 143.27844069, 141.46619993,\n        140.87239229, 141.57738767, 140.5790518 , 143.21027453,\n        141.86795416, 146.14898606, 147.39883864, 145.13195593,\n        141.11515884, 141.95224649, 140.49781241, 140.32029504,\n        139.45421819, 142.18814629, 140.68012531, 139.92677613,\n        139.90305713, 139.92528181, 140.02423793, 141.83956437,\n        141.90308307, 141.4326794 ]])lp(chain, draw)float64-139.9 -140.0 ... -141.6 -140.3array([[-139.87893778, -139.9645816 , -140.15023671, -139.72915805,\n        -140.95230947, -139.75107499, -140.01457839, -139.4408409 ,\n        -140.52869623, -140.55137532, -140.55137532, -142.39342002,\n        -143.70774595, -140.91220502, -142.25708851, -140.87749392,\n        -140.94220804, -139.82075105, -139.84129544, -140.14606113,\n        -140.18660279, -139.81366331, -139.4446184 , -141.12989093,\n        -141.18051022, -142.24699337, -139.55294872, -141.34902122,\n        -141.1207297 , -140.68010125, -141.41084446, -141.82800678,\n        -141.20536354, -140.59228081, -140.69416166, -139.86811985,\n        -139.72750674, -139.60666827, -140.26256251, -141.00322146,\n        -139.77866637, -140.37414036, -141.03275542, -141.08023336,\n        -141.6000328 , -140.14892777, -139.96123244, -140.06522244,\n        -139.78981573, -141.01735455, -139.75775472, -141.22535186,\n        -142.38556034, -141.97659086, -144.27384526, -142.67574989,\n        -142.35722033, -143.75557052, -139.98097631, -142.70952977,\n        -140.7217984 , -139.6410683 , -139.84543799, -139.64193129,\n        -139.6247126 , -140.62462715, -142.18994991, -140.89554653,\n        -139.30901481, -140.66999757, -140.09430283, -140.18511655,\n        -141.47845726, -143.08700779, -142.63691111, -140.3748972 ,\n        -140.30603842, -139.8354723 , -140.91681837, -140.8016408 ,\n...\n        -141.61328651, -142.16610216, -140.86932964, -141.66349492,\n        -141.24899794, -140.92756748, -141.35783924, -140.46980857,\n        -141.82709759, -141.4470513 , -141.67479038, -140.53378286,\n        -141.14396647, -140.86380423, -139.8673658 , -139.34853841,\n        -139.70693196, -139.74296053, -139.57680571, -140.83672532,\n        -140.07834751, -139.67448254, -140.52919726, -141.42318554,\n        -141.21657892, -144.92174257, -145.62381945, -139.76482799,\n        -140.08610874, -141.02022716, -141.7757176 , -141.0336296 ,\n        -141.39137894, -141.95233948, -140.33931036, -139.38387579,\n        -140.13741269, -140.30673995, -140.39845601, -141.29817581,\n        -139.47208887, -140.20293725, -141.72161449, -139.43675991,\n        -140.300563  , -140.72081646, -139.7054349 , -139.82631079,\n        -140.69282006, -141.25803608, -141.07708316, -140.04546292,\n        -141.29624749, -142.45655409, -141.21055678, -140.6477156 ,\n        -139.8087491 , -140.65559625, -139.34928981, -142.04465792,\n        -139.35271669, -145.90075   , -140.47978836, -141.11111359,\n        -140.53840869, -139.8545253 , -139.57336607, -139.29539907,\n        -139.39810176, -139.39810176, -139.70660711, -139.62054515,\n        -139.56840864, -139.5697715 , -139.73762165, -141.38313388,\n        -141.58458337, -140.34263295]])n_steps(chain, draw)int643 7 7 3 3 3 7 3 ... 3 3 3 3 3 3 3 3array([[ 3,  7,  7,  3,  3,  3,  7,  3,  3, 11,  3,  3,  7,  3,  7,  7,\n         3,  3,  7,  5,  7,  7,  3,  3,  7,  7,  7,  7,  3,  7,  7,  7,\n         7,  3,  7,  3,  3,  3,  3,  7,  7,  3,  3,  7,  7,  3,  7,  3,\n         3,  7,  3,  3,  7,  3,  3,  3,  7,  3,  3,  3,  7,  3,  3,  3,\n         3,  7,  7,  3,  3,  7,  7,  3,  3,  3,  3,  7,  3,  7,  3,  3,\n         3,  1,  7,  7,  3,  3,  7,  3,  3,  7,  3,  3,  3,  3,  3,  3,\n         3,  3,  3,  7,  7,  7,  3,  3,  3,  7,  7,  7,  7,  3,  5,  3,\n         1,  7,  7,  3,  3,  7,  3,  3, 15,  3,  7,  3,  3,  3,  3,  3,\n         3,  3,  7,  7,  7,  3,  3,  7,  7,  3,  3,  3,  7,  7,  3,  3,\n         3,  3,  3,  3,  3,  7,  7,  7,  3,  3,  7,  3,  5,  3,  3,  3,\n         3,  3,  3,  1,  3,  3,  3,  7,  1,  3,  7,  7,  3,  3,  3,  3,\n         7,  3,  1,  3,  1,  3,  3,  7,  3,  3, 15,  3,  3,  7,  3,  3,\n         3,  3,  7,  3,  3,  3,  3, 15,  3,  7,  1,  3,  1,  3,  3,  7,\n         3,  3,  7,  3,  3,  7,  7,  7,  7,  3,  3,  7,  7,  7,  3,  7,\n         3,  7,  3,  3,  7,  3,  3,  3,  7,  3,  3,  7,  3,  3,  7,  3,\n         7,  3,  3,  7,  7,  3,  3,  3,  7,  3],\n       [ 3,  7,  3,  3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n         3,  3,  7,  3,  3, 11,  3,  3,  3,  3,  3,  7,  3,  3,  3,  1,\n         3,  7,  3,  3, 11,  7,  3,  3,  7,  3,  3, 19,  3,  3,  3,  3,\n         3,  3, 11,  3,  3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3,  3,\n...\n         3,  3,  7,  3,  3,  7,  7,  3,  3,  3,  7,  3,  7,  3,  7,  3,\n         3,  3,  3,  3,  3,  7,  7,  3,  3, 19,  3,  3,  3,  3,  7,  3,\n         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  7,  3,  3,  3,\n         3,  3,  3,  3, 11,  3,  3,  3, 11,  3],\n       [ 3,  3,  3,  3, 11,  7,  3,  3,  3, 15,  3,  3,  3, 11,  3,  3,\n         3,  7,  3,  3,  3,  3,  3,  3,  3, 11,  7,  3,  3,  3,  3,  3,\n         3,  3,  3,  3,  3,  7,  3,  3,  3,  3,  7,  3,  1,  3,  3,  3,\n        11,  3,  3,  3,  3,  3,  3,  3,  3,  1,  3,  3,  3,  3,  3,  7,\n         7,  3,  7,  7,  3,  3,  3, 11,  3,  3,  3,  3,  3,  3,  7,  7,\n         3,  7,  3,  3,  3,  3,  7,  3,  7,  3,  7,  3,  3,  3, 11,  3,\n         3,  3, 15,  3,  3,  7,  3,  3,  3,  3,  3,  3,  1,  7,  3,  3,\n         3,  3,  1,  3,  3,  3,  7,  3,  3,  3,  3,  3,  7,  7,  3, 15,\n         3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3, 11,  3,  3,  7,  3,\n         3,  7,  3,  3,  1,  3,  3,  3,  3,  7,  7,  3,  1,  3,  1,  3,\n         3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3,  7,  7,  3,  3,  3,\n         3,  3,  3,  3,  3,  3, 11,  3,  3,  3,  7,  7,  3,  3,  3,  3,\n         3,  3, 11,  3,  3,  3,  3,  3,  3, 11,  3,  1,  3,  7,  3,  3,\n         3,  7,  3,  3,  3,  7,  3,  3,  3,  1,  3,  3,  3,  3,  3,  3,\n         7,  3,  7,  3,  7,  3,  3,  3,  3,  3,  3,  3,  1, 11,  3,  3,\n         3,  3,  3,  3,  3,  3,  3,  3,  3,  3]])step_size(chain, draw)float640.8923 0.8923 ... 0.9726 0.9726array([[0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n...\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ]])tree_depth(chain, draw)int642 3 3 2 2 2 3 2 ... 2 2 2 2 2 2 2 2array([[2, 3, 3, 2, 2, 2, 3, 2, 2, 4, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3,\n        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3,\n        3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3,\n        3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 1, 3, 3, 2, 2, 3, 2,\n        2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2,\n        3, 2, 1, 3, 3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n        3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2,\n        3, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2, 2, 2,\n        3, 2, 1, 2, 1, 2, 2, 3, 2, 2, 4, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2,\n        2, 4, 2, 3, 1, 2, 1, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3,\n        3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2,\n        2, 3, 3, 2, 2, 2, 3, 2],\n       [2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 4,\n        2, 2, 2, 2, 2, 3, 2, 2, 2, 1, 2, 3, 2, 2, 4, 3, 2, 2, 3, 2, 2, 5,\n        2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3,\n        2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3,\n        2, 3, 4, 2, 2, 1, 2, 4, 2, 2, 2, 2, 2, 2, 3, 2, 4, 2, 2, 3, 2, 2,\n        3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 3,\n        2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3,\n...\n        2, 2, 4, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2,\n        2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3,\n        2, 3, 2, 3, 2, 1, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2,\n        2, 3, 2, 3, 2, 2, 2, 4, 2, 1, 2, 2, 2, 3, 4, 2, 2, 4, 2, 2, 2, 2,\n        2, 2, 1, 3, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 3, 2, 2, 3,\n        3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 5, 2, 2,\n        2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,\n        2, 2, 4, 2, 2, 2, 4, 2],\n       [2, 2, 2, 2, 4, 3, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 3, 2, 2, 2, 2,\n        2, 2, 2, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2,\n        1, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 3, 2,\n        3, 3, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2,\n        3, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 3,\n        2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 4, 2, 2, 2, 3,\n        2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 2, 2, 2, 3,\n        3, 2, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2,\n        2, 2, 2, 4, 2, 1, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 1, 2, 2,\n        2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 4, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n      dtype='int64', name='draw', length=250))Attributes: (4)created_at :2024-04-13T05:36:20.441267+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\ntfp_nuts_idata = model.fit(inference_method=\"tfp_nuts\")\ntfp_nuts_idata\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 200kB\nDimensions:    (chain: 8, draw: 1000)\nCoordinates:\n  * chain      (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw       (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    Intercept  (chain, draw) float64 64kB -0.1597 0.2011 ... 0.1525 -0.171\n    x          (chain, draw) float64 64kB 0.2515 0.4686 0.4884 ... 0.5085 0.4896\n    y_sigma    (chain, draw) float64 64kB 0.9735 0.8969 0.8002 ... 0.9422 1.045\nAttributes:\n    created_at:                  2024-04-13T05:36:30.303342+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (3)Intercept(chain, draw)float64-0.1597 0.2011 ... 0.1525 -0.171array([[-0.15967084,  0.20113676,  0.01971475, ...,  0.00794969,\n         0.0279562 ,  0.03472592],\n       [-0.06799971,  0.14593555, -0.00604452, ...,  0.00924657,\n        -0.10235794, -0.10236953],\n       [ 0.043034  ,  0.08600223,  0.16562574, ...,  0.05851938,\n         0.00720315,  0.08258778],\n       ...,\n       [ 0.04807806,  0.11227424, -0.3172604 , ...,  0.02980962,\n        -0.13681545,  0.19177451],\n       [ 0.04374417, -0.0054294 ,  0.09305579, ...,  0.0232273 ,\n        -0.04073809,  0.025925  ],\n       [-0.07370367, -0.00152223,  0.06769584, ..., -0.09818811,\n         0.15246738, -0.17104419]])x(chain, draw)float640.2515 0.4686 ... 0.5085 0.4896array([[0.25153111, 0.4685625 , 0.48837809, ..., 0.28573626, 0.407775  ,\n        0.38347135],\n       [0.28165967, 0.36310827, 0.41225084, ..., 0.24255857, 0.45039439,\n        0.4954714 ],\n       [0.5386156 , 0.6228231 , 0.25313292, ..., 0.44280376, 0.4488854 ,\n        0.25456354],\n       ...,\n       [0.45168195, 0.46344655, 0.17750331, ..., 0.30371223, 0.29536054,\n        0.40431303],\n       [0.41455145, 0.43166272, 0.35213661, ..., 0.36384472, 0.3917272 ,\n        0.34092006],\n       [0.20620881, 0.51263399, 0.44056489, ..., 0.25237815, 0.50845624,\n        0.48960883]])y_sigma(chain, draw)float640.9735 0.8969 ... 0.9422 1.045array([[0.97352428, 0.89691108, 0.80020873, ..., 1.03087931, 0.84944049,\n        0.84158909],\n       [0.96226504, 0.92778234, 0.77909925, ..., 0.91397532, 1.00185137,\n        0.9513834 ],\n       [1.0042728 , 0.97580931, 0.94890477, ..., 0.92691038, 0.885916  ,\n        1.01934012],\n       ...,\n       [0.88671137, 0.91944589, 1.00541185, ..., 0.96151472, 0.93478611,\n        0.94631027],\n       [0.8367989 , 0.84727656, 1.05992876, ..., 0.91519111, 0.90516942,\n        0.9358838 ],\n       [0.94127918, 0.89667586, 0.91173519, ..., 0.96184559, 0.94224608,\n        1.04527058]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-13T05:36:30.303342+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 312kB\nDimensions:          (chain: 8, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    accept_ratio     (chain, draw) float64 64kB 0.8971 0.9944 ... 0.9174 0.8133\n    diverging        (chain, draw) bool 8kB False False False ... False False\n    is_accepted      (chain, draw) bool 8kB True True True ... True True True\n    n_steps          (chain, draw) int32 32kB 7 7 7 1 7 7 7 3 ... 3 7 7 7 3 7 7\n    step_size        (chain, draw) float64 64kB 0.534 0.534 0.534 ... nan nan\n    target_log_prob  (chain, draw) float64 64kB -141.5 -141.7 ... -141.0 -143.2\n    tune             (chain, draw) float64 64kB 0.0 0.0 0.0 0.0 ... nan nan nan\nAttributes:\n    created_at:                  2024-04-13T05:36:30.304788+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)accept_ratio(chain, draw)float640.8971 0.9944 ... 0.9174 0.8133array([[0.89706765, 0.99441475, 0.80397664, ..., 0.99407977, 1.        ,\n        0.73958291],\n       [0.99821982, 0.95159754, 0.77731848, ..., 0.98139297, 0.91789348,\n        0.96456953],\n       [0.76824526, 0.92239538, 1.        , ..., 0.94414437, 0.91605876,\n        0.92334246],\n       ...,\n       [0.99710475, 0.99154725, 0.58953539, ..., 1.        , 0.92397302,\n        0.99338491],\n       [0.98669117, 0.98477039, 0.95831938, ..., 0.92092812, 0.96842841,\n        0.95013437],\n       [0.91842649, 0.75186373, 0.99689159, ..., 1.        , 0.9173519 ,\n        0.81331846]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])is_accepted(chain, draw)boolTrue True True ... True True Truearray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])n_steps(chain, draw)int327 7 7 1 7 7 7 3 ... 3 3 7 7 7 3 7 7array([[ 7,  7,  7, ...,  7,  7,  7],\n       [ 7,  7,  3, ...,  7,  7,  3],\n       [ 3,  3,  7, ...,  3, 15,  7],\n       ...,\n       [ 7,  7,  3, ...,  3,  7,  7],\n       [ 7,  7,  7, ...,  7,  3,  3],\n       [ 7,  7,  3, ...,  3,  7,  7]], dtype=int32)step_size(chain, draw)float640.534 0.534 0.534 ... nan nan nanarray([[0.53403598, 0.53403598, 0.53403598, ..., 0.53403598, 0.53403598,\n        0.53403598],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       ...,\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan]])target_log_prob(chain, draw)float64-141.5 -141.7 ... -141.0 -143.2array([[-141.50324612, -141.68928918, -142.88857248, ..., -140.47032751,\n        -140.28212037, -140.3879345 ],\n       [-140.01375434, -140.13505676, -143.13641583, ..., -139.99147863,\n        -141.05078466, -141.26466845],\n       [-141.12400308, -142.52999266, -141.18044662, ..., -139.62740626,\n        -139.97278032, -140.75200575],\n       ...,\n       [-139.95465107, -140.14159326, -146.22556521, ..., -139.52421787,\n        -140.79404952, -140.86677285],\n       [-140.66929954, -140.59550579, -141.05595747, ..., -139.29239833,\n        -139.67236895, -139.28664301],\n       [-140.73991807, -140.71540356, -139.69662159, ..., -140.53100323,\n        -140.99477405, -143.18216599]])tune(chain, draw)float640.0 0.0 0.0 0.0 ... nan nan nan nanarray([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-13T05:36:30.304788+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\nnumpyro_nuts_idata = model.fit(inference_method=\"numpyro_nuts\")\nnumpyro_nuts_idata\n\nsample: 100%|██████████| 1500/1500 [00:02&lt;00:00, 599.25it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 200kB\nDimensions:    (chain: 8, draw: 1000)\nCoordinates:\n  * chain      (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw       (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    Intercept  (chain, draw) float64 64kB 0.0004764 0.02933 ... 0.1217 0.1668\n    x          (chain, draw) float64 64kB 0.3836 0.6556 0.2326 ... 0.48 0.5808\n    y_sigma    (chain, draw) float64 64kB 0.8821 0.9604 0.9652 ... 0.9063 0.9184\nAttributes:\n    created_at:                  2024-04-13T05:36:33.599519+00:00\n    arviz_version:               0.18.0\n    inference_library:           numpyro\n    inference_library_version:   0.14.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (3)Intercept(chain, draw)float640.0004764 0.02933 ... 0.1217 0.1668array([[ 0.00047641,  0.02933426, -0.19069113, ..., -0.04607573,\n         0.14408182, -0.07016404],\n       [-0.06542953,  0.09001091,  0.03811068, ...,  0.03170986,\n         0.20861147,  0.18706729],\n       [-0.08028089,  0.03625393,  0.05650287, ..., -0.0093195 ,\n         0.01912548,  0.00214345],\n       ...,\n       [ 0.18184083,  0.07906243,  0.06388914, ..., -0.07055763,\n         0.10986417,  0.09622923],\n       [-0.02521011,  0.15830259, -0.10214413, ...,  0.01471807,\n         0.10706226,  0.07562878],\n       [-0.02468806, -0.03414193, -0.06678234, ...,  0.08710519,\n         0.12166933,  0.16679929]])x(chain, draw)float640.3836 0.6556 ... 0.48 0.5808array([[0.38361084, 0.65556045, 0.23260059, ..., 0.65580692, 0.44095681,\n        0.22838517],\n       [0.30187358, 0.46285734, 0.31814527, ..., 0.38133365, 0.32358724,\n        0.37070791],\n       [0.44410357, 0.42831529, 0.3990648 , ..., 0.37993575, 0.40377358,\n        0.42804019],\n       ...,\n       [0.49080324, 0.20770949, 0.12142607, ..., 0.44054445, 0.38924394,\n        0.38167612],\n       [0.34590162, 0.30144285, 0.45780034, ..., 0.44424986, 0.52104263,\n        0.45543543],\n       [0.23738988, 0.68021684, 0.05589656, ..., 0.42147165, 0.48000601,\n        0.58081686]])y_sigma(chain, draw)float640.8821 0.9604 ... 0.9063 0.9184array([[0.88211276, 0.96036122, 0.96524442, ..., 0.94362502, 1.00228679,\n        0.88249142],\n       [0.93345676, 0.85184129, 1.07135935, ..., 0.92649839, 0.86831784,\n        0.92890112],\n       [0.973364  , 1.04138907, 0.96240687, ..., 0.9564475 , 1.0092212 ,\n        0.87607713],\n       ...,\n       [1.03355029, 0.98103228, 0.92902834, ..., 0.83197448, 0.99111854,\n        0.92967952],\n       [0.88101923, 1.0226885 , 0.87217557, ..., 0.94028186, 0.88687764,\n        0.85291778],\n       [0.98596365, 0.91083125, 0.9972831 , ..., 0.86419289, 0.90625839,\n        0.91841349]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (6)created_at :2024-04-13T05:36:33.599519+00:00arviz_version :0.18.0inference_library :numpyroinference_library_version :0.14.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 400kB\nDimensions:          (chain: 8, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 64kB 0.9366 0.3542 ... 0.9903 0.8838\n    diverging        (chain, draw) bool 8kB False False False ... False False\n    energy           (chain, draw) float64 64kB 140.3 145.4 ... 141.0 142.6\n    lp               (chain, draw) float64 64kB 139.6 143.3 ... 140.5 142.5\n    n_steps          (chain, draw) int64 64kB 3 3 7 3 7 1 3 3 ... 11 7 3 3 3 3 3\n    step_size        (chain, draw) float64 64kB 0.8891 0.8891 ... 0.7595 0.7595\n    tree_depth       (chain, draw) int64 64kB 2 2 3 2 3 1 2 2 ... 4 3 2 2 2 2 2\nAttributes:\n    created_at:                  2024-04-13T05:36:33.623197+00:00\n    arviz_version:               0.18.0\n    inference_library:           numpyro\n    inference_library_version:   0.14.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)acceptance_rate(chain, draw)float640.9366 0.3542 ... 0.9903 0.8838array([[0.93661577, 0.35419612, 0.99435023, ..., 0.59003267, 1.        ,\n        0.96452433],\n       [0.9974338 , 0.86250112, 0.95945138, ..., 0.78208773, 0.79906599,\n        1.        ],\n       [0.96468642, 0.97525962, 0.98495362, ..., 0.9775774 , 0.86156602,\n        0.89713276],\n       ...,\n       [0.96610793, 0.98086156, 0.89084022, ..., 0.95772457, 0.70497474,\n        0.99836264],\n       [0.99806445, 0.76652499, 0.98528715, ..., 0.87560309, 0.81183609,\n        1.        ],\n       [0.99527811, 0.68120359, 1.        , ..., 1.        , 0.99026377,\n        0.88375821]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64140.3 145.4 144.9 ... 141.0 142.6array([[140.3467186 , 145.35611236, 144.90015851, ..., 144.55362515,\n        144.91727277, 142.02764348],\n       [141.22868211, 142.17975426, 142.8293087 , ..., 142.28333599,\n        142.31235728, 142.52446205],\n       [144.0720805 , 141.47372901, 140.93450225, ..., 140.18268471,\n        140.91505172, 141.58829808],\n       ...,\n       [142.62435102, 143.2286207 , 142.80972656, ..., 144.52562661,\n        145.55539646, 140.15311622],\n       [142.10722077, 142.2552196 , 142.37900379, ..., 140.54316684,\n        141.75774836, 141.58904973],\n       [140.82607551, 145.03403209, 145.2241475 , ..., 141.6051566 ,\n        140.9854462 , 142.60201809]])lp(chain, draw)float64139.6 143.3 142.2 ... 140.5 142.5array([[139.63264222, 143.32720395, 142.23861021, ..., 143.97346231,\n        140.67090827, 140.87592102],\n       [139.81127204, 140.89739965, 141.16918465, ..., 139.28792859,\n        142.23961923, 140.80131401],\n       [140.45054751, 140.62345763, 139.48418855, ..., 139.42120354,\n        139.97119948, 139.9471655 ],\n       ...,\n       [141.9099355 , 140.83346483, 142.30482492, ..., 141.78502364,\n        140.03250066, 139.57119158],\n       [139.72932132, 141.21433158, 141.46193109, ..., 139.62199619,\n        141.12185494, 140.69980318],\n       [140.29191412, 144.98589172, 143.74390375, ..., 140.2609778 ,\n        140.48172152, 142.47099219]])n_steps(chain, draw)int643 3 7 3 7 1 3 3 ... 11 7 3 3 3 3 3array([[3, 3, 7, ..., 3, 3, 7],\n       [7, 3, 7, ..., 3, 3, 7],\n       [3, 7, 3, ..., 3, 3, 3],\n       ...,\n       [7, 7, 7, ..., 3, 7, 7],\n       [3, 7, 7, ..., 3, 3, 7],\n       [7, 7, 3, ..., 3, 3, 3]])step_size(chain, draw)float640.8891 0.8891 ... 0.7595 0.7595array([[0.8891145 , 0.8891145 , 0.8891145 , ..., 0.8891145 , 0.8891145 ,\n        0.8891145 ],\n       [0.70896111, 0.70896111, 0.70896111, ..., 0.70896111, 0.70896111,\n        0.70896111],\n       [0.8087902 , 0.8087902 , 0.8087902 , ..., 0.8087902 , 0.8087902 ,\n        0.8087902 ],\n       ...,\n       [0.69745418, 0.69745418, 0.69745418, ..., 0.69745418, 0.69745418,\n        0.69745418],\n       [0.88034552, 0.88034552, 0.88034552, ..., 0.88034552, 0.88034552,\n        0.88034552],\n       [0.7595237 , 0.7595237 , 0.7595237 , ..., 0.7595237 , 0.7595237 ,\n        0.7595237 ]])tree_depth(chain, draw)int642 2 3 2 3 1 2 2 ... 2 4 3 2 2 2 2 2array([[2, 2, 3, ..., 2, 2, 3],\n       [3, 2, 3, ..., 2, 2, 3],\n       [2, 3, 2, ..., 2, 2, 2],\n       ...,\n       [3, 3, 3, ..., 2, 3, 3],\n       [2, 3, 3, ..., 2, 2, 3],\n       [3, 3, 2, ..., 2, 2, 2]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (6)created_at :2024-04-13T05:36:33.623197+00:00arviz_version :0.18.0inference_library :numpyroinference_library_version :0.14.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\nflowmc_idata = model.fit(inference_method=\"flowmc_realnvp_hmc\")\nflowmc_idata\n\nNo autotune found, use input sampler_params\nTraining normalizing flow\n\n\nTuning global sampler: 100%|██████████| 5/5 [00:51&lt;00:00, 10.37s/it]\n\n\nStarting Production run\n\n\nProduction run: 100%|██████████| 5/5 [00:00&lt;00:00, 14.38it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 244kB\nDimensions:    (chain: 20, draw: 500)\nCoordinates:\n  * chain      (chain) int64 160B 0 1 2 3 4 5 6 7 8 ... 12 13 14 15 16 17 18 19\n  * draw       (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    Intercept  (chain, draw) float64 80kB -0.07404 -0.07404 ... -0.1455 0.09545\n    x          (chain, draw) float64 80kB 0.4401 0.4401 0.3533 ... 0.6115 0.3824\n    y_sigma    (chain, draw) float64 80kB 0.9181 0.9181 0.9732 ... 1.049 0.9643\nAttributes:\n    created_at:                  2024-04-13T05:37:29.798250+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 20draw: 500Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 ... 14 15 16 17 18 19array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (3)Intercept(chain, draw)float64-0.07404 -0.07404 ... 0.09545array([[-0.07403956, -0.07403956, -0.08175103, ..., -0.07505013,\n        -0.06280065, -0.06280065],\n       [ 0.15335447,  0.15335447,  0.12713003, ..., -0.02037623,\n         0.06826204,  0.01933312],\n       [ 0.00658099,  0.00658099,  0.00658099, ..., -0.04271278,\n        -0.04271278, -0.09780863],\n       ...,\n       [ 0.00629487,  0.01048304, -0.03193874, ...,  0.13237167,\n         0.08595727,  0.01442809],\n       [ 0.05972149,  0.02490161, -0.00084261, ...,  0.06751994,\n        -0.15926318, -0.15926318],\n       [ 0.23012418,  0.25630661,  0.23839857, ...,  0.07975465,\n        -0.14554836,  0.09545347]])x(chain, draw)float640.4401 0.4401 ... 0.6115 0.3824array([[0.44013865, 0.44013865, 0.35326474, ..., 0.30371128, 0.28793687,\n        0.28793687],\n       [0.45569737, 0.45569737, 0.55350522, ..., 0.37498493, 0.45850535,\n        0.40671648],\n       [0.24971734, 0.24971734, 0.24971734, ..., 0.19912158, 0.19912158,\n        0.46992411],\n       ...,\n       [0.44071041, 0.47684243, 0.35786393, ..., 0.37932871, 0.31101246,\n        0.25090813],\n       [0.43305649, 0.19703032, 0.21622992, ..., 0.39021766, 0.35161734,\n        0.35161734],\n       [0.52832789, 0.50016524, 0.19504762, ..., 0.25411208, 0.61146903,\n        0.38243421]])y_sigma(chain, draw)float640.9181 0.9181 ... 1.049 0.9643array([[0.91812597, 0.91812597, 0.97317218, ..., 0.87193011, 0.98202548,\n        0.98202548],\n       [0.92619283, 0.92619283, 0.89113835, ..., 1.00239178, 0.93585383,\n        0.93328517],\n       [0.96032594, 0.96032594, 0.96032594, ..., 0.90617649, 0.90617649,\n        0.95241728],\n       ...,\n       [1.01337917, 0.96203307, 0.81645174, ..., 1.00979845, 1.07249345,\n        0.9165658 ],\n       [0.97087149, 0.91876884, 0.87129204, ..., 1.09021385, 1.0093326 ,\n        1.0093326 ],\n       [0.89533883, 0.91515164, 1.07248889, ..., 0.95594426, 1.04908995,\n        0.96426064]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (4)created_at :2024-04-13T05:37:29.798250+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\nWith ArviZ, we can compare the inference result summaries of the samplers. Note: We can’t use az.compare as not each inference data object returns the pointwise log-probabilities. Thus, an error would be raised.\n\naz.summary(blackjax_nuts_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.023\n0.097\n-0.141\n0.209\n0.004\n0.003\n694.0\n508.0\n1.00\n\n\nx\n0.356\n0.111\n0.162\n0.571\n0.004\n0.003\n970.0\n675.0\n1.00\n\n\ny_sigma\n0.950\n0.069\n0.827\n1.072\n0.002\n0.001\n1418.0\n842.0\n1.01\n\n\n\n\n\n\n\n\naz.summary(tfp_nuts_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.023\n0.097\n-0.157\n0.205\n0.001\n0.001\n6785.0\n5740.0\n1.0\n\n\nx\n0.360\n0.105\n0.169\n0.563\n0.001\n0.001\n6988.0\n5116.0\n1.0\n\n\ny_sigma\n0.946\n0.067\n0.831\n1.081\n0.001\n0.001\n7476.0\n5971.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(numpyro_nuts_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.024\n0.095\n-0.162\n0.195\n0.001\n0.001\n6851.0\n5614.0\n1.0\n\n\nx\n0.362\n0.104\n0.176\n0.557\n0.001\n0.001\n9241.0\n6340.0\n1.0\n\n\ny_sigma\n0.946\n0.068\n0.826\n1.079\n0.001\n0.001\n7247.0\n5711.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(flowmc_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.015\n0.100\n-0.186\n0.190\n0.004\n0.003\n758.0\n1233.0\n1.02\n\n\nx\n0.361\n0.105\n0.174\n0.565\n0.001\n0.001\n5084.0\n4525.0\n1.00\n\n\ny_sigma\n0.951\n0.070\n0.823\n1.079\n0.001\n0.001\n5536.0\n5080.0\n1.00\n\n\n\n\n\n\n\n\n\n\nThanks to bayeux, we can use three different sampling backends and 10+ alternative MCMC methods in Bambi. Using these methods is as simple as passing the inference name to the inference_method of the fit method.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Sat Apr 13 2024\n\nPython implementation: CPython\nPython version       : 3.12.2\nIPython version      : 8.20.0\n\nbambi : 0.13.1.dev25+g1e7f677e.d20240413\npandas: 2.2.1\nnumpy : 1.26.4\nbayeux: 0.1.10\narviz : 0.18.0\n\nWatermark: 2.4.3"
  },
  {
    "objectID": "posts/2024-03-29-alternative_samplers.html#bayeux",
    "href": "posts/2024-03-29-alternative_samplers.html#bayeux",
    "title": "Using Alternative Samplers in Bambi",
    "section": "",
    "text": "Bambi leverages bayeux to access different sampling backends. In short, bayeux lets you write a probabilistic model in JAX and immediately have access to state-of-the-art inference methods.\nSince the underlying Bambi model is a PyMC model, this PyMC model can be “given” to bayeux. Then, we can choose from a variety of MCMC methods to perform inference.\nTo demonstrate the available backends, we will fist simulate data and build a model.\n\nnum_samples = 100\nnum_features = 1\nnoise_std = 1.0\nrandom_seed = 42\n\nnp.random.seed(random_seed)\n\ncoefficients = np.random.randn(num_features)\nX = np.random.randn(num_samples, num_features)\nerror = np.random.normal(scale=noise_std, size=num_samples)\ny = X @ coefficients + error\n\ndata = pd.DataFrame({\"y\": y, \"x\": X.flatten()})\n\n\nmodel = bmb.Model(\"y ~ x\", data)\nmodel.build()\n\nWe can call bmb.inference_methods.names that returns a nested dictionary of the backends and list of inference methods.\n\nmethods = bmb.inference_methods.names\nmethods\n\n{'pymc': {'mcmc': ['mcmc'], 'vi': ['vi']},\n 'bayeux': {'mcmc': ['tfp_hmc',\n   'tfp_nuts',\n   'tfp_snaper_hmc',\n   'blackjax_hmc',\n   'blackjax_chees_hmc',\n   'blackjax_meads_hmc',\n   'blackjax_nuts',\n   'blackjax_hmc_pathfinder',\n   'blackjax_nuts_pathfinder',\n   'flowmc_rqspline_hmc',\n   'flowmc_rqspline_mala',\n   'flowmc_realnvp_hmc',\n   'flowmc_realnvp_mala',\n   'numpyro_hmc',\n   'numpyro_nuts']}}\n\n\nWith the PyMC backend, we have access to their implementation of the NUTS sampler and mean-field variational inference.\n\nmethods[\"pymc\"]\n\n{'mcmc': ['mcmc'], 'vi': ['vi']}\n\n\nbayeux lets us have access to Tensorflow probability, Blackjax, FlowMC, and NumPyro backends.\n\nmethods[\"bayeux\"]\n\n{'mcmc': ['tfp_hmc',\n  'tfp_nuts',\n  'tfp_snaper_hmc',\n  'blackjax_hmc',\n  'blackjax_chees_hmc',\n  'blackjax_meads_hmc',\n  'blackjax_nuts',\n  'blackjax_hmc_pathfinder',\n  'blackjax_nuts_pathfinder',\n  'flowmc_rqspline_hmc',\n  'flowmc_rqspline_mala',\n  'flowmc_realnvp_hmc',\n  'flowmc_realnvp_mala',\n  'numpyro_hmc',\n  'numpyro_nuts']}\n\n\nThe values of the MCMC and VI keys in the dictionary are the names of the argument you would pass to inference_method in model.fit. This is shown in the section below."
  },
  {
    "objectID": "posts/2024-03-29-alternative_samplers.html#specifying-an-inference_method",
    "href": "posts/2024-03-29-alternative_samplers.html#specifying-an-inference_method",
    "title": "Using Alternative Samplers in Bambi",
    "section": "",
    "text": "By default, Bambi uses the PyMC NUTS implementation. To use a different backend, pass the name of the bayeux MCMC method to the inference_method parameter of the fit method.\n\n\n\nblackjax_nuts_idata = model.fit(inference_method=\"blackjax_nuts\")\nblackjax_nuts_idata\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 100kB\nDimensions:    (chain: 8, draw: 500)\nCoordinates:\n  * chain      (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw       (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    Intercept  (chain, draw) float64 32kB 0.04421 0.1077 ... 0.0259 0.06753\n    x          (chain, draw) float64 32kB 0.1353 0.232 0.5141 ... 0.2195 0.5014\n    y_sigma    (chain, draw) float64 32kB 0.9443 0.9102 0.922 ... 0.9597 0.9249\nAttributes:\n    created_at:                  2024-04-13T05:34:49.761913+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 500Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (3)Intercept(chain, draw)float640.04421 0.1077 ... 0.0259 0.06753array([[ 0.04420881,  0.10774889, -0.01477631, ...,  0.05546068,\n        -0.01057083,  0.09897323],\n       [ 0.00694954,  0.10744512, -0.017276  , ...,  0.19618115,\n         0.06402486, -0.01106827],\n       [ 0.16110577, -0.07458938,  0.04475104, ...,  0.16745381,\n        -0.00406837,  0.07311051],\n       ...,\n       [ 0.09943931, -0.03684845,  0.09735818, ..., -0.06556524,\n         0.11011645,  0.08414361],\n       [-0.07703379,  0.02738655,  0.02285994, ...,  0.14379745,\n        -0.10339471, -0.02836366],\n       [ 0.04903997, -0.03220716, -0.02720002, ...,  0.17203999,\n         0.02589751,  0.06752773]])x(chain, draw)float640.1353 0.232 ... 0.2195 0.5014array([[0.13526029, 0.23196226, 0.51413147, ..., 0.23278954, 0.32745043,\n        0.37862773],\n       [0.40584773, 0.51513052, 0.2268538 , ..., 0.41687492, 0.30601076,\n        0.2634667 ],\n       [0.41543724, 0.44571834, 0.23530532, ..., 0.6172463 , 0.29822452,\n        0.45765768],\n       ...,\n       [0.49946851, 0.29694244, 0.44142996, ..., 0.26425056, 0.46471836,\n        0.32217591],\n       [0.41877449, 0.33327679, 0.4045056 , ..., 0.66448843, 0.24280931,\n        0.50115044],\n       [0.51180277, 0.42393989, 0.56394504, ..., 0.29234944, 0.21949889,\n        0.5013853 ]])y_sigma(chain, draw)float640.9443 0.9102 ... 0.9597 0.9249array([[0.94428889, 0.91016104, 0.92196855, ..., 0.83634906, 0.79627853,\n        1.08163408],\n       [0.87025311, 0.85044922, 0.91347637, ..., 1.0028945 , 0.77749843,\n        0.87518191],\n       [0.94615571, 0.84280628, 1.05011189, ..., 1.0255364 , 0.96478417,\n        0.9140493 ],\n       ...,\n       [0.87146472, 1.04641364, 0.86900166, ..., 0.91303204, 0.95041789,\n        0.96797332],\n       [0.94906021, 0.99194229, 0.84058257, ..., 0.99087914, 0.96639345,\n        0.99059172],\n       [0.91025793, 0.8993632 , 1.03222263, ..., 0.9717563 , 0.95967178,\n        0.92491709]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (4)created_at :2024-04-13T05:34:49.761913+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 200kB\nDimensions:          (chain: 8, draw: 500)\nCoordinates:\n  * chain            (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw             (draw) int64 4kB 0 1 2 3 4 5 6 ... 494 495 496 497 498 499\nData variables:\n    acceptance_rate  (chain, draw) float64 32kB 0.8137 1.0 1.0 ... 0.9094 0.9834\n    diverging        (chain, draw) bool 4kB False False False ... False False\n    energy           (chain, draw) float64 32kB 142.0 142.5 ... 143.0 141.5\n    lp               (chain, draw) float64 32kB -141.8 -140.8 ... -140.3 -140.3\n    n_steps          (chain, draw) int64 32kB 3 3 7 7 7 3 3 7 ... 7 3 7 5 7 3 7\n    step_size        (chain, draw) float64 32kB 0.7326 0.7326 ... 0.7643 0.7643\n    tree_depth       (chain, draw) int64 32kB 2 2 3 3 3 2 2 3 ... 3 2 3 3 3 2 3\nAttributes:\n    created_at:                  2024-04-13T05:34:49.763427+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 500Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (7)acceptance_rate(chain, draw)float640.8137 1.0 1.0 ... 0.9094 0.9834array([[0.81368466, 1.        , 1.        , ..., 0.95726715, 0.95332204,\n        1.        ],\n       [0.98623155, 0.76947694, 1.        , ..., 0.6501189 , 0.6980205 ,\n        1.        ],\n       [0.98539058, 0.82802334, 0.96559601, ..., 0.72850635, 1.        ,\n        0.86563511],\n       ...,\n       [0.79238494, 0.97989654, 0.94005541, ..., 0.98283263, 0.99321313,\n        0.92314755],\n       [0.95823733, 0.94198648, 0.91853339, ..., 0.68699656, 0.972578  ,\n        0.74390253],\n       [0.99181102, 0.97429544, 0.78790853, ..., 1.        , 0.90941548,\n        0.98341956]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64142.0 142.5 141.8 ... 143.0 141.5array([[141.967537  , 142.54106626, 141.78398912, ..., 142.07442022,\n        143.72755872, 142.21633731],\n       [142.4254147 , 142.09857076, 141.94644772, ..., 142.39410249,\n        146.51261269, 143.65723574],\n       [141.4414682 , 142.96701481, 142.39395521, ..., 144.02150538,\n        142.66721068, 140.85225446],\n       ...,\n       [142.45649528, 142.75638401, 142.59874467, ..., 141.35824722,\n        140.94450824, 141.2808887 ],\n       [140.3251331 , 141.16320788, 140.88902952, ..., 144.52035375,\n        144.09031991, 143.92351871],\n       [142.65283365, 141.01504212, 142.60582761, ..., 143.46419056,\n        142.97607812, 141.46662296]])lp(chain, draw)float64-141.8 -140.8 ... -140.3 -140.3array([[-141.78590569, -140.75355135, -140.61320047, ..., -141.63502666,\n        -142.12600187, -141.57227603],\n       [-139.88014501, -141.79255751, -140.226333  , ..., -141.301519  ,\n        -143.3329595 , -140.20584575],\n       [-140.39038429, -141.56925705, -141.27509741, ..., -143.36048355,\n        -139.58615368, -139.84922801],\n       ...,\n       [-140.99893216, -140.82540718, -140.38825538, ..., -140.12098164,\n        -140.10850196, -139.71074945],\n       [-140.09932106, -139.69086444, -140.49414807, ..., -143.90263595,\n        -140.69641315, -140.7183776 ],\n       [-140.46042516, -139.8366111 , -142.15416918, ..., -140.96117584,\n        -140.27772734, -140.27024162]])n_steps(chain, draw)int643 3 7 7 7 3 3 7 ... 7 7 3 7 5 7 3 7array([[ 3,  3,  7, ...,  7,  7,  7],\n       [ 3,  3,  3, ...,  3,  7,  3],\n       [11,  3,  3, ...,  3,  3,  3],\n       ...,\n       [ 7,  7,  7, ...,  7,  7,  3],\n       [ 3,  3,  3, ...,  7,  7,  3],\n       [ 7,  3,  3, ...,  7,  3,  7]])step_size(chain, draw)float640.7326 0.7326 ... 0.7643 0.7643array([[0.73264667, 0.73264667, 0.73264667, ..., 0.73264667, 0.73264667,\n        0.73264667],\n       [0.84139296, 0.84139296, 0.84139296, ..., 0.84139296, 0.84139296,\n        0.84139296],\n       [0.90832794, 0.90832794, 0.90832794, ..., 0.90832794, 0.90832794,\n        0.90832794],\n       ...,\n       [0.75868138, 0.75868138, 0.75868138, ..., 0.75868138, 0.75868138,\n        0.75868138],\n       [0.83356209, 0.83356209, 0.83356209, ..., 0.83356209, 0.83356209,\n        0.83356209],\n       [0.76429536, 0.76429536, 0.76429536, ..., 0.76429536, 0.76429536,\n        0.76429536]])tree_depth(chain, draw)int642 2 3 3 3 2 2 3 ... 3 3 2 3 3 3 2 3array([[2, 2, 3, ..., 3, 3, 3],\n       [2, 2, 2, ..., 2, 3, 2],\n       [4, 2, 2, ..., 2, 2, 2],\n       ...,\n       [3, 3, 3, ..., 3, 3, 2],\n       [2, 2, 2, ..., 3, 3, 2],\n       [3, 2, 2, ..., 3, 2, 3]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (4)created_at :2024-04-13T05:34:49.763427+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\nDifferent backends have different naming conventions for the parameters specific to that MCMC method. Thus, to specify backend-specific parameters, pass your own kwargs to the fit method.\nThe following can be performend to identify the kwargs specific to each method.\n\nbmb.inference_methods.get_kwargs(\"blackjax_nuts\")\n\n{&lt;function blackjax.adaptation.window_adaptation.window_adaptation(algorithm: Union[blackjax.mcmc.hmc.hmc, blackjax.mcmc.nuts.nuts], logdensity_fn: Callable, is_mass_matrix_diagonal: bool = True, initial_step_size: float = 1.0, target_acceptance_rate: float = 0.8, progress_bar: bool = False, **extra_parameters) -&gt; blackjax.base.AdaptationAlgorithm&gt;: {'logdensity_fn': &lt;function bayeux._src.shared.constrain.&lt;locals&gt;.wrap_log_density.&lt;locals&gt;.wrapped(args)&gt;,\n  'is_mass_matrix_diagonal': True,\n  'initial_step_size': 1.0,\n  'target_acceptance_rate': 0.8,\n  'progress_bar': False,\n  'algorithm': blackjax.mcmc.nuts.nuts},\n 'adapt.run': {'num_steps': 500},\n blackjax.mcmc.nuts.nuts: {'max_num_doublings': 10,\n  'divergence_threshold': 1000,\n  'integrator': &lt;function blackjax.mcmc.integrators.generate_euclidean_integrator.&lt;locals&gt;.euclidean_integrator(logdensity_fn: Callable, kinetic_energy_fn: blackjax.mcmc.metrics.KineticEnergy) -&gt; Callable[[blackjax.mcmc.integrators.IntegratorState, float], blackjax.mcmc.integrators.IntegratorState]&gt;,\n  'logdensity_fn': &lt;function bayeux._src.shared.constrain.&lt;locals&gt;.wrap_log_density.&lt;locals&gt;.wrapped(args)&gt;,\n  'step_size': 0.5},\n 'extra_parameters': {'chain_method': 'vectorized',\n  'num_chains': 8,\n  'num_draws': 500,\n  'num_adapt_draws': 500,\n  'return_pytree': False}}\n\n\nNow, we can identify the kwargs we would like to change and pass to the fit method.\n\nkwargs = {\n        \"adapt.run\": {\"num_steps\": 500},\n        \"num_chains\": 4,\n        \"num_draws\": 250,\n        \"num_adapt_draws\": 250\n}\n\nblackjax_nuts_idata = model.fit(inference_method=\"blackjax_nuts\", **kwargs)\nblackjax_nuts_idata\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 26kB\nDimensions:    (chain: 4, draw: 250)\nCoordinates:\n  * chain      (chain) int64 32B 0 1 2 3\n  * draw       (draw) int64 2kB 0 1 2 3 4 5 6 7 ... 243 244 245 246 247 248 249\nData variables:\n    Intercept  (chain, draw) float64 8kB 0.112 -0.08016 ... -0.04784 -0.04427\n    x          (chain, draw) float64 8kB 0.4311 0.3077 0.2568 ... 0.557 0.4814\n    y_sigma    (chain, draw) float64 8kB 0.9461 0.9216 0.9021 ... 0.9574 0.9414\nAttributes:\n    created_at:                  2024-04-13T05:36:20.439151+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 4draw: 250Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 245 246 247 248 249array([  0,   1,   2, ..., 247, 248, 249])Data variables: (3)Intercept(chain, draw)float640.112 -0.08016 ... -0.04427array([[ 1.12014970e-01, -8.01554356e-02, -5.13623666e-02,\n         3.14326884e-02, -1.15157203e-02,  8.29955919e-02,\n         1.93598964e-02,  3.52348779e-02,  1.16428197e-01,\n         8.72568457e-02,  8.72568457e-02, -2.82423636e-02,\n        -1.84281659e-01, -7.19613600e-02,  1.91168564e-01,\n        -1.23364451e-01, -1.19337859e-01, -3.08974539e-02,\n        -7.18298964e-02,  1.19861654e-01, -1.00735526e-01,\n         6.32700448e-02, -5.72727355e-03,  1.17338655e-01,\n        -3.83727222e-02,  1.84746578e-01,  7.26989694e-02,\n         1.94564654e-01,  1.44827239e-01, -8.50718497e-02,\n         1.87666705e-01, -1.60104555e-01, -4.99203302e-02,\n         3.19979141e-03, -3.94543945e-03,  8.66672888e-04,\n         4.33390733e-02,  5.22653528e-02, -4.77497368e-02,\n        -1.88745071e-02,  5.03627424e-02,  8.24434767e-02,\n        -3.79889140e-03,  7.70856139e-03, -4.77259521e-02,\n        -1.90736318e-02,  6.76733158e-02,  5.14461069e-02,\n        -4.56113715e-02,  1.60543248e-01,  7.32836299e-02,\n         2.28842579e-03,  3.05194139e-02,  4.27895103e-02,\n        -2.51634507e-02, -4.60161935e-02,  2.44964388e-01,\n         2.76318153e-01,  4.33818171e-02,  1.46208904e-01,\n...\n        -5.68838115e-02, -6.14275201e-02, -5.96425618e-02,\n         6.11356758e-02,  6.42661723e-03,  5.41912583e-02,\n        -1.76976244e-01, -4.62930404e-02,  9.61963932e-02,\n        -1.40433636e-01,  2.05056910e-01,  1.82385197e-01,\n         1.21005125e-01, -9.65523825e-02,  9.11450646e-02,\n         1.49525640e-02, -8.32289763e-02,  3.24479331e-02,\n         1.09007071e-02, -6.92830705e-02,  6.64926592e-02,\n         3.23060974e-02, -1.73437807e-01, -1.25619389e-03,\n         8.89183729e-02,  1.02309051e-01,  4.12736086e-02,\n         1.03893380e-01,  6.89267255e-02,  1.37649597e-01,\n        -7.63849028e-02,  7.69987215e-02, -1.14605433e-01,\n        -1.59066163e-01,  2.02049201e-01,  1.67222994e-01,\n        -5.02468032e-02, -1.17601875e-01, -1.67595598e-03,\n        -1.97669449e-01,  4.36079372e-02,  8.41929183e-02,\n         1.31836071e-01,  1.65427331e-01,  1.26585460e-01,\n        -7.27516393e-02,  5.41849189e-02,  2.21844869e-02,\n         5.94315594e-02,  5.94315594e-02,  7.45566691e-02,\n        -9.33357688e-03, -4.93686976e-02, -3.43353187e-02,\n         6.89221401e-02, -3.19652375e-02, -4.78438315e-02,\n        -4.42738699e-02]])x(chain, draw)float640.4311 0.3077 ... 0.557 0.4814array([[0.43113703, 0.30774729, 0.25682166, 0.31641395, 0.51546729,\n        0.31100976, 0.44043531, 0.38164497, 0.42080014, 0.30069041,\n        0.30069041, 0.53305267, 0.11871418, 0.22919114, 0.22043383,\n        0.32368544, 0.28827739, 0.44216387, 0.38292596, 0.38328387,\n        0.31277052, 0.28380182, 0.39125062, 0.5436668 , 0.19914823,\n        0.23381157, 0.3952613 , 0.48281672, 0.27598205, 0.46597795,\n        0.48635971, 0.21363092, 0.39350997, 0.42601567, 0.3035345 ,\n        0.26553072, 0.44019149, 0.34397815, 0.23609522, 0.53683168,\n        0.45841485, 0.23891478, 0.54442998, 0.16697332, 0.19146859,\n        0.22799538, 0.39366724, 0.37134365, 0.34501806, 0.37506017,\n        0.28311981, 0.16254121, 0.61289656, 0.13063232, 0.03017502,\n        0.18434623, 0.36065819, 0.52235008, 0.24458848, 0.14313226,\n        0.22279879, 0.44892021, 0.3952106 , 0.34290512, 0.42439318,\n        0.23102895, 0.19110882, 0.25093658, 0.37681057, 0.36135287,\n        0.30745033, 0.27562781, 0.31724922, 0.45716849, 0.47116505,\n        0.43884602, 0.43553571, 0.29161261, 0.41998198, 0.40796597,\n        0.30405689, 0.31259796, 0.20570747, 0.39392466, 0.31348596,\n        0.4214938 , 0.52463068, 0.16792862, 0.28029374, 0.16153929,\n        0.16724633, 0.08144633, 0.19192458, 0.34938819, 0.13305379,\n        0.13881198, 0.37849938, 0.37084368, 0.27404992, 0.46209003,\n...\n        0.36307521, 0.28227501, 0.38551525, 0.23261809, 0.36514994,\n        0.35783934, 0.3823261 , 0.30670976, 0.32886498, 0.37068029,\n        0.34013729, 0.52474148, 0.639228  , 0.09371894, 0.42023135,\n        0.36733482, 0.40599032, 0.24382963, 0.40221825, 0.3047073 ,\n        0.24407962, 0.30213837, 0.44363912, 0.57883031, 0.48781764,\n        0.48525391, 0.29198732, 0.37745175, 0.31777746, 0.28262044,\n        0.18702892, 0.51867304, 0.52340339, 0.24125419, 0.23332597,\n        0.46851727, 0.46079104, 0.32301517, 0.35635714, 0.33111389,\n        0.37437903, 0.28657985, 0.43734974, 0.35478284, 0.30887643,\n        0.49867288, 0.2525673 , 0.33079942, 0.02800324, 0.31465776,\n        0.4585882 , 0.28368126, 0.4896697 , 0.44762422, 0.41453835,\n        0.246885  , 0.37482138, 0.40059614, 0.27591068, 0.21900013,\n        0.47128275, 0.21132567, 0.39900367, 0.2329504 , 0.39579287,\n        0.37344961, 0.34516518, 0.32227915, 0.35271413, 0.37687565,\n        0.31151605, 0.37301695, 0.26012957, 0.30024098, 0.2745939 ,\n        0.25698144, 0.37064686, 0.43608796, 0.29833848, 0.4057974 ,\n        0.37998817, 0.3505483 , 0.3385325 , 0.29122156, 0.46273061,\n        0.26565498, 0.27156778, 0.37355743, 0.30409736, 0.34103447,\n        0.33149781, 0.33149781, 0.43853323, 0.27834059, 0.36111445,\n        0.40162141, 0.42356887, 0.55111442, 0.55695307, 0.48135813]])y_sigma(chain, draw)float640.9461 0.9216 ... 0.9574 0.9414array([[0.94608021, 0.92160426, 0.90211804, 0.8784906 , 1.00443151,\n        0.96727798, 0.87524178, 0.96817024, 0.86457884, 1.0262922 ,\n        1.0262922 , 0.84391933, 0.98129848, 1.00988048, 0.99554283,\n        0.88587989, 1.00586764, 0.92997466, 0.94168997, 0.99374899,\n        0.92810392, 0.96878376, 0.90682728, 0.94883541, 1.01957489,\n        1.02278733, 0.90160336, 0.94497765, 0.87888132, 0.92205875,\n        0.9138956 , 0.96519328, 1.06316311, 0.84402636, 0.83729644,\n        0.89811997, 0.97144791, 0.98208145, 0.91289233, 0.96673035,\n        0.95542624, 0.91245841, 0.96527727, 0.92783747, 1.03786087,\n        0.94764661, 1.00547045, 0.85588467, 0.98223118, 0.8674327 ,\n        0.94037555, 0.91725845, 0.99391199, 0.92434293, 0.9638643 ,\n        1.08815478, 1.01399545, 1.02349856, 0.92934388, 0.96598116,\n        0.96311436, 0.93945143, 0.89124759, 0.98455184, 0.89591612,\n        1.006701  , 0.95597051, 1.00027136, 0.91409196, 0.97378494,\n        0.87137146, 0.87160277, 1.04749666, 0.8805835 , 0.8819731 ,\n        0.88645983, 1.00263402, 0.88708112, 0.99995189, 1.01743406,\n        0.87473936, 0.9076109 , 1.02202715, 0.88250374, 1.06665137,\n        0.84538309, 0.84109731, 1.0524254 , 0.97522117, 0.94564838,\n        1.05965236, 0.97217503, 0.96459187, 0.9413301 , 1.00422163,\n        1.00733854, 0.95474848, 0.94441562, 0.89236671, 0.96775448,\n...\n        0.9959878 , 0.93569281, 0.96401675, 0.88786078, 1.14540889,\n        0.8224594 , 0.84935646, 1.03698789, 1.00625543, 0.88735547,\n        0.99331278, 1.00797432, 0.94295773, 0.98513086, 1.0195952 ,\n        0.88995881, 0.84278984, 1.02888997, 0.96128787, 0.91245996,\n        0.97871983, 0.89146682, 0.98259937, 0.95369473, 1.02821356,\n        0.83242344, 1.06338194, 0.82728423, 1.06433136, 0.85249613,\n        0.92553966, 0.96450458, 1.05280513, 0.90353168, 0.84823849,\n        1.03949674, 0.92214448, 0.9231072 , 0.87897527, 0.95304901,\n        0.91455056, 0.97220005, 0.91253068, 0.92932491, 0.85741327,\n        1.05336522, 1.05774423, 1.20149457, 0.99443219, 0.89727566,\n        0.97462237, 0.9137672 , 0.99391023, 0.98467151, 0.83221799,\n        1.06702143, 0.85499338, 0.94884501, 0.94337727, 0.96101538,\n        0.87323245, 1.02556183, 0.95388553, 0.97263382, 0.98591673,\n        0.96502309, 0.85746496, 0.84968585, 0.99422795, 0.89441428,\n        1.04297339, 1.05277335, 0.85709214, 0.87885518, 1.03047245,\n        1.05704007, 0.91158198, 0.91662192, 0.90469643, 0.96868723,\n        0.91674164, 0.93328151, 0.91403954, 1.22344839, 0.98301442,\n        0.97414525, 0.97886513, 0.91856841, 0.95869794, 0.92022874,\n        0.92707182, 0.92707182, 0.95625616, 0.93941143, 0.93244475,\n        0.93337728, 0.97659533, 0.97438746, 0.95742502, 0.94139699]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n      dtype='int64', name='draw', length=250))Attributes: (4)created_at :2024-04-13T05:36:20.439151+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 51kB\nDimensions:          (chain: 4, draw: 250)\nCoordinates:\n  * chain            (chain) int64 32B 0 1 2 3\n  * draw             (draw) int64 2kB 0 1 2 3 4 5 6 ... 244 245 246 247 248 249\nData variables:\n    acceptance_rate  (chain, draw) float64 8kB 0.8489 0.9674 ... 0.983 1.0\n    diverging        (chain, draw) bool 1kB False False False ... False False\n    energy           (chain, draw) float64 8kB 141.2 140.7 140.5 ... 141.9 141.4\n    lp               (chain, draw) float64 8kB -139.9 -140.0 ... -141.6 -140.3\n    n_steps          (chain, draw) int64 8kB 3 7 7 3 3 3 7 3 ... 3 3 3 3 3 3 3 3\n    step_size        (chain, draw) float64 8kB 0.8923 0.8923 ... 0.9726 0.9726\n    tree_depth       (chain, draw) int64 8kB 2 3 3 2 2 2 3 2 ... 2 2 2 2 2 2 2 2\nAttributes:\n    created_at:                  2024-04-13T05:36:20.441267+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 4draw: 250Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 5 ... 245 246 247 248 249array([  0,   1,   2, ..., 247, 248, 249])Data variables: (7)acceptance_rate(chain, draw)float640.8489 0.9674 0.9733 ... 0.983 1.0array([[0.84888924, 0.9673831 , 0.9733441 , 0.89329671, 0.86130558,\n        0.70132658, 0.96381014, 1.        , 0.92972425, 0.99393407,\n        0.42991297, 0.91174018, 0.87340076, 1.        , 0.9589841 ,\n        0.87037497, 1.        , 1.        , 0.99496101, 0.88726023,\n        0.9885737 , 0.90697071, 0.99084364, 0.80724362, 0.96962189,\n        0.9833609 , 0.97786349, 0.86134776, 0.90956489, 0.90854708,\n        0.98462356, 0.89669834, 0.90547798, 0.98699424, 0.97484471,\n        1.        , 0.88021684, 0.97615242, 0.96073465, 0.91237892,\n        0.9845141 , 0.94372409, 0.92777163, 0.97342742, 0.95057722,\n        1.        , 0.9612599 , 0.98843436, 1.        , 0.8602412 ,\n        0.99473406, 0.82449416, 0.87100299, 0.89980582, 0.91056175,\n        1.        , 0.9853551 , 0.6339886 , 0.79539193, 0.79552437,\n        0.98590884, 0.7882424 , 0.95636624, 1.        , 0.91644093,\n        0.61071389, 0.9079233 , 1.        , 0.94421384, 0.89397927,\n        0.97441365, 0.74609534, 0.87623779, 0.65668758, 1.        ,\n        0.99754243, 1.        , 1.        , 0.91660863, 0.99472955,\n        0.7589626 , 1.        , 0.79027932, 0.96149457, 0.86777121,\n        0.98246178, 0.78602939, 0.98883522, 0.99371046, 0.87781383,\n        0.90548202, 0.91736797, 0.93188359, 1.        , 0.56722994,\n        0.92168357, 1.        , 0.99974158, 0.85855488, 0.9130902 ,\n...\n        0.933183  , 0.9759323 , 0.80154439, 0.58682022, 0.89919977,\n        0.96192085, 1.        , 1.        , 1.        , 0.92846989,\n        0.73777125, 0.78583409, 0.84084899, 0.99937813, 0.68385273,\n        0.90734862, 0.87463518, 0.86085163, 0.96549202, 1.        ,\n        0.73037943, 0.94028211, 1.        , 0.82836097, 0.98809743,\n        0.81764963, 1.        , 1.        , 0.98798519, 0.98349015,\n        0.67753562, 0.95280466, 0.90283301, 0.93024677, 0.93151669,\n        1.        , 0.96818253, 0.99781367, 0.91713181, 0.96014857,\n        1.        , 0.75023019, 0.96045113, 0.94362692, 0.7532761 ,\n        0.83880141, 0.87182051, 0.74171229, 0.94450692, 1.        ,\n        0.94232357, 0.86828235, 0.92197311, 1.        , 0.68644282,\n        0.97359373, 0.90638899, 1.        , 0.76594222, 0.91762793,\n        0.95915589, 0.87177042, 1.        , 0.82258746, 0.81200426,\n        1.        , 0.93555039, 0.8848828 , 1.        , 0.87053966,\n        0.85639141, 0.97637026, 0.98528981, 1.        , 0.85350081,\n        0.95414825, 1.        , 1.        , 0.98446748, 0.80153679,\n        1.        , 0.62828104, 1.        , 0.36846189, 0.86386322,\n        0.64099386, 1.        , 0.84170202, 0.95361984, 0.9205592 ,\n        0.98298569, 0.66323733, 0.85359733, 0.98170805, 0.99093396,\n        0.97552769, 0.95590249, 0.84734008, 0.98296291, 1.        ]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n...\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False]])energy(chain, draw)float64141.2 140.7 140.5 ... 141.9 141.4array([[141.20161744, 140.70598666, 140.49148926, 141.20239103,\n        141.19010811, 144.18660929, 140.33281704, 139.96308322,\n        140.57893835, 141.38941515, 148.64059094, 142.93289825,\n        146.04299163, 144.14605027, 142.67642036, 144.14576568,\n        141.7660787 , 141.06947956, 140.19356762, 141.08246659,\n        141.00591894, 141.35859185, 139.93979496, 141.51824106,\n        142.56997862, 142.56991682, 142.58758086, 141.72338033,\n        142.8969296 , 142.42932381, 141.68924271, 143.93789916,\n        143.12463584, 142.28934616, 141.66880823, 140.75972524,\n        141.01378042, 140.13124997, 140.51835247, 141.61498126,\n        141.31832015, 140.66043205, 142.12540545, 141.64700326,\n        142.85807033, 141.70816164, 140.81111539, 140.31405788,\n        140.29502047, 142.17134901, 141.34942234, 141.96753663,\n        144.45306366, 144.30037464, 144.92560947, 145.88202322,\n        145.57638211, 149.53691678, 146.82447002, 143.92894787,\n        142.945291  , 143.62675737, 140.2703418 , 140.10974452,\n        140.59959866, 143.312126  , 143.4798215 , 142.32917667,\n        141.59167508, 140.91590845, 141.55742637, 142.42319463,\n        142.17191796, 145.8462414 , 143.50482308, 143.38838779,\n        140.93972277, 140.42443743, 141.12402307, 141.13791   ,\n...\n        141.93825369, 144.56393892, 142.73608027, 144.07625796,\n        142.0531902 , 141.41700837, 141.51465137, 142.62280631,\n        143.79817982, 142.76116047, 143.85988869, 142.11378708,\n        141.76697822, 141.60731201, 141.3540055 , 139.81189634,\n        140.0125599 , 140.24700481, 139.84972186, 141.52789778,\n        141.49476174, 140.70968672, 141.66856485, 142.71296198,\n        143.85619075, 145.52274885, 149.21963527, 147.57321087,\n        140.39448216, 141.853325  , 143.43444642, 141.78434647,\n        145.09952198, 142.36990061, 143.10356634, 140.37811064,\n        141.38857059, 141.31196017, 140.94430523, 142.22846388,\n        141.29718806, 141.2077227 , 142.47077178, 141.54519327,\n        140.3278173 , 140.95240769, 140.60694954, 140.58587491,\n        140.9740406 , 141.73032551, 142.70897439, 141.88098807,\n        141.65131353, 142.52300291, 143.27844069, 141.46619993,\n        140.87239229, 141.57738767, 140.5790518 , 143.21027453,\n        141.86795416, 146.14898606, 147.39883864, 145.13195593,\n        141.11515884, 141.95224649, 140.49781241, 140.32029504,\n        139.45421819, 142.18814629, 140.68012531, 139.92677613,\n        139.90305713, 139.92528181, 140.02423793, 141.83956437,\n        141.90308307, 141.4326794 ]])lp(chain, draw)float64-139.9 -140.0 ... -141.6 -140.3array([[-139.87893778, -139.9645816 , -140.15023671, -139.72915805,\n        -140.95230947, -139.75107499, -140.01457839, -139.4408409 ,\n        -140.52869623, -140.55137532, -140.55137532, -142.39342002,\n        -143.70774595, -140.91220502, -142.25708851, -140.87749392,\n        -140.94220804, -139.82075105, -139.84129544, -140.14606113,\n        -140.18660279, -139.81366331, -139.4446184 , -141.12989093,\n        -141.18051022, -142.24699337, -139.55294872, -141.34902122,\n        -141.1207297 , -140.68010125, -141.41084446, -141.82800678,\n        -141.20536354, -140.59228081, -140.69416166, -139.86811985,\n        -139.72750674, -139.60666827, -140.26256251, -141.00322146,\n        -139.77866637, -140.37414036, -141.03275542, -141.08023336,\n        -141.6000328 , -140.14892777, -139.96123244, -140.06522244,\n        -139.78981573, -141.01735455, -139.75775472, -141.22535186,\n        -142.38556034, -141.97659086, -144.27384526, -142.67574989,\n        -142.35722033, -143.75557052, -139.98097631, -142.70952977,\n        -140.7217984 , -139.6410683 , -139.84543799, -139.64193129,\n        -139.6247126 , -140.62462715, -142.18994991, -140.89554653,\n        -139.30901481, -140.66999757, -140.09430283, -140.18511655,\n        -141.47845726, -143.08700779, -142.63691111, -140.3748972 ,\n        -140.30603842, -139.8354723 , -140.91681837, -140.8016408 ,\n...\n        -141.61328651, -142.16610216, -140.86932964, -141.66349492,\n        -141.24899794, -140.92756748, -141.35783924, -140.46980857,\n        -141.82709759, -141.4470513 , -141.67479038, -140.53378286,\n        -141.14396647, -140.86380423, -139.8673658 , -139.34853841,\n        -139.70693196, -139.74296053, -139.57680571, -140.83672532,\n        -140.07834751, -139.67448254, -140.52919726, -141.42318554,\n        -141.21657892, -144.92174257, -145.62381945, -139.76482799,\n        -140.08610874, -141.02022716, -141.7757176 , -141.0336296 ,\n        -141.39137894, -141.95233948, -140.33931036, -139.38387579,\n        -140.13741269, -140.30673995, -140.39845601, -141.29817581,\n        -139.47208887, -140.20293725, -141.72161449, -139.43675991,\n        -140.300563  , -140.72081646, -139.7054349 , -139.82631079,\n        -140.69282006, -141.25803608, -141.07708316, -140.04546292,\n        -141.29624749, -142.45655409, -141.21055678, -140.6477156 ,\n        -139.8087491 , -140.65559625, -139.34928981, -142.04465792,\n        -139.35271669, -145.90075   , -140.47978836, -141.11111359,\n        -140.53840869, -139.8545253 , -139.57336607, -139.29539907,\n        -139.39810176, -139.39810176, -139.70660711, -139.62054515,\n        -139.56840864, -139.5697715 , -139.73762165, -141.38313388,\n        -141.58458337, -140.34263295]])n_steps(chain, draw)int643 7 7 3 3 3 7 3 ... 3 3 3 3 3 3 3 3array([[ 3,  7,  7,  3,  3,  3,  7,  3,  3, 11,  3,  3,  7,  3,  7,  7,\n         3,  3,  7,  5,  7,  7,  3,  3,  7,  7,  7,  7,  3,  7,  7,  7,\n         7,  3,  7,  3,  3,  3,  3,  7,  7,  3,  3,  7,  7,  3,  7,  3,\n         3,  7,  3,  3,  7,  3,  3,  3,  7,  3,  3,  3,  7,  3,  3,  3,\n         3,  7,  7,  3,  3,  7,  7,  3,  3,  3,  3,  7,  3,  7,  3,  3,\n         3,  1,  7,  7,  3,  3,  7,  3,  3,  7,  3,  3,  3,  3,  3,  3,\n         3,  3,  3,  7,  7,  7,  3,  3,  3,  7,  7,  7,  7,  3,  5,  3,\n         1,  7,  7,  3,  3,  7,  3,  3, 15,  3,  7,  3,  3,  3,  3,  3,\n         3,  3,  7,  7,  7,  3,  3,  7,  7,  3,  3,  3,  7,  7,  3,  3,\n         3,  3,  3,  3,  3,  7,  7,  7,  3,  3,  7,  3,  5,  3,  3,  3,\n         3,  3,  3,  1,  3,  3,  3,  7,  1,  3,  7,  7,  3,  3,  3,  3,\n         7,  3,  1,  3,  1,  3,  3,  7,  3,  3, 15,  3,  3,  7,  3,  3,\n         3,  3,  7,  3,  3,  3,  3, 15,  3,  7,  1,  3,  1,  3,  3,  7,\n         3,  3,  7,  3,  3,  7,  7,  7,  7,  3,  3,  7,  7,  7,  3,  7,\n         3,  7,  3,  3,  7,  3,  3,  3,  7,  3,  3,  7,  3,  3,  7,  3,\n         7,  3,  3,  7,  7,  3,  3,  3,  7,  3],\n       [ 3,  7,  3,  3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n         3,  3,  7,  3,  3, 11,  3,  3,  3,  3,  3,  7,  3,  3,  3,  1,\n         3,  7,  3,  3, 11,  7,  3,  3,  7,  3,  3, 19,  3,  3,  3,  3,\n         3,  3, 11,  3,  3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3,  3,\n...\n         3,  3,  7,  3,  3,  7,  7,  3,  3,  3,  7,  3,  7,  3,  7,  3,\n         3,  3,  3,  3,  3,  7,  7,  3,  3, 19,  3,  3,  3,  3,  7,  3,\n         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  7,  3,  3,  3,\n         3,  3,  3,  3, 11,  3,  3,  3, 11,  3],\n       [ 3,  3,  3,  3, 11,  7,  3,  3,  3, 15,  3,  3,  3, 11,  3,  3,\n         3,  7,  3,  3,  3,  3,  3,  3,  3, 11,  7,  3,  3,  3,  3,  3,\n         3,  3,  3,  3,  3,  7,  3,  3,  3,  3,  7,  3,  1,  3,  3,  3,\n        11,  3,  3,  3,  3,  3,  3,  3,  3,  1,  3,  3,  3,  3,  3,  7,\n         7,  3,  7,  7,  3,  3,  3, 11,  3,  3,  3,  3,  3,  3,  7,  7,\n         3,  7,  3,  3,  3,  3,  7,  3,  7,  3,  7,  3,  3,  3, 11,  3,\n         3,  3, 15,  3,  3,  7,  3,  3,  3,  3,  3,  3,  1,  7,  3,  3,\n         3,  3,  1,  3,  3,  3,  7,  3,  3,  3,  3,  3,  7,  7,  3, 15,\n         3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3, 11,  3,  3,  7,  3,\n         3,  7,  3,  3,  1,  3,  3,  3,  3,  7,  7,  3,  1,  3,  1,  3,\n         3,  3,  3,  7,  3,  3,  3,  3,  3,  3,  3,  7,  7,  3,  3,  3,\n         3,  3,  3,  3,  3,  3, 11,  3,  3,  3,  7,  7,  3,  3,  3,  3,\n         3,  3, 11,  3,  3,  3,  3,  3,  3, 11,  3,  1,  3,  7,  3,  3,\n         3,  7,  3,  3,  3,  7,  3,  3,  3,  1,  3,  3,  3,  3,  3,  3,\n         7,  3,  7,  3,  7,  3,  3,  3,  3,  3,  3,  3,  1, 11,  3,  3,\n         3,  3,  3,  3,  3,  3,  3,  3,  3,  3]])step_size(chain, draw)float640.8923 0.8923 ... 0.9726 0.9726array([[0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n        0.89234976, 0.89234976, 0.89234976, 0.89234976, 0.89234976,\n...\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ,\n        0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 , 0.9726011 ]])tree_depth(chain, draw)int642 3 3 2 2 2 3 2 ... 2 2 2 2 2 2 2 2array([[2, 3, 3, 2, 2, 2, 3, 2, 2, 4, 2, 2, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3,\n        2, 2, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 3, 2, 2, 2, 2, 3, 3, 2, 2, 3,\n        3, 2, 3, 2, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 2, 3,\n        3, 2, 2, 3, 3, 2, 2, 2, 2, 3, 2, 3, 2, 2, 2, 1, 3, 3, 2, 2, 3, 2,\n        2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 3, 3, 3, 2,\n        3, 2, 1, 3, 3, 2, 2, 3, 2, 2, 4, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3,\n        3, 2, 2, 3, 3, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 2, 2,\n        3, 2, 3, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 3, 1, 2, 3, 3, 2, 2, 2, 2,\n        3, 2, 1, 2, 1, 2, 2, 3, 2, 2, 4, 2, 2, 3, 2, 2, 2, 2, 3, 2, 2, 2,\n        2, 4, 2, 3, 1, 2, 1, 2, 2, 3, 2, 2, 3, 2, 2, 3, 3, 3, 3, 2, 2, 3,\n        3, 3, 2, 3, 2, 3, 2, 2, 3, 2, 2, 2, 3, 2, 2, 3, 2, 2, 3, 2, 3, 2,\n        2, 3, 3, 2, 2, 2, 3, 2],\n       [2, 3, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 4,\n        2, 2, 2, 2, 2, 3, 2, 2, 2, 1, 2, 3, 2, 2, 4, 3, 2, 2, 3, 2, 2, 5,\n        2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3,\n        2, 3, 2, 1, 2, 2, 2, 2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 2, 1,\n        2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 3,\n        2, 3, 4, 2, 2, 1, 2, 4, 2, 2, 2, 2, 2, 2, 3, 2, 4, 2, 2, 3, 2, 2,\n        3, 3, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 3,\n        2, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 3,\n...\n        2, 2, 4, 2, 3, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 3, 2,\n        2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3,\n        2, 3, 2, 3, 2, 1, 3, 2, 2, 3, 2, 3, 3, 3, 2, 2, 2, 2, 2, 3, 2, 2,\n        2, 3, 2, 3, 2, 2, 2, 4, 2, 1, 2, 2, 2, 3, 4, 2, 2, 4, 2, 2, 2, 2,\n        2, 2, 1, 3, 1, 2, 3, 2, 2, 2, 2, 2, 2, 3, 2, 1, 2, 2, 3, 2, 2, 3,\n        3, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 5, 2, 2,\n        2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2,\n        2, 2, 4, 2, 2, 2, 4, 2],\n       [2, 2, 2, 2, 4, 3, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 2, 3, 2, 2, 2, 2,\n        2, 2, 2, 4, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 2, 2, 2, 2, 3, 2,\n        1, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 3, 3, 2,\n        3, 3, 2, 2, 2, 4, 2, 2, 2, 2, 2, 2, 3, 3, 2, 3, 2, 2, 2, 2, 3, 2,\n        3, 2, 3, 2, 2, 2, 4, 2, 2, 2, 4, 2, 2, 3, 2, 2, 2, 2, 2, 2, 1, 3,\n        2, 2, 2, 2, 1, 2, 2, 2, 3, 2, 2, 2, 2, 2, 3, 3, 2, 4, 2, 2, 2, 3,\n        2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 3, 2, 2, 3, 2, 2, 1, 2, 2, 2, 2, 3,\n        3, 2, 1, 2, 1, 2, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 3, 3, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 4, 2, 2, 2, 3, 3, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2,\n        2, 2, 2, 4, 2, 1, 2, 3, 2, 2, 2, 3, 2, 2, 2, 3, 2, 2, 2, 1, 2, 2,\n        2, 2, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 2, 2, 1, 4, 2, 2, 2, 2,\n        2, 2, 2, 2, 2, 2, 2, 2]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       240, 241, 242, 243, 244, 245, 246, 247, 248, 249],\n      dtype='int64', name='draw', length=250))Attributes: (4)created_at :2024-04-13T05:36:20.441267+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\ntfp_nuts_idata = model.fit(inference_method=\"tfp_nuts\")\ntfp_nuts_idata\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 200kB\nDimensions:    (chain: 8, draw: 1000)\nCoordinates:\n  * chain      (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw       (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    Intercept  (chain, draw) float64 64kB -0.1597 0.2011 ... 0.1525 -0.171\n    x          (chain, draw) float64 64kB 0.2515 0.4686 0.4884 ... 0.5085 0.4896\n    y_sigma    (chain, draw) float64 64kB 0.9735 0.8969 0.8002 ... 0.9422 1.045\nAttributes:\n    created_at:                  2024-04-13T05:36:30.303342+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (3)Intercept(chain, draw)float64-0.1597 0.2011 ... 0.1525 -0.171array([[-0.15967084,  0.20113676,  0.01971475, ...,  0.00794969,\n         0.0279562 ,  0.03472592],\n       [-0.06799971,  0.14593555, -0.00604452, ...,  0.00924657,\n        -0.10235794, -0.10236953],\n       [ 0.043034  ,  0.08600223,  0.16562574, ...,  0.05851938,\n         0.00720315,  0.08258778],\n       ...,\n       [ 0.04807806,  0.11227424, -0.3172604 , ...,  0.02980962,\n        -0.13681545,  0.19177451],\n       [ 0.04374417, -0.0054294 ,  0.09305579, ...,  0.0232273 ,\n        -0.04073809,  0.025925  ],\n       [-0.07370367, -0.00152223,  0.06769584, ..., -0.09818811,\n         0.15246738, -0.17104419]])x(chain, draw)float640.2515 0.4686 ... 0.5085 0.4896array([[0.25153111, 0.4685625 , 0.48837809, ..., 0.28573626, 0.407775  ,\n        0.38347135],\n       [0.28165967, 0.36310827, 0.41225084, ..., 0.24255857, 0.45039439,\n        0.4954714 ],\n       [0.5386156 , 0.6228231 , 0.25313292, ..., 0.44280376, 0.4488854 ,\n        0.25456354],\n       ...,\n       [0.45168195, 0.46344655, 0.17750331, ..., 0.30371223, 0.29536054,\n        0.40431303],\n       [0.41455145, 0.43166272, 0.35213661, ..., 0.36384472, 0.3917272 ,\n        0.34092006],\n       [0.20620881, 0.51263399, 0.44056489, ..., 0.25237815, 0.50845624,\n        0.48960883]])y_sigma(chain, draw)float640.9735 0.8969 ... 0.9422 1.045array([[0.97352428, 0.89691108, 0.80020873, ..., 1.03087931, 0.84944049,\n        0.84158909],\n       [0.96226504, 0.92778234, 0.77909925, ..., 0.91397532, 1.00185137,\n        0.9513834 ],\n       [1.0042728 , 0.97580931, 0.94890477, ..., 0.92691038, 0.885916  ,\n        1.01934012],\n       ...,\n       [0.88671137, 0.91944589, 1.00541185, ..., 0.96151472, 0.93478611,\n        0.94631027],\n       [0.8367989 , 0.84727656, 1.05992876, ..., 0.91519111, 0.90516942,\n        0.9358838 ],\n       [0.94127918, 0.89667586, 0.91173519, ..., 0.96184559, 0.94224608,\n        1.04527058]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-13T05:36:30.303342+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 312kB\nDimensions:          (chain: 8, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    accept_ratio     (chain, draw) float64 64kB 0.8971 0.9944 ... 0.9174 0.8133\n    diverging        (chain, draw) bool 8kB False False False ... False False\n    is_accepted      (chain, draw) bool 8kB True True True ... True True True\n    n_steps          (chain, draw) int32 32kB 7 7 7 1 7 7 7 3 ... 3 7 7 7 3 7 7\n    step_size        (chain, draw) float64 64kB 0.534 0.534 0.534 ... nan nan\n    target_log_prob  (chain, draw) float64 64kB -141.5 -141.7 ... -141.0 -143.2\n    tune             (chain, draw) float64 64kB 0.0 0.0 0.0 0.0 ... nan nan nan\nAttributes:\n    created_at:                  2024-04-13T05:36:30.304788+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)accept_ratio(chain, draw)float640.8971 0.9944 ... 0.9174 0.8133array([[0.89706765, 0.99441475, 0.80397664, ..., 0.99407977, 1.        ,\n        0.73958291],\n       [0.99821982, 0.95159754, 0.77731848, ..., 0.98139297, 0.91789348,\n        0.96456953],\n       [0.76824526, 0.92239538, 1.        , ..., 0.94414437, 0.91605876,\n        0.92334246],\n       ...,\n       [0.99710475, 0.99154725, 0.58953539, ..., 1.        , 0.92397302,\n        0.99338491],\n       [0.98669117, 0.98477039, 0.95831938, ..., 0.92092812, 0.96842841,\n        0.95013437],\n       [0.91842649, 0.75186373, 0.99689159, ..., 1.        , 0.9173519 ,\n        0.81331846]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])is_accepted(chain, draw)boolTrue True True ... True True Truearray([[ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       ...,\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True],\n       [ True,  True,  True, ...,  True,  True,  True]])n_steps(chain, draw)int327 7 7 1 7 7 7 3 ... 3 3 7 7 7 3 7 7array([[ 7,  7,  7, ...,  7,  7,  7],\n       [ 7,  7,  3, ...,  7,  7,  3],\n       [ 3,  3,  7, ...,  3, 15,  7],\n       ...,\n       [ 7,  7,  3, ...,  3,  7,  7],\n       [ 7,  7,  7, ...,  7,  3,  3],\n       [ 7,  7,  3, ...,  3,  7,  7]], dtype=int32)step_size(chain, draw)float640.534 0.534 0.534 ... nan nan nanarray([[0.53403598, 0.53403598, 0.53403598, ..., 0.53403598, 0.53403598,\n        0.53403598],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       ...,\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan],\n       [       nan,        nan,        nan, ...,        nan,        nan,\n               nan]])target_log_prob(chain, draw)float64-141.5 -141.7 ... -141.0 -143.2array([[-141.50324612, -141.68928918, -142.88857248, ..., -140.47032751,\n        -140.28212037, -140.3879345 ],\n       [-140.01375434, -140.13505676, -143.13641583, ..., -139.99147863,\n        -141.05078466, -141.26466845],\n       [-141.12400308, -142.52999266, -141.18044662, ..., -139.62740626,\n        -139.97278032, -140.75200575],\n       ...,\n       [-139.95465107, -140.14159326, -146.22556521, ..., -139.52421787,\n        -140.79404952, -140.86677285],\n       [-140.66929954, -140.59550579, -141.05595747, ..., -139.29239833,\n        -139.67236895, -139.28664301],\n       [-140.73991807, -140.71540356, -139.69662159, ..., -140.53100323,\n        -140.99477405, -143.18216599]])tune(chain, draw)float640.0 0.0 0.0 0.0 ... nan nan nan nanarray([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (4)created_at :2024-04-13T05:36:30.304788+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\nnumpyro_nuts_idata = model.fit(inference_method=\"numpyro_nuts\")\nnumpyro_nuts_idata\n\nsample: 100%|██████████| 1500/1500 [00:02&lt;00:00, 599.25it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 200kB\nDimensions:    (chain: 8, draw: 1000)\nCoordinates:\n  * chain      (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw       (draw) int64 8kB 0 1 2 3 4 5 6 7 ... 993 994 995 996 997 998 999\nData variables:\n    Intercept  (chain, draw) float64 64kB 0.0004764 0.02933 ... 0.1217 0.1668\n    x          (chain, draw) float64 64kB 0.3836 0.6556 0.2326 ... 0.48 0.5808\n    y_sigma    (chain, draw) float64 64kB 0.8821 0.9604 0.9652 ... 0.9063 0.9184\nAttributes:\n    created_at:                  2024-04-13T05:36:33.599519+00:00\n    arviz_version:               0.18.0\n    inference_library:           numpyro\n    inference_library_version:   0.14.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (3)Intercept(chain, draw)float640.0004764 0.02933 ... 0.1217 0.1668array([[ 0.00047641,  0.02933426, -0.19069113, ..., -0.04607573,\n         0.14408182, -0.07016404],\n       [-0.06542953,  0.09001091,  0.03811068, ...,  0.03170986,\n         0.20861147,  0.18706729],\n       [-0.08028089,  0.03625393,  0.05650287, ..., -0.0093195 ,\n         0.01912548,  0.00214345],\n       ...,\n       [ 0.18184083,  0.07906243,  0.06388914, ..., -0.07055763,\n         0.10986417,  0.09622923],\n       [-0.02521011,  0.15830259, -0.10214413, ...,  0.01471807,\n         0.10706226,  0.07562878],\n       [-0.02468806, -0.03414193, -0.06678234, ...,  0.08710519,\n         0.12166933,  0.16679929]])x(chain, draw)float640.3836 0.6556 ... 0.48 0.5808array([[0.38361084, 0.65556045, 0.23260059, ..., 0.65580692, 0.44095681,\n        0.22838517],\n       [0.30187358, 0.46285734, 0.31814527, ..., 0.38133365, 0.32358724,\n        0.37070791],\n       [0.44410357, 0.42831529, 0.3990648 , ..., 0.37993575, 0.40377358,\n        0.42804019],\n       ...,\n       [0.49080324, 0.20770949, 0.12142607, ..., 0.44054445, 0.38924394,\n        0.38167612],\n       [0.34590162, 0.30144285, 0.45780034, ..., 0.44424986, 0.52104263,\n        0.45543543],\n       [0.23738988, 0.68021684, 0.05589656, ..., 0.42147165, 0.48000601,\n        0.58081686]])y_sigma(chain, draw)float640.8821 0.9604 ... 0.9063 0.9184array([[0.88211276, 0.96036122, 0.96524442, ..., 0.94362502, 1.00228679,\n        0.88249142],\n       [0.93345676, 0.85184129, 1.07135935, ..., 0.92649839, 0.86831784,\n        0.92890112],\n       [0.973364  , 1.04138907, 0.96240687, ..., 0.9564475 , 1.0092212 ,\n        0.87607713],\n       ...,\n       [1.03355029, 0.98103228, 0.92902834, ..., 0.83197448, 0.99111854,\n        0.92967952],\n       [0.88101923, 1.0226885 , 0.87217557, ..., 0.94028186, 0.88687764,\n        0.85291778],\n       [0.98596365, 0.91083125, 0.9972831 , ..., 0.86419289, 0.90625839,\n        0.91841349]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (6)created_at :2024-04-13T05:36:33.599519+00:00arviz_version :0.18.0inference_library :numpyroinference_library_version :0.14.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 400kB\nDimensions:          (chain: 8, draw: 1000)\nCoordinates:\n  * chain            (chain) int64 64B 0 1 2 3 4 5 6 7\n  * draw             (draw) int64 8kB 0 1 2 3 4 5 6 ... 994 995 996 997 998 999\nData variables:\n    acceptance_rate  (chain, draw) float64 64kB 0.9366 0.3542 ... 0.9903 0.8838\n    diverging        (chain, draw) bool 8kB False False False ... False False\n    energy           (chain, draw) float64 64kB 140.3 145.4 ... 141.0 142.6\n    lp               (chain, draw) float64 64kB 139.6 143.3 ... 140.5 142.5\n    n_steps          (chain, draw) int64 64kB 3 3 7 3 7 1 3 3 ... 11 7 3 3 3 3 3\n    step_size        (chain, draw) float64 64kB 0.8891 0.8891 ... 0.7595 0.7595\n    tree_depth       (chain, draw) int64 64kB 2 2 3 2 3 1 2 2 ... 4 3 2 2 2 2 2\nAttributes:\n    created_at:                  2024-04-13T05:36:33.623197+00:00\n    arviz_version:               0.18.0\n    inference_library:           numpyro\n    inference_library_version:   0.14.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 8draw: 1000Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 7array([0, 1, 2, 3, 4, 5, 6, 7])draw(draw)int640 1 2 3 4 5 ... 995 996 997 998 999array([  0,   1,   2, ..., 997, 998, 999])Data variables: (7)acceptance_rate(chain, draw)float640.9366 0.3542 ... 0.9903 0.8838array([[0.93661577, 0.35419612, 0.99435023, ..., 0.59003267, 1.        ,\n        0.96452433],\n       [0.9974338 , 0.86250112, 0.95945138, ..., 0.78208773, 0.79906599,\n        1.        ],\n       [0.96468642, 0.97525962, 0.98495362, ..., 0.9775774 , 0.86156602,\n        0.89713276],\n       ...,\n       [0.96610793, 0.98086156, 0.89084022, ..., 0.95772457, 0.70497474,\n        0.99836264],\n       [0.99806445, 0.76652499, 0.98528715, ..., 0.87560309, 0.81183609,\n        1.        ],\n       [0.99527811, 0.68120359, 1.        , ..., 1.        , 0.99026377,\n        0.88375821]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       ...,\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])energy(chain, draw)float64140.3 145.4 144.9 ... 141.0 142.6array([[140.3467186 , 145.35611236, 144.90015851, ..., 144.55362515,\n        144.91727277, 142.02764348],\n       [141.22868211, 142.17975426, 142.8293087 , ..., 142.28333599,\n        142.31235728, 142.52446205],\n       [144.0720805 , 141.47372901, 140.93450225, ..., 140.18268471,\n        140.91505172, 141.58829808],\n       ...,\n       [142.62435102, 143.2286207 , 142.80972656, ..., 144.52562661,\n        145.55539646, 140.15311622],\n       [142.10722077, 142.2552196 , 142.37900379, ..., 140.54316684,\n        141.75774836, 141.58904973],\n       [140.82607551, 145.03403209, 145.2241475 , ..., 141.6051566 ,\n        140.9854462 , 142.60201809]])lp(chain, draw)float64139.6 143.3 142.2 ... 140.5 142.5array([[139.63264222, 143.32720395, 142.23861021, ..., 143.97346231,\n        140.67090827, 140.87592102],\n       [139.81127204, 140.89739965, 141.16918465, ..., 139.28792859,\n        142.23961923, 140.80131401],\n       [140.45054751, 140.62345763, 139.48418855, ..., 139.42120354,\n        139.97119948, 139.9471655 ],\n       ...,\n       [141.9099355 , 140.83346483, 142.30482492, ..., 141.78502364,\n        140.03250066, 139.57119158],\n       [139.72932132, 141.21433158, 141.46193109, ..., 139.62199619,\n        141.12185494, 140.69980318],\n       [140.29191412, 144.98589172, 143.74390375, ..., 140.2609778 ,\n        140.48172152, 142.47099219]])n_steps(chain, draw)int643 3 7 3 7 1 3 3 ... 11 7 3 3 3 3 3array([[3, 3, 7, ..., 3, 3, 7],\n       [7, 3, 7, ..., 3, 3, 7],\n       [3, 7, 3, ..., 3, 3, 3],\n       ...,\n       [7, 7, 7, ..., 3, 7, 7],\n       [3, 7, 7, ..., 3, 3, 7],\n       [7, 7, 3, ..., 3, 3, 3]])step_size(chain, draw)float640.8891 0.8891 ... 0.7595 0.7595array([[0.8891145 , 0.8891145 , 0.8891145 , ..., 0.8891145 , 0.8891145 ,\n        0.8891145 ],\n       [0.70896111, 0.70896111, 0.70896111, ..., 0.70896111, 0.70896111,\n        0.70896111],\n       [0.8087902 , 0.8087902 , 0.8087902 , ..., 0.8087902 , 0.8087902 ,\n        0.8087902 ],\n       ...,\n       [0.69745418, 0.69745418, 0.69745418, ..., 0.69745418, 0.69745418,\n        0.69745418],\n       [0.88034552, 0.88034552, 0.88034552, ..., 0.88034552, 0.88034552,\n        0.88034552],\n       [0.7595237 , 0.7595237 , 0.7595237 , ..., 0.7595237 , 0.7595237 ,\n        0.7595237 ]])tree_depth(chain, draw)int642 2 3 2 3 1 2 2 ... 2 4 3 2 2 2 2 2array([[2, 2, 3, ..., 2, 2, 3],\n       [3, 2, 3, ..., 2, 2, 3],\n       [2, 3, 2, ..., 2, 2, 2],\n       ...,\n       [3, 3, 3, ..., 2, 3, 3],\n       [2, 3, 3, ..., 2, 2, 3],\n       [3, 3, 2, ..., 2, 2, 2]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       990, 991, 992, 993, 994, 995, 996, 997, 998, 999],\n      dtype='int64', name='draw', length=1000))Attributes: (6)created_at :2024-04-13T05:36:33.623197+00:00arviz_version :0.18.0inference_library :numpyroinference_library_version :0.14.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413\n                      \n                  \n            \n            \n              \n            \n            \n\n\n\n\n\n\nflowmc_idata = model.fit(inference_method=\"flowmc_realnvp_hmc\")\nflowmc_idata\n\nNo autotune found, use input sampler_params\nTraining normalizing flow\n\n\nTuning global sampler: 100%|██████████| 5/5 [00:51&lt;00:00, 10.37s/it]\n\n\nStarting Production run\n\n\nProduction run: 100%|██████████| 5/5 [00:00&lt;00:00, 14.38it/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 244kB\nDimensions:    (chain: 20, draw: 500)\nCoordinates:\n  * chain      (chain) int64 160B 0 1 2 3 4 5 6 7 8 ... 12 13 14 15 16 17 18 19\n  * draw       (draw) int64 4kB 0 1 2 3 4 5 6 7 ... 493 494 495 496 497 498 499\nData variables:\n    Intercept  (chain, draw) float64 80kB -0.07404 -0.07404 ... -0.1455 0.09545\n    x          (chain, draw) float64 80kB 0.4401 0.4401 0.3533 ... 0.6115 0.3824\n    y_sigma    (chain, draw) float64 80kB 0.9181 0.9181 0.9732 ... 1.049 0.9643\nAttributes:\n    created_at:                  2024-04-13T05:37:29.798250+00:00\n    arviz_version:               0.18.0\n    modeling_interface:          bambi\n    modeling_interface_version:  0.13.1.dev25+g1e7f677e.d20240413xarray.DatasetDimensions:chain: 20draw: 500Coordinates: (2)chain(chain)int640 1 2 3 4 5 6 ... 14 15 16 17 18 19array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19])draw(draw)int640 1 2 3 4 5 ... 495 496 497 498 499array([  0,   1,   2, ..., 497, 498, 499])Data variables: (3)Intercept(chain, draw)float64-0.07404 -0.07404 ... 0.09545array([[-0.07403956, -0.07403956, -0.08175103, ..., -0.07505013,\n        -0.06280065, -0.06280065],\n       [ 0.15335447,  0.15335447,  0.12713003, ..., -0.02037623,\n         0.06826204,  0.01933312],\n       [ 0.00658099,  0.00658099,  0.00658099, ..., -0.04271278,\n        -0.04271278, -0.09780863],\n       ...,\n       [ 0.00629487,  0.01048304, -0.03193874, ...,  0.13237167,\n         0.08595727,  0.01442809],\n       [ 0.05972149,  0.02490161, -0.00084261, ...,  0.06751994,\n        -0.15926318, -0.15926318],\n       [ 0.23012418,  0.25630661,  0.23839857, ...,  0.07975465,\n        -0.14554836,  0.09545347]])x(chain, draw)float640.4401 0.4401 ... 0.6115 0.3824array([[0.44013865, 0.44013865, 0.35326474, ..., 0.30371128, 0.28793687,\n        0.28793687],\n       [0.45569737, 0.45569737, 0.55350522, ..., 0.37498493, 0.45850535,\n        0.40671648],\n       [0.24971734, 0.24971734, 0.24971734, ..., 0.19912158, 0.19912158,\n        0.46992411],\n       ...,\n       [0.44071041, 0.47684243, 0.35786393, ..., 0.37932871, 0.31101246,\n        0.25090813],\n       [0.43305649, 0.19703032, 0.21622992, ..., 0.39021766, 0.35161734,\n        0.35161734],\n       [0.52832789, 0.50016524, 0.19504762, ..., 0.25411208, 0.61146903,\n        0.38243421]])y_sigma(chain, draw)float640.9181 0.9181 ... 1.049 0.9643array([[0.91812597, 0.91812597, 0.97317218, ..., 0.87193011, 0.98202548,\n        0.98202548],\n       [0.92619283, 0.92619283, 0.89113835, ..., 1.00239178, 0.93585383,\n        0.93328517],\n       [0.96032594, 0.96032594, 0.96032594, ..., 0.90617649, 0.90617649,\n        0.95241728],\n       ...,\n       [1.01337917, 0.96203307, 0.81645174, ..., 1.00979845, 1.07249345,\n        0.9165658 ],\n       [0.97087149, 0.91876884, 0.87129204, ..., 1.09021385, 1.0093326 ,\n        1.0093326 ],\n       [0.89533883, 0.91515164, 1.07248889, ..., 0.95594426, 1.04908995,\n        0.96426064]])Indexes: (2)chainPandasIndexPandasIndex(Index([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], dtype='int64', name='chain'))drawPandasIndexPandasIndex(Index([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,\n       ...\n       490, 491, 492, 493, 494, 495, 496, 497, 498, 499],\n      dtype='int64', name='draw', length=500))Attributes: (4)created_at :2024-04-13T05:37:29.798250+00:00arviz_version :0.18.0modeling_interface :bambimodeling_interface_version :0.13.1.dev25+g1e7f677e.d20240413"
  },
  {
    "objectID": "posts/2024-03-29-alternative_samplers.html#sampler-comparisons",
    "href": "posts/2024-03-29-alternative_samplers.html#sampler-comparisons",
    "title": "Using Alternative Samplers in Bambi",
    "section": "",
    "text": "With ArviZ, we can compare the inference result summaries of the samplers. Note: We can’t use az.compare as not each inference data object returns the pointwise log-probabilities. Thus, an error would be raised.\n\naz.summary(blackjax_nuts_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.023\n0.097\n-0.141\n0.209\n0.004\n0.003\n694.0\n508.0\n1.00\n\n\nx\n0.356\n0.111\n0.162\n0.571\n0.004\n0.003\n970.0\n675.0\n1.00\n\n\ny_sigma\n0.950\n0.069\n0.827\n1.072\n0.002\n0.001\n1418.0\n842.0\n1.01\n\n\n\n\n\n\n\n\naz.summary(tfp_nuts_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.023\n0.097\n-0.157\n0.205\n0.001\n0.001\n6785.0\n5740.0\n1.0\n\n\nx\n0.360\n0.105\n0.169\n0.563\n0.001\n0.001\n6988.0\n5116.0\n1.0\n\n\ny_sigma\n0.946\n0.067\n0.831\n1.081\n0.001\n0.001\n7476.0\n5971.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(numpyro_nuts_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.024\n0.095\n-0.162\n0.195\n0.001\n0.001\n6851.0\n5614.0\n1.0\n\n\nx\n0.362\n0.104\n0.176\n0.557\n0.001\n0.001\n9241.0\n6340.0\n1.0\n\n\ny_sigma\n0.946\n0.068\n0.826\n1.079\n0.001\n0.001\n7247.0\n5711.0\n1.0\n\n\n\n\n\n\n\n\naz.summary(flowmc_idata)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n0.015\n0.100\n-0.186\n0.190\n0.004\n0.003\n758.0\n1233.0\n1.02\n\n\nx\n0.361\n0.105\n0.174\n0.565\n0.001\n0.001\n5084.0\n4525.0\n1.00\n\n\ny_sigma\n0.951\n0.070\n0.823\n1.079\n0.001\n0.001\n5536.0\n5080.0\n1.00"
  },
  {
    "objectID": "posts/2024-03-29-alternative_samplers.html#summary",
    "href": "posts/2024-03-29-alternative_samplers.html#summary",
    "title": "Using Alternative Samplers in Bambi",
    "section": "",
    "text": "Thanks to bayeux, we can use three different sampling backends and 10+ alternative MCMC methods in Bambi. Using these methods is as simple as passing the inference name to the inference_method of the fit method.\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Sat Apr 13 2024\n\nPython implementation: CPython\nPython version       : 3.12.2\nIPython version      : 8.20.0\n\nbambi : 0.13.1.dev25+g1e7f677e.d20240413\npandas: 2.2.1\nnumpy : 1.26.4\nbayeux: 0.1.10\narviz : 0.18.0\n\nWatermark: 2.4.3"
  },
  {
    "objectID": "posts/2022-10-08-inference-mh.html",
    "href": "posts/2022-10-08-inference-mh.html",
    "title": "Inference - Metropolis Hastings from Scratch",
    "section": "",
    "text": "Main Idea\nMetropolis-Hastings (MH) is one of the simplest kinds of MCMC algorithms. The idea with MH is that at each step, we propose to move from the current state \\(x\\) to a new state \\(x'\\) with probability \\(q(x'|x)\\), where \\(q\\) is the proposal distribution. The user is free to choose the proposal distribution and the choice of the proposal is dependent on the form of the target distribution. Once a proposal has been made to move to \\(x'\\), we then decide whether to accept or reject the proposal according to some rule. If the proposal is accepted, the new state is \\(x'\\), else the new state is the same as the current state \\(x\\).\nProposals can be symmetric and asymmetric. In the case of symmetric proposals \\(q(x'|x) = q(x|x')\\), the acceptance probability is given by the rule:\n\\[A = min(1, \\frac{p^*(x')}{p^*(x)})\\]\nThe fraction is a ratio between the probabilities of the proposed state \\(x'\\) and current state \\(x\\). If \\(x'\\) is more probable than \\(x\\), the ratio is \\(&gt; 1\\), and we move to the proposed state. However, if \\(x'\\) is less probable, we may still move there, depending on the relative probabilities. If the relative probabilities are similar, we may code exploration into the algorithm such that they go in the opposite direction. This helps with the greediness of the original algorithm—only moving to more probable states.\n\n\nThe Algorithm\n\nInitialize \\(x^0\\)\nfor \\(s = 0, 1, 2, 3, ...\\) do:\n\nDefine \\(x = x^s\\)\nSample \\(x' \\sim q(x'|x)\\) where \\(q\\) is the user’s proposal distribution\nCompute the acceptance probability given by:\n\\(p_a(x_{t+1}|x_i) = min(1, \\frac{p(x_{i+1})q(x_i | x_{i+1})}{p(x_i)q(x_{i+1}|x_I)})\\)\n\nCompute \\(A = min(1, \\alpha)\\)\nSample \\(u \\sim U(0, 1)\\)\nSet new sample to: \\(x^{s+1} =\n\\left\\{\n\\begin{array}{ll}\nx' & \\quad \\text{if} \\quad u \\leq A(\\text{accept}) \\\\\nx & \\quad \\text{if} \\quad x &gt; A(\\text{reject}) \\\\\n\\end{array}\n\\right.\\)\n\n\n\nRandom Walk Metropolis-Hastings\nThe random walk metropolis-hastings (RWMH) corresponds to MH with a Gaussian propsal distribution of the form:\n\\[q(x'|x) = \\mathcal{N}(x'|x, \\tau^2 I)\\]\nBelow, I implement the RWMH for sampling from a 1-dimenensional mixture of Gaussians (implemented using the MixtureSameFamily PyTorch class) with the following parameters:\n\n\\(\\mu = -20, 20\\)\nMixture component probability \\(= 0.3, 0.7\\)\n\\(\\sum = 10, 10\\)\n\n\n\nCode\ndef plot(distribution, trace_history, xmin, xmax, n_iterations, n_evals=500):\n\n    x_evals = torch.linspace(xmin, xmax, n_evals)\n    evals = torch.exp(distribution.log_prob(x_evals))\n    \n    fig = plt.figure(figsize=(12, 4))\n\n    ax = fig.add_subplot(1, 2, 1)\n    ax.plot(torch.arange(n_iterations), trace_history)\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Sampled Value')\n\n    ax = fig.add_subplot(1, 2, 2, projection='3d')\n    ax.plot(torch.arange(n_iterations), trace_history)\n    ax.plot(torch.zeros(n_evals), x_evals, evals)\n    ax.set_xlabel('Iterations')\n    ax.set_ylabel('Sampled Value')\n\n    fig.suptitle('Random Walk Metropolis-Hastings')\n    plt.show()\n\n\ndef true_distribution(mixture_probs, mus, scales):\n\n    return dist.MixtureSameFamily(\n        mixture_distribution=dist.Categorical(probs=mixture_probs),\n        component_distribution=dist.Normal(loc=mus, scale=scales)\n        )\n\n\n\ndef metropolis_hasting(x0, tau, mixture, n_iterations, rng_key=None):\n    \"\"\"\n    implements the random walk metropolis-hasting algorithm\n    \"\"\"\n\n    x_current = x0\n    x_samples = torch.zeros(n_iterations)\n    x_samples[0] = x_current\n    cnt_acceptance = 0\n     \n    for n in range(1, n_iterations):\n        \n        # datum of proposed state x'\n        x_candidate = x_current + tau * dist.Normal(loc=0, scale=1).sample()\n        # probs. of proposed state x'\n        p_candidate = torch.exp(mixture.log_prob(x_candidate))\n        # probs. of current state x\n        p_current = torch.exp(mixture.log_prob(x_current))\n        \n        # acceptance formula\n        alpha = p_candidate / p_current\n        probs_accept = min(1, alpha)\n        \n        # sample u ~ U(0, 1)\n        u = dist.Uniform(0, 1).sample()\n\n        if u &gt;= probs_accept:\n            x_current = x_current\n        else:\n            x_current = x_candidate\n            cnt_acceptance += 1\n\n        x_samples[n] = x_current\n\n    acceptence_ratio = cnt_acceptance / n_iterations\n    print('---- statistics ----')\n    print(f'acceptance rate = {acceptence_ratio}')\n\n    return x_samples\n\n\ndef main(args):\n\n    # initial parameter value\n    x0 = torch.tensor(20.)\n\n    # mixture dist. parameters\n    mixture_probs = torch.tensor([0.3, 0.7])\n    mus = torch.tensor([-20., 20.])\n    scales = torch.tensor([10., 10.])\n\n    n_iters = args.iters\n    tau = torch.tensor(args.tau)\n\n    mixture_distribution = true_distribution(mixture_probs, mus, scales)\n    x_samples = metropolis_hasting(x0, tau, mixture_distribution, n_iters)\n\n    plot(mixture_distribution, x_samples, -100, 100, n_iters)\n\n\nparser = argparse.ArgumentParser(description='rw-mh')\nparser.add_argument('--iters', type=int, default=1000)\nparser.add_argument('--tau', type=float, default=8.)\nargs = parser.parse_args(\"\")\n\nmain(args)\n\n---- statistics ----\nacceptance rate = 0.803\n\n\n\n\n\n\n\n\n\n\n\nResults\nThe mixture distribution can be tricky to sample from as it is has more than one model, i.e., it is a bimodal distribution. However, we can see that the RWMH spends time sampling from both component distributions, albeit, the distribution with the higher probability more. Due to the random search based perturbations (random walk), the sampler seems to randomly jump from component to component, showing that the chain is not sticky. Additionally, the acceptance rate is \\(0.803\\) indicating that about \\(80\\%\\) of new proposals were accepted."
  },
  {
    "objectID": "posts/2023-gsoc-update-slopes.html",
    "href": "posts/2023-gsoc-update-slopes.html",
    "title": "Google Summer of Code - Average Predictive Slopes",
    "section": "",
    "text": "It is currently the beginning of week ten of Google Summer of Code 2023. According to the original deliverables table outlined in my proposal, the goal was to have opened a draft PR for the basic functionality of the plot_slopes. Subsequently, week 11 was reserved to further develop the plot_slopes function, and to write tests and a notebook for the documentation, respectively.\nHowever, at the beginning of week ten, I have a PR open with the majority of the functionality that marginaleffects has for slopes. In addition, I also exposed the slopes function, added tests, and have a PR open for the documentation.\nBelow is the documentation for slopes and plot_slopes in Bambi, and is a culmination of the work completed in weeks five through nine."
  },
  {
    "objectID": "posts/2023-gsoc-update-slopes.html#interpretation-of-regression-coefficients",
    "href": "posts/2023-gsoc-update-slopes.html#interpretation-of-regression-coefficients",
    "title": "Google Summer of Code - Average Predictive Slopes",
    "section": "Interpretation of Regression Coefficients",
    "text": "Interpretation of Regression Coefficients\nAssuming we have fit a linear regression model of the form\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k x_k + \\epsilon\\]\nthe “safest” interpretation of the regression coefficients \\(\\beta\\) is as a comparison between two groups of items that differ by \\(1\\) in the relevant predictor variable \\(x_i\\) while being identical in all the other predictors. Formally, the predicted difference between two items \\(i\\) and \\(j\\) that differ by an amount \\(n\\) on predictor \\(k\\), but are identical on all other predictors, the predicted difference is \\(y_i - y_j\\) is \\(\\beta_kx\\), on average.\nHowever, once we move away from a regression model with a Gaussian response, the identity function, and no interaction terms, the interpretation of the coefficients are not as straightforward. For example, in a logistic regression model, the coefficients are on a different scale and are measured in logits (log odds), not probabilities or percentage points. Thus, you cannot interpret the coefficents as a “one unit increase in \\(x_k\\) is associated with an \\(n\\) percentage point decrease in \\(y\\)”. First, the logits must be converted to the probability scale. Secondly, a one unit change in \\(x_k\\) may produce a larger or smaller change in the outcome, depending upon how far away from zero the logits are.\nslopes and plot_slopes, by default, computes quantities of interest on the response scale for GLMs. For example, for a logistic regression model, this is the probability scale, and for a Poisson regression model, this is the count scale.\n\nInterpreting interaction effects\nSpecifying interactions in a regression model is a way of allowing parameters to be conditional on certain aspects of the data. By contrast, for a model with no interactions, the parameters are not conditional and thus, the value of one parameter is not dependent on the value of another covariate. However, once interactions exist, multiple parameters are always in play at the same time. Additionally, interactions can be specified for either categorical, continuous, or both types of covariates. Thus, making the interpretation of the parameters more difficult.\nWith GLMs, every covariate essentially interacts with itself because of the link function. To demonstrate parameters interacting with themselves, consider the mean of a Gaussian linear model with an identity link function\n\\[\\mu = \\alpha + \\beta x\\]\nwhere the rate of change in \\(\\mu\\) with respect to \\(x\\) is just \\(\\beta\\), i.e., the rate of change is constant no matter what the value of \\(x\\) is. But when we consider GLMs with link functions used to map outputs to exponential family distribution parameters, calculating the derivative of the mean output \\(\\mu\\) with respect to the predictor is not as straightforward as in the Gaussian linear model. For example, computing the rate of change in a binomial probability \\(p\\) with respect to \\(x\\)\n\\[p = \\frac{exp(\\alpha + \\beta x)}{1 + exp(\\alpha + \\beta x)}\\]\nAnd taking the derivative of \\(p\\) with respect to \\(x\\) yields\n\\[\\frac{\\partial p}{\\partial x} = \\frac{\\beta}{2(1 + cosh(\\alpha + \\beta x))}\\]\nSince \\(x\\) appears in the derivative, the impact of a change in \\(x\\) depends upon \\(x\\), i.e., an interaction with itself even though no interaction term was specified in the model.Thus, visualizing the rate of change in the mean response with respect to a covariate \\(x\\) becomes a useful tool in interpreting GLMs."
  },
  {
    "objectID": "posts/2023-gsoc-update-slopes.html#average-predictive-slopes",
    "href": "posts/2023-gsoc-update-slopes.html#average-predictive-slopes",
    "title": "Google Summer of Code - Average Predictive Slopes",
    "section": "Average Predictive Slopes",
    "text": "Average Predictive Slopes\nHere, we adopt the notation from Chapter 14.4 of Regression and Other Stories to first describe average predictive differences which is essential to computing slopes, and then secondly, average predictive slopes. Assume we have fit a Bambi model predicting an outcome \\(Y\\) based on inputs \\(X\\) and parameters \\(\\theta\\). Consider the following scalar inputs:\n\\[w: \\text{the input of interest}\\] \\[c: \\text{all the other inputs}\\] \\[X = (w, c)\\]\nIn contrast to comparisons, for slopes we are interested in comparing \\(w^{\\text{value}}\\) to \\(w^{\\text{value}+\\epsilon}\\) (perhaps age = 60 and 60.0001 respectively) with all other inputs \\(c\\) held constant. The predictive difference in the outcome changing only \\(w\\) is:\n\\[\\text{average predictive difference} = \\mathbb{E}(y|w^{\\text{value}}, c, \\theta) - \\mathbb{E}(y|w^{\\text{value}+\\epsilon}, c, \\theta)\\]\nSelecting \\(w\\) and \\(w^{\\text{value}+\\epsilon}\\) and averaging over all other inputs \\(c\\) in the data gives you a new “hypothetical” dataset and corresponds to counting all pairs of transitions of \\((w^\\text{value})\\) to \\((w^{\\text{value}+\\epsilon})\\), i.e., differences in \\(w\\) with \\(c\\) held constant. The difference between these two terms is the average predictive difference.\nHowever, to obtain the slope estimate, we need to take the above formula and divide by \\(\\epsilon\\) to obtain the average predictive slope:\n\\[\\text{average predictive slope} = \\frac{\\mathbb{E}(y|w^{\\text{value}}, c, \\theta) - \\mathbb{E}(y|w^{\\text{value}+\\epsilon}, c, \\theta)}{\\epsilon}\\]"
  },
  {
    "objectID": "posts/2023-gsoc-update-slopes.html#computing-slopes",
    "href": "posts/2023-gsoc-update-slopes.html#computing-slopes",
    "title": "Google Summer of Code - Average Predictive Slopes",
    "section": "Computing Slopes",
    "text": "Computing Slopes\nThe objective of slopes and plot_slopes is to compute the rate of change (slope) in the mean of the response \\(y\\) with respect to a small change \\(\\epsilon\\) in the predictor \\(x\\) conditional on other covariates \\(c\\) specified in the model. \\(w\\) is specified by the user and the original value is either provided by the user, else a default value (the mean) is computed by Bambi. The values for the other covariates \\(c\\) specified in the model can be determined under the following three scenarios:\n\nuser provided values\na grid of equally spaced and central values\nempirical distribution (original data used to fit the model)\n\nIn the case of (1) and (2) above, Bambi assembles all pairwise combinations (transitions) of \\(w\\) and \\(c\\) into a new “hypothetical” dataset. In (3), Bambi uses the original \\(c\\), and adds a small amount \\(\\epsilon\\) to each unit of observation’s \\(w\\). In each scenario, predictions are made on the data using the fitted model. Once the predictions are made, comparisons are computed using the posterior samples by taking the difference in the predicted outcome for each pair of transitions and dividing by \\(\\epsilon\\). The average of these slopes is the average predictive slopes.\nFor variables \\(w\\) with a string or categorical data type, the comparisons function is called to compute the expected difference in group means. Please refer to the comparisons documentation for more details.\nBelow, we present several examples showing how to use Bambi to perform these computations for us, and to return either a summary dataframe, or a visualization of the results.\n\nimport arviz as az\nimport pandas as pd\n\nimport bambi as bmb"
  },
  {
    "objectID": "posts/2023-gsoc-update-slopes.html#logistic-regression",
    "href": "posts/2023-gsoc-update-slopes.html#logistic-regression",
    "title": "Google Summer of Code - Average Predictive Slopes",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nTo demonstrate slopes and plot_slopes, we will use the well switching dataset to model the probability a household in Bangladesh switches water wells. The data are for an area of Arahazar Upazila, Bangladesh. The researchers labelled each well with its level of arsenic and an indication of whether the well was “safe” or “unsafe”. Those using unsafe wells were encouraged to switch. After several years, it was determined whether each household using an unsafe well had changed its well. The data contains \\(3020\\) observations on the following five variables:\n\nswitch: a factor with levels no and yes indicating whether the household switched to a new well\narsenic: the level of arsenic in the old well (measured in micrograms per liter)\ndist: the distance to the nearest safe well (measured in meters)\nassoc: a factor with levels no and yes indicating whether the household is a member of an arsenic education group\neduc: years of education of the household head\n\nFirst, a logistic regression model with no interactions is fit to the data. Subsequently, to demonstrate the benefits of plot_slopes in interpreting interactions, we will fit a logistic regression model with an interaction term.\n\ndata = pd.read_csv(\"http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat\", sep=\" \")\ndata[\"switch\"] = pd.Categorical(data[\"switch\"])\ndata[\"dist100\"] = data[\"dist\"] / 100\ndata[\"educ4\"] = data[\"educ\"] / 4\ndata.head()\n\n\n\n\n\n\n\n\nswitch\narsenic\ndist\nassoc\neduc\ndist100\neduc4\n\n\n\n\n1\n1\n2.36\n16.826000\n0\n0\n0.16826\n0.0\n\n\n2\n1\n0.71\n47.321999\n0\n0\n0.47322\n0.0\n\n\n3\n0\n2.07\n20.966999\n0\n10\n0.20967\n2.5\n\n\n4\n1\n1.15\n21.486000\n0\n12\n0.21486\n3.0\n\n\n5\n1\n1.10\n40.874001\n1\n14\n0.40874\n3.5\n\n\n\n\n\n\n\n\nwell_model = bmb.Model(\n    \"switch ~ dist100 + arsenic + educ4\",\n    data,\n    family=\"bernoulli\"\n)\n\nwell_idata = well_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nModeling the probability that switch==0\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, dist100, arsenic, educ4]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:02&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 3 seconds.\n\n\n\nUser provided values\nFirst, an example of scenario 1 (user provided values) is given below. In both plot_slopes and slopes, \\(w\\) and \\(c\\) are represented by wrt (with respect to) and conditional, respectively. The modeler has the ability to pass their own values for wrt and conditional by using a dictionary where the key-value pairs are the covariate and value(s) of interest.\nFor example, if we wanted to compute the slope of the probability of switching wells for a typical arsenic value of \\(1.3\\) conditional on a range of dist and educ values, we would pass the following dictionary in the code block below. By default, for \\(w\\), Bambi compares \\(w^\\text{value}\\) to \\(w^{\\text{value} + \\epsilon}\\) where \\(\\epsilon =\\) 1e-4. However, the value for \\(\\epsilon\\) can be changed by passing a value to the argument eps.\nThus, in this example, \\(w^\\text{value} = 1.3\\) and \\(w^{\\text{value} + \\epsilon} = 1.3001\\). The user is not limited to passing a list for the values. A np.array can also be used. Furthermore, Bambi by default, maps the order of the dict keys to the main, group, and panel of the matplotlib figure. Below, since dist100 is the first key, this is used for the x-axis, and educ4 is used for the group (color). If a third key was passed, it would be used for the panel (facet).\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model,\n    well_idata,\n    wrt={\"arsenic\": 1.3},\n    conditional={\"dist100\": [0.20, 0.50, 0.80], \"educ4\": [1.00, 1.20, 2.00]},\n)\nfig.set_size_inches(7, 3)\nfig.axes[0].set_ylabel(\"Slope of Well Switching Probability\");\n\n\n\n\n\n\n\n\nThe plot above shows that, for example, conditional on dist100 \\(= 0.2\\) and educ4 \\(= 1.0\\) a unit increase in arsenic is associated with households being \\(11\\)% less likely to switch wells. Notice that even though we fit a logistic regression model where the coefficients are on the log-odds scale, the slopes function returns the slope on the probability scale. Thus, we can interpret the y-axis (slope) as the expected change in the probability of switching wells for a unit increase in arsenic conditional on the specified covariates.\nslopes can be called directly to view a summary dataframe that includes the term name, estimate type (discussed in detail in the interpreting coefficients as an elasticity section), values \\(w\\) used to compute the estimate, the specified conditional covariates \\(c\\), and the expected slope of the outcome with the uncertainty interval (by default the \\(94\\)% highest density interval is computed).\n\nbmb.interpret.slopes(\n    well_model,\n    well_idata,\n    wrt={\"arsenic\": 1.5},\n    conditional={\n        \"dist100\": [0.20, 0.50, 0.80], \n        \"educ4\": [1.00, 1.20, 2.00]\n        }\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ndist100\neduc4\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndydx\n(1.5, 1.5001)\n0.2\n1.0\n-0.110797\n-0.128775\n-0.092806\n\n\n1\narsenic\ndydx\n(1.5, 1.5001)\n0.2\n1.2\n-0.109867\n-0.126725\n-0.091065\n\n\n2\narsenic\ndydx\n(1.5, 1.5001)\n0.2\n2.0\n-0.105618\n-0.122685\n-0.088383\n\n\n3\narsenic\ndydx\n(1.5, 1.5001)\n0.5\n1.0\n-0.116087\n-0.134965\n-0.096843\n\n\n4\narsenic\ndydx\n(1.5, 1.5001)\n0.5\n1.2\n-0.115632\n-0.134562\n-0.096543\n\n\n5\narsenic\ndydx\n(1.5, 1.5001)\n0.5\n2.0\n-0.113140\n-0.130448\n-0.093209\n\n\n6\narsenic\ndydx\n(1.5, 1.5001)\n0.8\n1.0\n-0.117262\n-0.136850\n-0.098549\n\n\n7\narsenic\ndydx\n(1.5, 1.5001)\n0.8\n1.2\n-0.117347\n-0.136475\n-0.098044\n\n\n8\narsenic\ndydx\n(1.5, 1.5001)\n0.8\n2.0\n-0.116957\n-0.135079\n-0.096476\n\n\n\n\n\n\n\nSince all covariates used to fit the model were also specified to compute the slopes, no default value is used for unspecified covariates. A default value is computed for the unspecified covariates because in order to peform predictions, Bambi is expecting a value for each covariate used to fit the model. Additionally, with GLM models, average predictive slopes are conditional in the sense that the estimate depends on the values of all the covariates in the model. Thus, for unspecified covariates, slopes and plot_slopes computes a default value (mean or mode based on the data type of the covariate). Each row in the summary dataframe is read as “the slope (or rate of change) of the probability of switching wells with respect to a small change in \\(w\\) conditional on \\(c\\) is \\(y\\)”.\n\n\nMultiple slope values\nUsers can also compute slopes on multiple values for wrt. For example, if we want to compute the slope of \\(y\\) with respect to arsenic \\(= 1.5\\), \\(2.0\\), and \\(2.5\\), simply pass a list or numpy array as the dictionary values for wrt. Keeping the conditional covariate and values the same, the following slope estimates are computed below.\n\nmultiple_values = bmb.interpret.slopes(\n    well_model,\n    well_idata,\n    wrt={\"arsenic\": [1.5, 2.0, 2.5]},\n    conditional={\n        \"dist100\": [0.20, 0.50, 0.80], \n        \"educ4\": [1.00, 1.20, 2.00]\n        }\n)\n\nmultiple_values.head(6)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ndist100\neduc4\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndydx\n(1.5, 1.5001)\n0.2\n1.0\n-0.110797\n-0.128775\n-0.092806\n\n\n1\narsenic\ndydx\n(2.0, 2.0001)\n0.2\n1.0\n-0.109867\n-0.126725\n-0.091065\n\n\n2\narsenic\ndydx\n(2.5, 2.5001)\n0.2\n1.0\n-0.105618\n-0.122685\n-0.088383\n\n\n3\narsenic\ndydx\n(1.5, 1.5001)\n0.2\n1.2\n-0.116087\n-0.134965\n-0.096843\n\n\n4\narsenic\ndydx\n(2.0, 2.0001)\n0.2\n1.2\n-0.115632\n-0.134562\n-0.096543\n\n\n5\narsenic\ndydx\n(2.5, 2.5001)\n0.2\n1.2\n-0.113140\n-0.130448\n-0.093209\n\n\n\n\n\n\n\nThe output above is essentially the same as the summary dataframe when we only passed one value to wrt. However, now each element (value) in the list gets a small amount \\(\\epsilon\\) added to it, and the slope is calculated for each of these values.\n\n\nConditional slopes\nAs stated in the interpreting interaction effects section, interpreting coefficients of multiple interaction terms can be difficult and cumbersome. Thus, plot_slopes provides an effective way to visualize the conditional slopes of the interaction effects. Below, we will use the same well switching dataset, but with interaction terms. Specifically, one interaction is added between dist100 and educ4, and another between arsenic and educ4.\n\nwell_model_interact = bmb.Model(\n    \"switch ~ dist100 + arsenic + educ4 + dist100:educ4 + arsenic:educ4\",\n    data,\n    family=\"bernoulli\"\n)\n\nwell_idata_interact = well_model_interact.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nModeling the probability that switch==0\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, dist100, arsenic, educ4, dist100:educ4, arsenic:educ4]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:15&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 15 seconds.\n\n\n\n# summary of coefficients\naz.summary(well_idata_interact)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nIntercept\n-0.097\n0.122\n-0.322\n0.137\n0.003\n0.002\n2259.0\n2203.0\n1.0\n\n\ndist100\n1.320\n0.175\n0.982\n1.640\n0.004\n0.003\n2085.0\n2457.0\n1.0\n\n\narsenic\n-0.398\n0.061\n-0.521\n-0.291\n0.001\n0.001\n2141.0\n2558.0\n1.0\n\n\neduc4\n0.102\n0.080\n-0.053\n0.246\n0.002\n0.001\n1935.0\n2184.0\n1.0\n\n\ndist100:educ4\n-0.330\n0.106\n-0.528\n-0.136\n0.002\n0.002\n2070.0\n2331.0\n1.0\n\n\narsenic:educ4\n-0.079\n0.043\n-0.161\n-0.000\n0.001\n0.001\n2006.0\n2348.0\n1.0\n\n\n\n\n\n\n\nThe coefficients of the linear model are shown in the table above. The interaction coefficents indicate the slope varies in a continuous fashion with the continuous variable.\nA negative value for arsenic:dist100 indicates that the “effect” of arsenic on the outcome is less negative as distance from the well increases. Similarly, a negative value for arsenic:educ4 indicates that the “effect” of arsenic on the outcome is more negative as education increases. Remember, these coefficients are still on the logit scale. Furthermore, as more variables and interaction terms are added to the model, interpreting these coefficients becomes more difficult.\nThus, lets use plot_slopes to visually see how the slope changes with respect to arsenic conditional on dist100 and educ4 changing. Notice in the code block below how parameters are passed to the subplot_kwargs and fig_kwargs arguments. At times, it can be useful to pass specific group and panel arguments to aid in the interpretation of the plot. Therefore, subplot_kwargs allows the user to manipulate the plotting by passing a dictionary where the keys are {\"main\": ..., \"group\": ..., \"panel\": ...} and the values are the names of the covariates to be plotted. fig_kwargs are figure level key word arguments such as figsize and sharey.\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=[\"dist100\", \"educ4\"],\n    subplot_kwargs={\"main\": \"dist100\", \"group\": \"educ4\", \"panel\": \"educ4\"},\n    fig_kwargs={\"figsize\": (16, 4), \"sharey\": True},\n    legend=False\n)\n\n\n\n\n\n\n\n\nWith interaction terms now defined, it can be seen how the slope of the outcome with respect to arsenic differ depending on the value of educ4. Especially in the case of educ4 \\(= 4.25\\), the slope is more “constant”, but with greater uncertainty. Lets compare this with the model that does not include any interaction terms.\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model,\n    well_idata,\n    wrt=\"arsenic\",\n    conditional=[\"dist100\", \"educ4\"],\n    subplot_kwargs={\"main\": \"dist100\", \"group\": \"educ4\", \"panel\": \"educ4\"},\n    fig_kwargs={\"figsize\": (16, 4), \"sharey\": True},\n    legend=False\n)\n\n\n\n\n\n\n\n\nFor the non-interaction model, conditional on a range of values for educ4 and dist100, the slopes of the outcome are nearly identical.\n\n\nUnit level slopes\nEvaluating average predictive slopes at central values for the conditional covariates \\(c\\) can be problematic when the inputs have a large variance since no single central value (mean, median, etc.) is representative of the covariate. This is especially true when \\(c\\) exhibits bi or multimodality. Thus, it may be desireable to use the empirical distribution of \\(c\\) to compute the predictive slopes, and then average over a specific or set of covariates to obtain average slopes. To achieve unit level slopes, do not pass a parameter into conditional and or specify None.\n\nunit_level = bmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=None\n)\n\n# empirical distribution\nprint(unit_level.shape[0] == well_model_interact.data.shape[0])\nunit_level.head(10)\n\nTrue\n\n\n\n\n\n\n\n\n\nterm\nestimate_type\nvalue\ndist100\neduc4\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndydx\n(2.36, 2.3601)\n0.16826\n0.00\n-0.084280\n-0.105566\n-0.063403\n\n\n1\narsenic\ndydx\n(0.71, 0.7101)\n0.47322\n0.00\n-0.097837\n-0.125057\n-0.070959\n\n\n2\narsenic\ndydx\n(2.07, 2.0701)\n0.20967\n2.50\n-0.118093\n-0.139848\n-0.093442\n\n\n3\narsenic\ndydx\n(1.15, 1.1501)\n0.21486\n3.00\n-0.150638\n-0.194765\n-0.108946\n\n\n4\narsenic\ndydx\n(1.1, 1.1001)\n0.40874\n3.50\n-0.161272\n-0.214761\n-0.108663\n\n\n5\narsenic\ndydx\n(3.9, 3.9001)\n0.69518\n2.25\n-0.073908\n-0.080525\n-0.067493\n\n\n6\narsenic\ndydx\n(2.97, 2.9701000000000004)\n0.80711\n1.00\n-0.108482\n-0.123517\n-0.093042\n\n\n7\narsenic\ndydx\n(3.24, 3.2401000000000004)\n0.55146\n2.50\n-0.088049\n-0.097939\n-0.078020\n\n\n8\narsenic\ndydx\n(3.28, 3.2801)\n0.52647\n0.00\n-0.087388\n-0.107331\n-0.068076\n\n\n9\narsenic\ndydx\n(2.52, 2.5201000000000002)\n0.75072\n0.00\n-0.099035\n-0.129517\n-0.073222\n\n\n\n\n\n\n\n\nwell_model_interact.data.head(10)\n\n\n\n\n\n\n\n\nswitch\narsenic\ndist\nassoc\neduc\ndist100\neduc4\n\n\n\n\n1\n1\n2.36\n16.826000\n0\n0\n0.16826\n0.00\n\n\n2\n1\n0.71\n47.321999\n0\n0\n0.47322\n0.00\n\n\n3\n0\n2.07\n20.966999\n0\n10\n0.20967\n2.50\n\n\n4\n1\n1.15\n21.486000\n0\n12\n0.21486\n3.00\n\n\n5\n1\n1.10\n40.874001\n1\n14\n0.40874\n3.50\n\n\n6\n1\n3.90\n69.517998\n1\n9\n0.69518\n2.25\n\n\n7\n1\n2.97\n80.710999\n1\n4\n0.80711\n1.00\n\n\n8\n1\n3.24\n55.146000\n0\n10\n0.55146\n2.50\n\n\n9\n1\n3.28\n52.646999\n1\n0\n0.52647\n0.00\n\n\n10\n1\n2.52\n75.071999\n1\n0\n0.75072\n0.00\n\n\n\n\n\n\n\nAbove, unit_level is the slopes summary dataframe and well_model_interact.data is the empirical data used to fit the model. Notice how the values for \\(c\\) are identical in both dataframes. However, for \\(w\\), the values are the original \\(w\\) value plus \\(\\epsilon\\). Thus, the estimate value represents the instantaneous rate of change for that unit of observation. However, these unit level slopes are difficult to interpret since each row may have a different slope estimate. Therefore, it is useful to average over (marginalize) the estimates to summarize the unit level predictive slopes.\n\nMarginalizing over covariates\nSince the empirical distrubution is used for computing the average predictive slopes, the same number of rows (\\(3020\\)) is returned as the data used to fit the model. To average over a covariate, use the average_by argument. If True is passed, then slopes averages over all covariates. Else, if a single or list of covariates are passed, then slopes averages by the covariates passed.\n\nbmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=None,\n    average_by=True\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndydx\n-0.111342\n-0.134846\n-0.088171\n\n\n\n\n\n\n\nThe code block above is equivalent to taking the mean of the estimate and uncertainty columns. For example:\n\nunit_level[[\"estimate\", \"lower_3.0%\", \"upper_97.0%\"]].mean()\n\nestimate      -0.111342\nlower_3.0%    -0.134846\nupper_97.0%   -0.088171\ndtype: float64\n\n\n\n\nAverage by subgroups\nAveraging over all covariates may not be desired, and you would rather average by a group or specific covariate. To perform averaging by subgroups, users can pass a single or list of covariates to average_by to average over specific covariates. For example, if we wanted to average by educ4:\n\n# average by educ4\nbmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=None,\n    average_by=\"educ4\"\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\neduc4\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndydx\n0.00\n-0.092389\n-0.119320\n-0.068167\n\n\n1\narsenic\ndydx\n0.25\n-0.101704\n-0.126096\n-0.076910\n\n\n2\narsenic\ndydx\n0.50\n-0.102112\n-0.122443\n-0.082142\n\n\n3\narsenic\ndydx\n0.75\n-0.106004\n-0.124247\n-0.088132\n\n\n4\narsenic\ndydx\n1.00\n-0.110580\n-0.127803\n-0.093221\n\n\n5\narsenic\ndydx\n1.25\n-0.112334\n-0.128771\n-0.094870\n\n\n6\narsenic\ndydx\n1.50\n-0.114875\n-0.132652\n-0.096790\n\n\n7\narsenic\ndydx\n1.75\n-0.122557\n-0.142921\n-0.101423\n\n\n8\narsenic\ndydx\n2.00\n-0.125187\n-0.148096\n-0.101350\n\n\n9\narsenic\ndydx\n2.25\n-0.125367\n-0.150676\n-0.099852\n\n\n10\narsenic\ndydx\n2.50\n-0.130748\n-0.159912\n-0.101058\n\n\n11\narsenic\ndydx\n2.75\n-0.137422\n-0.170662\n-0.102995\n\n\n12\narsenic\ndydx\n3.00\n-0.136103\n-0.172119\n-0.099548\n\n\n13\narsenic\ndydx\n3.25\n-0.156941\n-0.202215\n-0.107625\n\n\n14\narsenic\ndydx\n3.50\n-0.142571\n-0.186079\n-0.098362\n\n\n15\narsenic\ndydx\n3.75\n-0.138336\n-0.181042\n-0.093120\n\n\n16\narsenic\ndydx\n4.00\n-0.138152\n-0.185974\n-0.089611\n\n\n17\narsenic\ndydx\n4.25\n-0.176623\n-0.244273\n-0.107141\n\n\n\n\n\n\n\n\n# average by both educ4 and dist100\nbmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=None,\n    average_by=[\"educ4\", \"dist100\"]\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\neduc4\ndist100\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndydx\n0.00\n0.00591\n-0.085861\n-0.109133\n-0.061614\n\n\n1\narsenic\ndydx\n0.00\n0.02409\n-0.096272\n-0.127518\n-0.069670\n\n\n2\narsenic\ndydx\n0.00\n0.02454\n-0.056617\n-0.065433\n-0.046970\n\n\n3\narsenic\ndydx\n0.00\n0.02791\n-0.097646\n-0.128131\n-0.069660\n\n\n4\narsenic\ndydx\n0.00\n0.03252\n-0.076300\n-0.095832\n-0.057900\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2992\narsenic\ndydx\n4.00\n1.13727\n-0.070078\n-0.094698\n-0.046623\n\n\n2993\narsenic\ndydx\n4.00\n1.14418\n-0.125547\n-0.172943\n-0.075368\n\n\n2994\narsenic\ndydx\n4.00\n1.25308\n-0.156780\n-0.218836\n-0.088258\n\n\n2995\narsenic\ndydx\n4.00\n1.67025\n-0.161465\n-0.227211\n-0.085394\n\n\n2996\narsenic\ndydx\n4.25\n0.29633\n-0.176623\n-0.244273\n-0.107141\n\n\n\n\n2997 rows × 7 columns\n\n\n\nIt is still possible to use plot_slopes when passing an argument to average_by. In the plot below, the empirical distribution is used to compute unit level slopes with respect to arsenic and then averaged over educ4 to obtain the average predictive slopes.\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=None,\n    average_by=\"educ4\"\n)\nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\n\n\n\nInterpreting coefficients as an elasticity\nIn some fields, such as economics, it is useful to interpret the results of a regression model in terms of an elasticity (a percent change in \\(x\\) is associated with a percent change in \\(y\\)) or semi-elasticity (a unit change in \\(x\\) is associated with a percent change in \\(y\\), or vice versa). Typically, this is achieved by fitting a model where either the outcome and or the covariates are log-transformed. However, since the log transformation is performed by the modeler, to compute elasticities for slopes and plot_slopes, Bambi “post-processes” the predictions to compute the elasticities. Below, it is shown the possible elasticity arguments and how they are computed for slopes and plot_slopes:\n\neyex: a percentage point increase in \\(x_1\\) is associated with an \\(n\\) percentage point increase in \\(y\\)\n\n\\[\\frac{\\partial \\hat{y}}{\\partial x_1} * \\frac{x_1}{\\hat{y}}\\]\n\neydx: a unit increase in \\(x_1\\) is associated with an \\(n\\) percentage point increase in \\(y\\)\n\n\\[\\frac{\\partial \\hat{y}}{\\partial x_1} * \\frac{1}{\\hat{y}}\\]\n\ndyex: a percentage point increase in \\(x_1\\) is associated with an \\(n\\) unit increase in \\(y\\)\n\n\\[\\frac{\\partial \\hat{y}}{\\partial x_1} * x_1\\]\nBelow, each code cell shows the same model, and wrt and conditional argument, but with a different elasticity (slope) argument. By default, dydx (a derivative with no post-processing) is used.\n\nbmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    slope=\"eyex\",\n    conditional=None,\n    average_by=True\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\neyex\n-0.525124\n-0.652708\n-0.396082\n\n\n\n\n\n\n\n\nbmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    slope=\"eydx\",\n    conditional=None,\n    average_by=True\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\neydx\n-0.286753\n-0.351592\n-0.220459\n\n\n\n\n\n\n\n\nbmb.interpret.slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    slope=\"dyex\",\n    conditional=None,\n    average_by=True\n)\n\n\n\n\n\n\n\n\nterm\nestimate_type\nestimate\nlower_3.0%\nupper_97.0%\n\n\n\n\n0\narsenic\ndyex\n-0.167616\n-0.201147\n-0.134605\n\n\n\n\n\n\n\nslope is also an argument for plot_slopes. Below, we visualize the elasticity with respect to arsenic conditional on a range of dist100 and educ4 values (notice this is the same plot as in the conditional slopes section).\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"arsenic\",\n    conditional=[\"dist100\", \"educ4\"],\n    slope=\"eyex\",\n    subplot_kwargs={\"main\": \"dist100\", \"group\": \"educ4\", \"panel\": \"educ4\"},\n    fig_kwargs={\"figsize\": (16, 4), \"sharey\": True},\n    legend=False\n)\n\n\n\n\n\n\n\n\n\n\nCategorical covariates\nAs mentioned in the computing slopes section, if you pass a variable with a string or categorical data type, the comparisons function will be called to compute the expected difference in group means. Here, we fit the same interaction model as above, albeit, by specifying educ4 as an ordinal data type.\n\ndata = pd.read_csv(\"http://www.stat.columbia.edu/~gelman/arm/examples/arsenic/wells.dat\", sep=\" \")\ndata[\"switch\"] = pd.Categorical(data[\"switch\"])\ndata[\"dist100\"] = data[\"dist\"] / 100\ndata[\"educ4\"] = pd.Categorical(data[\"educ\"] / 4, ordered=True)\n\n\nwell_model_interact = bmb.Model(\n    \"switch ~ dist100 + arsenic + educ4 + dist100:educ4 + arsenic:educ4\",\n    data,\n    family=\"bernoulli\"\n)\n\nwell_idata_interact = well_model_interact.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234, \n    chains=4\n)\n\nModeling the probability that switch==0\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, dist100, arsenic, educ4, dist100:educ4, arsenic:educ4]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 05:18&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 319 seconds.\n\n\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt=\"educ4\",\n    conditional=\"dist100\",\n    average_by=\"dist100\"\n)\nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\nAs the model was fit with educ4 as a categorical data type, Bambi recognized this, and calls comparisons to compute the differences between each level of educ4. As educ4 contains many category levels, a covariate must be passed to average_by in order to perform plotting. Below, we can see this plot is equivalent to plot_comparisons.\n\nfig, ax = bmb.interpret.plot_comparisons(\n    well_model_interact,\n    well_idata_interact,\n    contrast=\"educ4\",\n    conditional=\"dist100\",\n    average_by=\"dist100\"\n)\nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\nHowever, computing the predictive difference between each educ4 level may not be desired. Thus, in plot_slopes, as in plot_comparisons, if wrt is a categorical or string data type, it is possible to specify the wrt values. For example, if we wanted to compute the expected difference in probability of switching wells for when educ4 is \\(4\\) versus \\(1\\) conditional on a range of dist100 and arsenic values, we would pass the following dictionary in the code block below. Please refer to the comparisons documentation for more details.\n\nfig, ax = bmb.interpret.plot_slopes(\n    well_model_interact,\n    well_idata_interact,\n    wrt={\"educ4\": [1, 4]},\n    conditional=\"dist100\",\n    average_by=\"dist100\"\n)\nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\n\n%load_ext watermark\n%watermark -n -u -v -iv -w\n\nLast updated: Wed Aug 16 2023\n\nPython implementation: CPython\nPython version       : 3.11.0\nIPython version      : 8.13.2\n\npandas: 2.0.1\narviz : 0.15.1\nbambi : 0.10.0.dev0\n\nWatermark: 2.3.1"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html",
    "href": "posts/2022-07-23-bmcp-ch-5.html",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport arviz as az\nimport torch\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.distributions import constraints, transforms\nfrom pyro.infer import Predictive, TracePredictive, NUTS, MCMC\nfrom pyro.infer.autoguide import AutoLaplaceApproximation\nfrom pyro.infer import SVI, Trace_ELBO\nfrom pyro.optim import Adam\nfrom patsy import dmatrix\nplt.style.use('ggplot')\nplt.rcParams[\"figure.figsize\"] = (9, 4)"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html#fitting-splines-in-pyro",
    "href": "posts/2022-07-23-bmcp-ch-5.html#fitting-splines-in-pyro",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "5.5 - Fitting splines in Pyro",
    "text": "5.5 - Fitting splines in Pyro\n\nday = pd.read_csv('./data/Bike-Sharing-Dataset/day.csv')\nhour = pd.read_csv('./data/Bike-Sharing-Dataset/hour.csv')\nhour['cnt_std'] = hour['cnt'] / hour['cnt'].max()\n\n\nsns.scatterplot(x=hour['hr'], y=hour['cnt_std'], alpha=0.1, color='grey')\nplt.ylabel('Count')\nplt.xlabel('Hour of Day (0-23)')\nplt.title('Actual Bike Demand');\n\n\n\n\n\n\n\n\n\nnum_knots = 6\nknot_list = torch.linspace(0, 23, num_knots + 2)[1:-1]\n\nB = dmatrix(\n    \"bs(cnt_std, knots=knots, degree=3, include_intercept=True) - 1\",\n    {'cnt_std': hour.hr.values, 'knots': knot_list[1:-1]}\n)\n\nB = torch.tensor(np.asarray(B)).float()\ncnt_bikes = torch.tensor(hour['cnt_std'].values).float()\nhour_bikes = torch.tensor(hour['hr'].values).reshape(-1, 1).float()\n\n\nSplines Model - MCMC\n\ndef splines(design_matrix, count_bikes=None):\n\n    N, P = design_matrix.shape\n\n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n\n    with pyro.plate('knot_list', P):\n        beta = pyro.sample('beta', dist.Normal(0., tau))\n\n    mu = pyro.deterministic('mu', torch.matmul(beta, design_matrix.T))\n\n    with pyro.plate('output', N):\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=count_bikes)\n\n\npyro.render_model(\n    splines, (B, cnt_bikes), render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nkernel = NUTS(splines)\nmcmc_splines = MCMC(kernel, 500, 300)\nmcmc_splines.run(B, cnt_bikes)\n\nSample: 100%|██████████| 800/800 [00:12, 62.91it/s, step size=2.93e-01, acc. prob=0.887]\n\n\n\nprior_predictive = Predictive(splines, num_samples=500)(B, None)\nspline_samples = mcmc_splines.get_samples(500)\nsplines_predictive = Predictive(splines, spline_samples)(B, None)\n\naz_splines_pred = az.from_pyro(\n    prior=prior_predictive,\n    posterior=mcmc_splines, \n    posterior_predictive=splines_predictive\n    )\n\n\nsns.lineplot(\n    x=hour_bikes.flatten(), y=splines_predictive['mu'].mean(axis=0).T.flatten(),\n    color='black'\n    )\nsns.lineplot(\n    x=hour_bikes.flatten(), y=(B * spline_samples['beta'].mean(axis=0))[:, 5],\n    linestyle='--'\n    );\n\n\n\n\n\n\n\n\n\ncnt_mu = splines_predictive['y'].mean(axis=0).T.flatten()\ncnt_std = splines_predictive['y'].std(axis=0).T.flatten()\n\ndf = pd.DataFrame({\n    'hr': hour['hr'].values,\n    'cnt_scaled': hour['cnt_std'].values,\n    'cnt_mu': cnt_mu,\n    'cnt_std': cnt_std,\n    'cnt_high': cnt_mu + cnt_std,\n    'cnt_low': cnt_mu - cnt_std\n})\n\ndf = df.sort_values(by=['hr'])\n\n\n\nFigure 5.9\n\nsns.lineplot(\n    x=df['hr'], y=df['cnt_mu'], color='blue')\nsns.scatterplot(\n    x=hour['hr'], y=hour['cnt_std'], color='grey', alpha=0.3\n    )\nplt.fill_between(\n    x=df['hr'], y1=df['cnt_high'], y2=df['cnt_low'], color='grey',\n    alpha=0.3\n    )\nplt.scatter(knot_list, np.zeros_like(knot_list), color='black')\nplt.title('Cubic Splines using 6 Knots');"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html#choosing-knots-and-priors-for-splines",
    "href": "posts/2022-07-23-bmcp-ch-5.html#choosing-knots-and-priors-for-splines",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "5.6 - Choosing knots and priors for splines",
    "text": "5.6 - Choosing knots and priors for splines\n\nBs = []\nnum_knots = [3, 6, 9, 12, 18]\nfor nk in num_knots:\n    knot_list = torch.linspace(0, 24, nk+2)[1:-1]\n    B = dmatrix(\n        'bs(cnt, knots=knots, degree=3, include_intercept=True) - 1',\n        {'cnt': hour.hr.values, 'knots': knot_list[1:-1]}\n    )\n    B = torch.tensor(np.asarray(B)).float()\n    Bs.append(B)\n\n\ninf_data = []\nfor B in Bs:\n\n    mcmc_obj = MCMC(NUTS(splines), 500, 300)\n    mcmc_obj.run(B, cnt_bikes)\n\n    post_samples = mcmc_obj.get_samples(500)\n    post_pred = Predictive(\n        splines, post_samples\n    )(B, None)\n\n    az_obj = az.from_pyro(\n        posterior=mcmc_obj,\n        posterior_predictive=post_pred\n    )\n\n    inf_data.append(az_obj)\n\nSample: 100%|██████████| 800/800 [00:16, 48.58it/s, step size=2.62e-01, acc. prob=0.924]\nSample: 100%|██████████| 800/800 [00:13, 58.92it/s, step size=2.90e-01, acc. prob=0.888]\nSample: 100%|██████████| 800/800 [00:14, 55.82it/s, step size=2.67e-01, acc. prob=0.910]\nSample: 100%|██████████| 800/800 [00:13, 58.91it/s, step size=3.04e-01, acc. prob=0.886]\nSample: 100%|██████████| 800/800 [00:17, 45.97it/s, step size=2.59e-01, acc. prob=0.890]\n\n\n\n# something is not right here\ndict_cmp = {f\"m_{k}k\": v for k, v in zip(num_knots, inf_data)}\ncmp = az.compare(dict_cmp, ic='loo', var_name='y')\ncmp\n\n['m_18k', 'm_12k', 'm_9k', 'm_6k', 'm_3k']\n\n\n\n\n\n\n\n\n\nrank\nelpd_loo\np_loo\nelpd_diff\nweight\nse\ndse\nwarning\nscale\n\n\n\n\nm_3k\n0\n10575.715589\n20.874493\n0.000000\n0.850828\n129.781140\n0.000000\nFalse\nlog\n\n\nm_6k\n1\n10423.693963\n14.518533\n152.021626\n0.000000\n131.808422\n19.387344\nFalse\nlog\n\n\nm_9k\n2\n10094.535427\n12.458113\n481.180162\n0.000000\n133.635244\n36.071652\nFalse\nlog\n\n\nm_12k\n3\n9580.695289\n8.562018\n995.020300\n0.000000\n136.447458\n53.955052\nFalse\nlog\n\n\nm_18k\n4\n8600.222467\n6.347949\n1975.493122\n0.149172\n142.746908\n81.838699\nFalse\nlog\n\n\n\n\n\n\n\n\ncolors = ['black', 'blue', 'grey', 'grey', 'black']\nlinestyle = [\"-\",\"-\",\"--\",\"--\",\"-\"]\nlinewidth = [1.5, 3, 1.5, 1.5, 3]\n\nfor ob, col, knots, ls, lw in zip(\n    inf_data, colors, sorted(num_knots, reverse=True), linestyle, linewidth\n    ):\n\n    sns.lineplot(\n    x=hour['hr'], y=ob['posterior_predictive']['y'][0].mean(axis=0), color=col,\n    label=f'knots={knots}', linestyle=ls, linewidth=lw\n    )\n    sns.scatterplot(\n    x=hour['hr'], y=hour['cnt_std'].values, color='lightgrey', alpha=0.5, edgecolor='grey'\n    )\n    plt.title('Model fit with different number of knots');\n\n\n\n\n\n\n\n\n\n5.6.1 Regularizing priors for splines\n\nclass GaussianRandomWalk(dist.TorchDistribution):\n    has_rsample = True\n    arg_constraints = {'scale': constraints.positive}\n    support = constraints.real\n\n    def __init__(self, scale, num_steps=1):\n        self.scale = scale\n        batch_shape, event_shape = scale.shape, torch.Size([num_steps])\n        super(GaussianRandomWalk, self).__init__(batch_shape, event_shape)\n    \n    def rsample(self, sample_shape=torch.Size()):\n        shape = sample_shape + self.batch_shape + self.event_shape\n        walks = self.scale.new_empty(shape).normal_()\n        return walks.cumsum(-1) * self.scale.unsqueeze(-1)\n    \n    def log_prob(self, x):\n        init_prob = dist.Normal(self.scale.new_tensor(0.), self.scale).log_prob(x[..., 0])\n        step_probs = dist.Normal(x[..., :-1], self.scale).log_prob(x[..., 1:])\n        return init_prob + step_probs.sum(-1)\n\n\ndef splines_grw(design_matrix, count_bikes=None):\n\n    N, P = design_matrix.shape\n\n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n\n    with pyro.plate('knot_list', P):\n        beta = pyro.sample('beta', GaussianRandomWalk(scale=tau, num_steps=14))\n\n    mu = pyro.deterministic('mu', torch.matmul(beta, design_matrix.T))\n\n    with pyro.plate('output', N):\n        output = pyro.sample('y', dist.Normal(mu, sigma), obs=count_bikes)\n\n\nnum_knots = 12\nknot_list = torch.linspace(0, 23, num_knots + 2)[1:-1]\n\nB = dmatrix(\n    \"bs(cnt_std, knots=knots, degree=3, include_intercept=True) - 1\",\n    {'cnt_std': hour.hr.values, 'knots': knot_list[1:-1]}\n)\n\nB = torch.tensor(np.asarray(B)).float()\ncnt_bikes = torch.tensor(hour['cnt_std'].values).float()\nhour_bikes = torch.tensor(hour['hr'].values).reshape(-1, 1).float()\n\n\nsplines_grw_mcmc = MCMC(NUTS(splines_grw), 500, 300)\nsplines_grw_mcmc.run(B, cnt_bikes)\n\nSample: 100%|██████████| 800/800 [01:48,  7.40it/s, step size=1.51e-01, acc. prob=0.878]\n\n\n\nsplines_grw_mcmc.summary()\n\n\n                 mean       std    median      5.0%     95.0%     n_eff     r_hat\n  beta[0,0]      0.06      0.00      0.06      0.05      0.06    793.18      1.00\n  beta[0,1]     -0.00      0.01     -0.00     -0.02      0.02    543.60      1.00\n  beta[0,2]      0.07      0.01      0.07      0.05      0.08    539.25      1.00\n  beta[0,3]     -0.07      0.01     -0.07     -0.08     -0.05    593.26      1.00\n  beta[0,4]      0.32      0.01      0.32      0.31      0.34    598.96      1.00\n  beta[0,5]      0.30      0.01      0.30      0.29      0.32    647.04      1.00\n  beta[0,6]      0.10      0.01      0.10      0.09      0.12    580.51      1.00\n  beta[0,7]      0.34      0.01      0.34      0.33      0.36    588.65      1.00\n  beta[0,8]      0.19      0.01      0.19      0.18      0.20    556.40      1.00\n  beta[0,9]      0.31      0.01      0.31      0.30      0.32    542.28      1.00\n beta[0,10]      0.56      0.01      0.56      0.55      0.58    614.87      1.00\n beta[0,11]      0.11      0.01      0.11      0.09      0.13    651.78      1.00\n beta[0,12]      0.19      0.01      0.19      0.17      0.20    619.13      1.00\n beta[0,13]      0.09      0.01      0.09      0.08      0.10    684.85      1.00\n  beta[1,0]      0.06      0.01      0.06      0.05      0.07    684.00      1.00\n  beta[1,1]     -0.00      0.01     -0.00     -0.02      0.01    546.69      1.00\n  beta[1,2]      0.07      0.01      0.07      0.05      0.09    424.07      1.01\n  beta[1,3]     -0.07      0.01     -0.07     -0.08     -0.05    269.08      1.02\n  beta[1,4]      0.32      0.01      0.32      0.31      0.33    414.38      1.02\n  beta[1,5]      0.30      0.01      0.30      0.29      0.32    618.06      1.01\n  beta[1,6]      0.10      0.01      0.10      0.09      0.12    563.03      1.00\n  beta[1,7]      0.34      0.01      0.34      0.33      0.36    422.95      1.00\n  beta[1,8]      0.19      0.01      0.19      0.18      0.20    553.61      1.00\n  beta[1,9]      0.31      0.01      0.31      0.29      0.32    468.49      1.00\n beta[1,10]      0.56      0.01      0.56      0.55      0.58    446.50      1.00\n beta[1,11]      0.11      0.01      0.11      0.10      0.13    373.58      1.00\n beta[1,12]      0.19      0.01      0.19      0.17      0.20    500.06      1.00\n beta[1,13]      0.09      0.01      0.09      0.08      0.10    610.37      1.00\n  beta[2,0]      0.06      0.01      0.06      0.05      0.07    564.38      1.00\n  beta[2,1]     -0.00      0.01     -0.00     -0.02      0.01    599.84      1.00\n  beta[2,2]      0.07      0.01      0.07      0.05      0.09    452.10      1.00\n  beta[2,3]     -0.07      0.01     -0.07     -0.08     -0.05    550.86      1.00\n  beta[2,4]      0.32      0.01      0.32      0.31      0.34    599.10      1.00\n  beta[2,5]      0.30      0.01      0.30      0.29      0.32    688.41      1.00\n  beta[2,6]      0.10      0.01      0.10      0.09      0.11    497.99      1.00\n  beta[2,7]      0.34      0.01      0.34      0.33      0.36    439.54      1.00\n  beta[2,8]      0.19      0.01      0.19      0.17      0.20    695.93      1.00\n  beta[2,9]      0.31      0.01      0.31      0.29      0.32    728.54      1.00\n beta[2,10]      0.56      0.01      0.56      0.55      0.58    574.70      1.00\n beta[2,11]      0.11      0.01      0.11      0.09      0.13    543.40      1.00\n beta[2,12]      0.19      0.01      0.19      0.17      0.20    512.70      1.00\n beta[2,13]      0.09      0.01      0.09      0.08      0.10    635.21      1.00\n  beta[3,0]      0.06      0.01      0.06      0.05      0.07    598.90      1.00\n  beta[3,1]     -0.00      0.01     -0.00     -0.02      0.01    480.85      1.00\n  beta[3,2]      0.07      0.01      0.07      0.05      0.08    540.00      1.00\n  beta[3,3]     -0.07      0.01     -0.07     -0.08     -0.05    461.03      1.00\n  beta[3,4]      0.32      0.01      0.32      0.31      0.33    489.37      1.00\n  beta[3,5]      0.30      0.01      0.30      0.29      0.32    484.36      1.00\n  beta[3,6]      0.10      0.01      0.10      0.09      0.12    591.71      1.00\n  beta[3,7]      0.34      0.01      0.34      0.33      0.36    468.37      1.00\n  beta[3,8]      0.19      0.01      0.19      0.18      0.20    325.26      1.00\n  beta[3,9]      0.31      0.01      0.31      0.30      0.32    618.83      1.00\n beta[3,10]      0.56      0.01      0.56      0.55      0.58    502.47      1.00\n beta[3,11]      0.11      0.01      0.11      0.09      0.12    542.12      1.00\n beta[3,12]      0.19      0.01      0.19      0.17      0.20    576.58      1.00\n beta[3,13]      0.09      0.01      0.09      0.08      0.09    709.30      1.00\n  beta[4,0]      0.06      0.00      0.06      0.05      0.07    785.06      1.00\n  beta[4,1]     -0.00      0.01     -0.00     -0.02      0.01    604.06      1.00\n  beta[4,2]      0.07      0.01      0.07      0.05      0.09    493.62      1.00\n  beta[4,3]     -0.07      0.01     -0.07     -0.08     -0.05    511.71      1.00\n  beta[4,4]      0.32      0.01      0.32      0.31      0.34    497.03      1.00\n  beta[4,5]      0.30      0.01      0.30      0.29      0.31    657.27      1.00\n  beta[4,6]      0.10      0.01      0.10      0.09      0.12    577.98      1.00\n  beta[4,7]      0.34      0.01      0.34      0.33      0.36    514.37      1.00\n  beta[4,8]      0.19      0.01      0.19      0.18      0.20    470.99      1.00\n  beta[4,9]      0.31      0.01      0.31      0.29      0.32    669.33      1.00\n beta[4,10]      0.56      0.01      0.56      0.55      0.58    540.58      1.00\n beta[4,11]      0.11      0.01      0.11      0.09      0.12    502.99      1.00\n beta[4,12]      0.19      0.01      0.19      0.17      0.20    417.16      1.00\n beta[4,13]      0.09      0.00      0.09      0.08      0.09    592.79      1.00\n  beta[5,0]      0.06      0.01      0.06      0.05      0.06    841.34      1.00\n  beta[5,1]     -0.00      0.01     -0.00     -0.02      0.01    598.37      1.00\n  beta[5,2]      0.07      0.01      0.07      0.05      0.08    393.97      1.00\n  beta[5,3]     -0.07      0.01     -0.07     -0.08     -0.05    470.53      1.00\n  beta[5,4]      0.32      0.01      0.32      0.31      0.33    519.00      1.00\n  beta[5,5]      0.30      0.01      0.30      0.29      0.32    454.97      1.00\n  beta[5,6]      0.10      0.01      0.10      0.09      0.11    467.47      1.00\n  beta[5,7]      0.34      0.01      0.34      0.33      0.36    453.50      1.00\n  beta[5,8]      0.19      0.01      0.19      0.17      0.20    476.61      1.00\n  beta[5,9]      0.31      0.01      0.31      0.29      0.32    468.98      1.00\n beta[5,10]      0.56      0.01      0.56      0.55      0.58    412.85      1.00\n beta[5,11]      0.11      0.01      0.11      0.09      0.13    417.32      1.00\n beta[5,12]      0.19      0.01      0.19      0.17      0.20    487.35      1.01\n beta[5,13]      0.09      0.00      0.09      0.08      0.10    855.13      1.00\n  beta[6,0]      0.06      0.00      0.06      0.05      0.06    469.53      1.00\n  beta[6,1]     -0.00      0.01     -0.00     -0.02      0.01    490.03      1.00\n  beta[6,2]      0.07      0.01      0.07      0.05      0.08    412.12      1.00\n  beta[6,3]     -0.07      0.01     -0.07     -0.08     -0.05    547.10      1.00\n  beta[6,4]      0.32      0.01      0.32      0.31      0.34    513.47      1.00\n  beta[6,5]      0.30      0.01      0.30      0.29      0.32    527.11      1.00\n  beta[6,6]      0.10      0.01      0.10      0.09      0.11    568.13      1.00\n  beta[6,7]      0.34      0.01      0.34      0.33      0.36    621.95      1.00\n  beta[6,8]      0.19      0.01      0.19      0.18      0.20    611.84      1.00\n  beta[6,9]      0.31      0.01      0.31      0.30      0.32    591.34      1.00\n beta[6,10]      0.56      0.01      0.56      0.55      0.58    485.72      1.00\n beta[6,11]      0.11      0.01      0.11      0.10      0.13    454.76      1.00\n beta[6,12]      0.19      0.01      0.19      0.17      0.20    418.77      1.00\n beta[6,13]      0.09      0.00      0.09      0.08      0.10    635.51      1.00\n  beta[7,0]      0.06      0.00      0.06      0.05      0.07    633.33      1.00\n  beta[7,1]     -0.00      0.01     -0.00     -0.02      0.01    574.07      1.00\n  beta[7,2]      0.07      0.01      0.07      0.05      0.08    519.25      1.00\n  beta[7,3]     -0.07      0.01     -0.07     -0.08     -0.05    619.73      1.00\n  beta[7,4]      0.32      0.01      0.32      0.31      0.34    623.32      1.00\n  beta[7,5]      0.30      0.01      0.30      0.29      0.32    487.51      1.00\n  beta[7,6]      0.10      0.01      0.10      0.09      0.12    521.75      1.00\n  beta[7,7]      0.34      0.01      0.34      0.33      0.36    528.14      1.00\n  beta[7,8]      0.19      0.01      0.19      0.18      0.20    536.29      1.00\n  beta[7,9]      0.31      0.01      0.31      0.30      0.32    474.66      1.01\n beta[7,10]      0.56      0.01      0.56      0.55      0.58    444.83      1.01\n beta[7,11]      0.11      0.01      0.11      0.10      0.13    578.87      1.00\n beta[7,12]      0.18      0.01      0.19      0.17      0.20    636.52      1.00\n beta[7,13]      0.09      0.00      0.09      0.08      0.09    726.01      1.00\n  beta[8,0]      0.06      0.00      0.06      0.05      0.06    759.45      1.00\n  beta[8,1]     -0.00      0.01     -0.00     -0.02      0.02    451.96      1.00\n  beta[8,2]      0.07      0.01      0.07      0.05      0.09    556.69      1.00\n  beta[8,3]     -0.07      0.01     -0.07     -0.08     -0.05    587.00      1.00\n  beta[8,4]      0.32      0.01      0.32      0.31      0.34    432.25      1.01\n  beta[8,5]      0.30      0.01      0.30      0.29      0.32    497.88      1.00\n  beta[8,6]      0.10      0.01      0.10      0.09      0.12    493.79      1.00\n  beta[8,7]      0.34      0.01      0.34      0.33      0.36    473.27      1.00\n  beta[8,8]      0.19      0.01      0.19      0.18      0.20    511.87      1.00\n  beta[8,9]      0.31      0.01      0.31      0.30      0.32    560.39      1.00\n beta[8,10]      0.56      0.01      0.56      0.55      0.57    643.56      1.00\n beta[8,11]      0.11      0.01      0.11      0.09      0.13    615.96      1.00\n beta[8,12]      0.19      0.01      0.19      0.17      0.20    789.36      1.00\n beta[8,13]      0.09      0.00      0.09      0.08      0.10    722.99      1.00\n  beta[9,0]      0.06      0.01      0.06      0.05      0.07    705.95      1.00\n  beta[9,1]     -0.00      0.01     -0.00     -0.02      0.02    539.48      1.01\n  beta[9,2]      0.07      0.01      0.07      0.05      0.09    729.97      1.01\n  beta[9,3]     -0.07      0.01     -0.07     -0.08     -0.05    606.84      1.01\n  beta[9,4]      0.32      0.01      0.32      0.31      0.33    533.78      1.00\n  beta[9,5]      0.30      0.01      0.30      0.29      0.32    770.46      1.00\n  beta[9,6]      0.10      0.01      0.10      0.09      0.11    672.34      1.00\n  beta[9,7]      0.34      0.01      0.34      0.33      0.36    590.69      1.00\n  beta[9,8]      0.19      0.01      0.19      0.17      0.20    701.83      1.00\n  beta[9,9]      0.31      0.01      0.31      0.30      0.32    491.28      1.00\n beta[9,10]      0.56      0.01      0.56      0.55      0.58    501.94      1.00\n beta[9,11]      0.11      0.01      0.11      0.10      0.13    458.62      1.00\n beta[9,12]      0.19      0.01      0.19      0.17      0.20    491.62      1.01\n beta[9,13]      0.09      0.00      0.09      0.08      0.09    694.76      1.00\n beta[10,0]      0.06      0.01      0.06      0.05      0.07    661.86      1.00\n beta[10,1]     -0.00      0.01     -0.00     -0.02      0.01    560.89      1.00\n beta[10,2]      0.07      0.01      0.07      0.05      0.09    487.57      1.00\n beta[10,3]     -0.07      0.01     -0.07     -0.08     -0.05    538.75      1.00\n beta[10,4]      0.32      0.01      0.32      0.31      0.34    582.82      1.00\n beta[10,5]      0.30      0.01      0.30      0.29      0.32    441.16      1.00\n beta[10,6]      0.10      0.01      0.10      0.09      0.12    413.37      1.01\n beta[10,7]      0.34      0.01      0.34      0.33      0.36    529.81      1.02\n beta[10,8]      0.19      0.01      0.19      0.18      0.20    615.98      1.01\n beta[10,9]      0.31      0.01      0.31      0.29      0.32    518.03      1.02\nbeta[10,10]      0.56      0.01      0.56      0.55      0.57    549.82      1.00\nbeta[10,11]      0.11      0.01      0.11      0.10      0.13    526.15      1.00\nbeta[10,12]      0.19      0.01      0.19      0.17      0.20    506.01      1.00\nbeta[10,13]      0.09      0.00      0.09      0.08      0.09    626.06      1.00\n beta[11,0]      0.06      0.01      0.06      0.05      0.07    656.89      1.00\n beta[11,1]     -0.00      0.01     -0.00     -0.02      0.01    501.48      1.01\n beta[11,2]      0.07      0.01      0.07      0.05      0.08    434.22      1.01\n beta[11,3]     -0.07      0.01     -0.07     -0.08     -0.06    455.18      1.01\n beta[11,4]      0.32      0.01      0.32      0.31      0.34    609.90      1.00\n beta[11,5]      0.30      0.01      0.30      0.29      0.31    742.15      1.00\n beta[11,6]      0.10      0.01      0.10      0.09      0.12    811.31      1.00\n beta[11,7]      0.34      0.01      0.34      0.33      0.36    572.83      1.00\n beta[11,8]      0.19      0.01      0.19      0.17      0.20    642.85      1.00\n beta[11,9]      0.31      0.01      0.31      0.30      0.32    541.95      1.00\nbeta[11,10]      0.56      0.01      0.56      0.55      0.58    488.09      1.00\nbeta[11,11]      0.11      0.01      0.11      0.10      0.13    400.39      1.00\nbeta[11,12]      0.19      0.01      0.19      0.17      0.20    502.89      1.00\nbeta[11,13]      0.09      0.00      0.09      0.08      0.09    770.16      1.00\n beta[12,0]      0.06      0.00      0.06      0.05      0.06    835.10      1.00\n beta[12,1]     -0.00      0.01     -0.00     -0.02      0.01    525.41      1.00\n beta[12,2]      0.07      0.01      0.07      0.05      0.09    461.99      1.00\n beta[12,3]     -0.07      0.01     -0.07     -0.08     -0.05    402.14      1.00\n beta[12,4]      0.32      0.01      0.32      0.31      0.33    536.10      1.00\n beta[12,5]      0.30      0.01      0.30      0.29      0.32    508.00      1.00\n beta[12,6]      0.10      0.01      0.10      0.09      0.12    432.68      1.00\n beta[12,7]      0.34      0.01      0.34      0.33      0.36    456.73      1.00\n beta[12,8]      0.19      0.01      0.19      0.18      0.20    454.93      1.00\n beta[12,9]      0.31      0.01      0.31      0.29      0.32    429.57      1.00\nbeta[12,10]      0.56      0.01      0.56      0.55      0.58    428.65      1.00\nbeta[12,11]      0.11      0.01      0.11      0.09      0.13    467.15      1.00\nbeta[12,12]      0.19      0.01      0.19      0.17      0.20    514.43      1.00\nbeta[12,13]      0.09      0.01      0.09      0.08      0.09    648.53      1.00\n beta[13,0]      0.06      0.00      0.06      0.05      0.06    693.73      1.00\n beta[13,1]     -0.00      0.01     -0.00     -0.02      0.01    571.39      1.00\n beta[13,2]      0.07      0.01      0.07      0.05      0.08    705.45      1.00\n beta[13,3]     -0.07      0.01     -0.07     -0.08     -0.05    672.76      1.00\n beta[13,4]      0.32      0.01      0.32      0.31      0.34    675.63      1.00\n beta[13,5]      0.30      0.01      0.30      0.29      0.32    508.98      1.00\n beta[13,6]      0.10      0.01      0.10      0.09      0.12    562.72      1.00\n beta[13,7]      0.34      0.01      0.34      0.33      0.36    554.37      1.00\n beta[13,8]      0.19      0.01      0.19      0.18      0.20    516.77      1.00\n beta[13,9]      0.31      0.01      0.31      0.29      0.32    493.20      1.00\nbeta[13,10]      0.56      0.01      0.56      0.55      0.58    494.85      1.00\nbeta[13,11]      0.11      0.01      0.11      0.09      0.13    510.48      1.00\nbeta[13,12]      0.19      0.01      0.19      0.17      0.20    676.12      1.00\nbeta[13,13]      0.09      0.00      0.09      0.08      0.10    843.17      1.00\n      sigma      0.13      0.00      0.13      0.13      0.13    562.53      1.00\n        tau      0.21      0.01      0.21      0.19      0.23    968.57      1.00\n\nNumber of divergences: 0\n\n\n\nsplines_grw_samples = splines_grw_mcmc.get_samples(1000)\nsplines_grw_post_pred = Predictive(\n    splines_grw,\n    splines_grw_samples\n)(B, None)\n\n\n# mean -&gt; mean b/c I take mean of GRW first, then mean of\n# 1000 posterior samples\nsns.lineplot(\n    x=hour['hr'], y=splines_grw_post_pred['y'].mean(axis=1).mean(axis=0), \n    color='blue', lw=2, label='splines_grw mean function'\n    )\nsns.scatterplot(\n    x=hour['hr'], y=hour['cnt_std'].values, \n    color='lightgrey', alpha=0.5, edgecolor='grey'\n    )\nplt.title('Gaussian Random Walk Prior');"
  },
  {
    "objectID": "posts/2022-07-23-bmcp-ch-5.html#modeling-textco_2-uptake-with-splines",
    "href": "posts/2022-07-23-bmcp-ch-5.html#modeling-textco_2-uptake-with-splines",
    "title": "Bayesian Modeling and Computation in Pyro - Chapter 5",
    "section": "Modeling \\(\\text{CO}_2\\) Uptake with Splines",
    "text": "Modeling \\(\\text{CO}_2\\) Uptake with Splines\n\nplants_CO2 = pd.read_csv(\"./data/CO2_uptake.csv\")\nplant_names = plants_CO2.Plant.unique()\nCO2_conc = plants_CO2.conc.values[:7]\nCO2_concs = plants_CO2.conc.values\nuptake = plants_CO2.uptake.values\nindex = range(12)\ngroups = len(index)\n\n\nnum_knots = 2\nknot_list = np.linspace(CO2_conc[0], CO2_conc[-1], num_knots+2)[1:-1]\n\nBg = dmatrix(\n    \"bs(conc, knots=knots, degree=3, include_intercept=True) - 1\",\n    {\"conc\": CO2_concs, \"knots\": knot_list},\n)\n\nBg = torch.tensor(np.asarray(Bg)).float()\nuptake = torch.tensor(uptake).float()\n\n\nPooled Model - MCMC\n\ndef single_response(design_matrix, obs=None):\n\n    N, P = design_matrix.shape\n    \n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n\n    with pyro.plate('coef', P):\n        beta = pyro.sample('beta', dist.Normal(0., tau))\n\n    ug = pyro.deterministic('ug', torch.matmul(beta, design_matrix.T))\n\n    with pyro.plate('obs', N):\n        up = pyro.sample('uptake', dist.Normal(ug, sigma), obs=obs)\n\n\npyro.render_model(\n    single_response,\n    (Bg, uptake), \n    render_distributions=True\n)\n\n\n\n\n\n\n\n\n\nsr_mcmc = MCMC(NUTS(single_response), 500, 300)\nsr_mcmc.run(Bg, uptake)\n\nSample: 100%|██████████| 800/800 [00:38, 20.99it/s, step size=1.98e-01, acc. prob=0.934]\n\n\n\nsr_mcmc.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n   beta[0]     12.21      1.82     12.27      9.20     15.06    261.26      1.00\n   beta[1]     30.06      3.52     30.11     24.50     35.95    251.13      1.00\n   beta[2]     30.06      6.05     29.81     20.25     39.69    193.86      1.00\n   beta[3]     33.86      9.16     33.72     20.00     49.44    171.13      1.00\n   beta[4]     25.27     22.92     24.96    -14.30     60.07    186.48      1.01\n   beta[5]     33.41      2.03     33.29     30.06     36.65    395.37      1.00\n     sigma      6.77      0.38      6.75      6.24      7.48    596.67      1.00\n       tau     31.24     10.56     28.94     17.24     47.38    226.15      1.00\n\nNumber of divergences: 0\n\n\n\nsr_samples = sr_mcmc.get_samples(1000)\nsr_post_pred = Predictive(\n    single_response, \n    sr_samples\n)(Bg, None)\n\n\nfig, axes = plt.subplots(4, 3, figsize=(10, 6), sharey=True, sharex=True)\n\nfor count, (idx, ax) in enumerate(zip(range(0, 84, 7), axes.ravel())):\n    ax.plot(CO2_conc, uptake[idx:idx+7], '.', lw=1, color='black')\n    ax.plot(CO2_conc, sr_post_pred['uptake'].mean(axis=0)[idx:idx+7], \"k\", alpha=0.5);\n    az.plot_hdi(CO2_conc, sr_post_pred['uptake'][:,idx:idx+7], color=\"C2\", smooth=False, ax=ax)\n    ax.set_title(plant_names[count])\n\nplt.tight_layout()\nfig.text(0.4, -0.05, \"CO2 concentration\", size=18)\nfig.text(-0.03, 0.4, \"CO2 uptake\", size=18, rotation=90);\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:156: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)\n\n\n\n\n\n\n\n\n\n\nnum_knots = 2\nknot_list = np.linspace(CO2_conc[0], CO2_conc[-1], num_knots+2)[1:-1]\n\nBi = dmatrix(\n    \"bs(conc, knots=knots, degree=3, include_intercept=True) - 1\",\n    {\"conc\": CO2_conc, \"knots\": knot_list},\n)\n\nBi = torch.tensor(np.asarray(Bi)).float()\n\n\n\nMixed Effects Model - MCMC\n\ndef individual_response(design_matrix, groups, obs=None):\n\n    N, P = design_matrix.size()\n    \n    tau = pyro.sample('tau', dist.HalfCauchy(1.))\n    sigma = pyro.sample('sigma', dist.HalfNormal(1.))\n    beta = pyro.sample('beta', dist.Normal(0., tau).expand([P, groups]))\n    ug = pyro.deterministic('ug', torch.matmul(design_matrix, beta))\n    ug = ug[:, index].T.ravel()\n\n    with pyro.plate('obs', ug.size(0)):\n        up = pyro.sample('uptake', dist.Normal(ug, sigma), obs=obs)\n\n\nir_mcmc = MCMC(NUTS(individual_response), 500, 300)\nir_mcmc.run(Bi, groups, uptake)\n\nSample: 100%|██████████| 800/800 [01:03, 12.67it/s, step size=1.36e-01, acc. prob=0.895]\n\n\n\nir_mcmc.summary()\n\n\n                mean       std    median      5.0%     95.0%     n_eff     r_hat\n beta[0,0]     15.89      2.02     15.80     12.85     19.17    749.09      1.00\n beta[0,1]     13.24      1.97     13.24     10.27     16.83    818.95      1.00\n beta[0,2]     16.11      2.09     16.17     13.20     20.12    861.96      1.00\n beta[0,3]     14.01      2.04     13.96     10.65     17.05   1070.16      1.00\n beta[0,4]      9.39      2.04      9.29      6.12     12.53    646.55      1.00\n beta[0,5]     13.72      2.06     13.76     10.31     17.12    797.97      1.00\n beta[0,6]     10.36      1.84     10.24      7.20     13.16    733.61      1.00\n beta[0,7]     11.61      1.96     11.58      8.36     14.59    499.54      1.00\n beta[0,8]     11.02      1.85     10.98      7.65     13.60    414.56      1.00\n beta[0,9]     10.31      1.97     10.33      6.76     13.22    907.83      1.01\nbeta[0,10]      7.64      2.00      7.59      4.22     10.82    602.67      1.00\nbeta[0,11]     10.94      2.01     11.01      7.74     14.01    803.46      1.00\n beta[1,0]     41.13      3.72     40.87     35.36     47.45    294.46      1.00\n beta[1,1]     37.47      4.01     37.73     30.86     43.60    274.05      1.00\n beta[1,2]     44.81      3.81     44.62     39.36     51.19    373.35      1.00\n beta[1,3]     31.45      3.68     31.47     26.03     38.26    499.04      1.00\n beta[1,4]     39.25      3.74     39.25     33.99     45.84    327.53      1.00\n beta[1,5]     33.63      3.66     33.62     27.48     39.48    535.10      1.00\n beta[1,6]     25.75      3.68     25.54     20.20     31.82    485.46      1.00\n beta[1,7]     30.41      3.55     30.30     25.06     36.66    319.34      1.00\n beta[1,8]     25.89      3.73     25.82     19.97     31.73    307.24      1.00\n beta[1,9]     19.24      3.86     19.20     12.78     24.82    514.52      1.01\nbeta[1,10]     14.15      3.80     14.05      8.83     20.76    286.51      1.00\nbeta[1,11]     22.33      3.90     22.39     16.56     28.69    250.93      1.00\n beta[2,0]     30.70      5.89     30.87     21.00     39.83    218.51      1.00\n beta[2,1]     43.32      6.25     43.17     31.48     52.51    365.20      1.00\n beta[2,2]     38.19      6.03     38.38     28.38     47.50    315.04      1.00\n beta[2,3]     34.33      6.01     33.96     25.13     44.26    376.64      1.00\n beta[2,4]     36.97      5.78     36.70     27.90     46.70    287.45      1.00\n beta[2,5]     36.23      5.68     36.39     26.48     45.10    335.15      1.00\n beta[2,6]     31.01      5.85     31.13     20.69     39.98    387.93      1.00\n beta[2,7]     33.15      5.64     33.24     24.02     41.86    238.67      1.00\n beta[2,8]     28.14      5.80     28.03     18.82     37.71    262.62      1.01\n beta[2,9]     16.99      6.29     16.97      5.62     26.27    375.90      1.00\nbeta[2,10]     10.97      6.47     10.75      0.52     20.80    246.43      1.00\nbeta[2,11]     13.48      6.39     13.07      3.82     23.94    166.02      1.00\n beta[3,0]     43.46      9.05     43.38     30.07     58.98    160.78      1.01\n beta[3,1]     40.72      9.80     40.54     23.31     55.08    375.00      1.00\n beta[3,2]     51.19      9.04     51.03     36.54     65.81    279.54      1.00\n beta[3,3]     33.55      9.70     33.58     16.81     48.18    380.20      1.00\n beta[3,4]     42.53      8.86     42.41     29.50     57.63    276.04      1.00\n beta[3,5]     44.49      8.99     44.56     29.90     59.10    320.28      1.00\n beta[3,6]     34.04      9.10     33.86     18.79     48.64    388.05      1.00\n beta[3,7]     32.81      9.16     32.71     18.91     47.81    245.83      1.00\n beta[3,8]     31.09      9.19     31.39     13.62     43.80    248.20      1.00\n beta[3,9]     24.42      9.38     24.31     10.53     40.01    389.07      1.00\nbeta[3,10]     15.10      9.82     15.47     -1.38     30.21    228.23      1.00\nbeta[3,11]     23.38      9.72     23.52      8.32     39.32    160.46      1.00\n beta[4,0]     32.31     22.64     31.24     -3.10     70.85    241.13      1.00\n beta[4,1]     37.32     24.16     38.20      3.44     80.53    407.01      1.00\n beta[4,2]     24.10     22.41     23.79    -11.63     62.44    347.81      1.00\n beta[4,3]     37.11     24.25     38.23     -3.03     70.66    349.09      1.00\n beta[4,4]     20.83     22.29     21.67    -13.15     58.67    329.19      1.00\n beta[4,5]     25.82     23.11     26.21     -9.84     68.61    371.96      1.00\n beta[4,6]     26.63     23.04     25.68    -16.67     60.41    422.46      1.00\n beta[4,7]     21.17     23.67     22.25    -16.18     60.12    273.25      1.00\n beta[4,8]     17.28     23.27     17.68    -21.30     52.39    324.63      1.00\n beta[4,9]     18.38     23.02     18.19    -13.46     55.14    419.31      1.00\nbeta[4,10]     11.79     23.83     11.44    -25.03     48.55    253.85      1.00\nbeta[4,11]      9.50     23.80      9.69    -26.23     49.25    195.24      1.00\n beta[5,0]     39.49      1.95     39.52     36.04     42.50   1449.89      1.00\n beta[5,1]     44.12      1.93     44.08     40.64     46.90   1125.14      1.00\n beta[5,2]     45.41      2.21     45.38     41.92     48.87    468.95      1.00\n beta[5,3]     38.56      2.18     38.59     34.91     42.08    661.23      1.00\n beta[5,4]     42.21      2.11     42.32     38.65     45.23    615.67      1.00\n beta[5,5]     41.19      1.98     41.11     38.21     44.66    756.89      1.00\n beta[5,6]     35.32      2.11     35.43     31.69     38.64   1030.06      1.00\n beta[5,7]     31.34      2.05     31.39     27.53     34.23    928.45      1.00\n beta[5,8]     27.55      1.94     27.57     24.71     31.08   1275.81      1.00\n beta[5,9]     21.57      1.89     21.64     18.33     24.45    982.11      1.00\nbeta[5,10]     14.30      2.08     14.28     10.81     17.45    958.17      1.00\nbeta[5,11]     19.76      1.96     19.83     16.75     23.04    805.23      1.00\n     sigma      2.04      0.29      2.02      1.56      2.48     86.44      1.00\n       tau     31.47      2.90     31.15     26.90     36.18    586.94      1.00\n\nNumber of divergences: 0\n\n\n\nir_samples = ir_mcmc.get_samples(1000)\nir_post_pred = Predictive(\n    individual_response, \n    ir_samples\n)(Bi, groups, None)\n\n\nfig, axes = plt.subplots(4, 3, figsize=(10, 6), sharey=True, sharex=True)\n\nfor count, (idx, ax) in enumerate(zip(range(0, 84, 7), axes.ravel())):\n    ax.plot(CO2_conc, uptake[idx:idx+7], '.', lw=1, color='black')\n    ax.plot(CO2_conc, ir_post_pred['uptake'].mean(axis=0)[idx:idx+7], \"k\", alpha=0.5);\n    az.plot_hdi(CO2_conc, ir_post_pred['uptake'][:,idx:idx+7], color=\"C2\", smooth=False, ax=ax)\n    ax.set_title(plant_names[count])\n\nplt.tight_layout()\nfig.text(0.4, -0.05, \"CO2 concentration\", size=18)\nfig.text(-0.03, 0.4, \"CO2 uptake\", size=18, rotation=90);\n\n/Users/gabestechschulte/miniforge3/envs/probs/lib/python3.10/site-packages/arviz/plots/hdiplot.py:156: FutureWarning: hdi currently interprets 2d data as (draw, shape) but this will change in a future release to (chain, draw) for coherence with other functions\n  hdi_data = hdi(y, hdi_prob=hdi_prob, circular=circular, multimodal=False, **hdi_kwargs)"
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html",
    "title": "No Code, Dependency, and Building Technology",
    "section": "",
    "text": "‘Programmers’, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his blog on the matter, “A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect”.\nEver since the rise of machine learning and data science, the industry was bound to develop no and or low code products for everything between data cleaning, processing, and annotation, to the implementation of models. This is an example of one of the highest forms of abstraction - you don’t even need to be able to code. Knowledge of the problem at hand and some simple intuition into which model may provide a low validation error is almost all you need. A lower level form of abstraction is through the use of application programming interfaces (APIs) such as scikit-learn, PyTorch, etc. Using these APIs requires more technical skillsets, knowledge of first principles, and a deeper understanding of the problem than the no-code substitutes stated above. Lastly, we have the ‘bare bones’ implementation of algorithms - what most people usually call, “programming things from scratch”. But even then, good luck writing your favorite model without using numpy, scipy, or JAX.\nAs teams of couragous developers and organizations seek to create products to help make everyday routines a commodity and more productive, it can be harder to learn technical methods from first principles as they have been abstracted away. There’s no need, nor the time, to start and go writing everything from scratch, but having a solid intuition into what is going on under the hood can allow you to uniquely solve complex problems, debug more efficiently, contribute to open source software, etc."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#modernity-and-abstraction",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#modernity-and-abstraction",
    "title": "No Code, Dependency, and Building Technology",
    "section": "",
    "text": "‘Programmers’, loosely speaking, in some form or another have always been developing software to automate tedious and repetitive tasks. Rightly so, as this is one of the tasks computers are designed to perform. As science and technology progresses, and gets more technological, there is a growing seperation between the maker and the user. This is one of the negative externalities of modernism - we enjoy the benefits of a more advanced and technologically adept society, but fewer and fewer people understand the inner workings. Andrej Karpathy has a jokingly short paragraph in his blog on the matter, “A courageous developer has taken the burden of understanding query strings, urls, GET/POST requests, HTTP connections, and so on from you and largely hidden the complexity behind a few lines of code. This is what we are now familiar with and expect”.\nEver since the rise of machine learning and data science, the industry was bound to develop no and or low code products for everything between data cleaning, processing, and annotation, to the implementation of models. This is an example of one of the highest forms of abstraction - you don’t even need to be able to code. Knowledge of the problem at hand and some simple intuition into which model may provide a low validation error is almost all you need. A lower level form of abstraction is through the use of application programming interfaces (APIs) such as scikit-learn, PyTorch, etc. Using these APIs requires more technical skillsets, knowledge of first principles, and a deeper understanding of the problem than the no-code substitutes stated above. Lastly, we have the ‘bare bones’ implementation of algorithms - what most people usually call, “programming things from scratch”. But even then, good luck writing your favorite model without using numpy, scipy, or JAX.\nAs teams of couragous developers and organizations seek to create products to help make everyday routines a commodity and more productive, it can be harder to learn technical methods from first principles as they have been abstracted away. There’s no need, nor the time, to start and go writing everything from scratch, but having a solid intuition into what is going on under the hood can allow you to uniquely solve complex problems, debug more efficiently, contribute to open source software, etc."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#switching-costs-dependency-and-open-source-goods",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#switching-costs-dependency-and-open-source-goods",
    "title": "No Code, Dependency, and Building Technology",
    "section": "Switching Costs, Dependency and Open Source Goods",
    "text": "Switching Costs, Dependency and Open Source Goods\nAt least the two lower levels of abstractions usually rely on open-source software, whereas the no-code alternative is typically (at the moment) proprietary technology in the form of machine learning as a service and or platform. [Edit 14.09.2021: H20.ai, HuggingFace, MakeML, CreateML, Google Cloud AutoML are all open-source services or platforms in the low or no code ML space] Open-source gives you the biggest flexibility that, if the space again changes, you can move things. Otherwise, you find yourself locked into a technology stack the way you were locked in to technologies from the ’80s and ’90s and 2000s.\nBeing locked into IT components can have serious opportunity costs. For example, switching from Mac to a Windows based PC involves not only the hardware costs of the computer itself, but also involves purchasing of a whole new library of software, and even more importantly, learning how to use a brand new system. When you, or an organization, decides to go the propriety route, these switching costs can be very high, and users may find themselves experiencing lock-in; a situation where the cost of changing to a different system is so high that switching is virtually inconcievable.\nOf course, the producers of this hardware / sofware love the fact that you have an inelastic demand curve - a rise in prices won’t affect demand much as switching costs are high. In summary, as machine learning platforms didn’t really exist ~15 years ago, they will more than likely change quite a bit and you are dependent on the producer of continually adapting to industry trends and technological advancements while at the same time, staying flexible to cater to your edge cases when you need it."
  },
  {
    "objectID": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#leverage-and-building-technology",
    "href": "posts/2021-08-10-No-Code-Dependency-and-Building-Technology.html#leverage-and-building-technology",
    "title": "No Code, Dependency, and Building Technology",
    "section": "Leverage and Building Technology",
    "text": "Leverage and Building Technology\nIt would be a full time job to stay present and up to date on everything being released in the space of “machine learning”, but also to be knowledgeable of first princples and have skillsets to use the technologies is in another class of its own. Before AWS, APIs, open source, etc., as an organization or startup, it was likely the question, “we need amazing technical people who can build it all”. Now, with the increase and rise of PaaS, SaaS, open source librarys and tooling, the question shifts from a question of “building”, to a question of “leveraging existing tools”. How long before no / low code gets good enough before we are asking, “why did we not build this with no-code tools?”.\nThe highest form of leverage for a company is to develop and build out difficult and new technology. No-code, if developed right (and is ideally open-source), can still provide leverage and value-add, but any advantage just becomes table stakes. This is what will distinguish great teams vs. good teams; non-linear returns will continue to be through building out proprietery and difficult technology."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-update-community-bonding.html",
    "href": "posts/2023-06-10-gsoc-update-community-bonding.html",
    "title": "Google Summer of Code - Community Bonding and Modularization",
    "section": "",
    "text": "I have been selected as a contributor under the organization NumFOCUS for the Google Summer of Code 2023 program. I will be working on developing better tools to interpret complex Bambi regression models. More details about the project can be found here. This blog post will be a summary of the community bonding period and the progress made over the first two weeks of the program.\nTimeline of events:\n\nCommunity Bonding Period (May 4th - 28th)\nWeek 1 (May 29th - June 4th)\nWeek 2 (June 5th - June 11th)\n\n\n\nThe community bonding period is a time for students to get to know their mentors and organizations, and to change / finalize project details. I have been in contact with my mentors Tomás Capretto and Osvaldo Martin via Slack and GitHub. A goal of mine throughout the community bonding period was not only to get to know my mentors, but also to familiarize myself with the Bambi codebase before coding officially began. To achieve this, I read through the codebase attempting to understand how the different modules interact with one another. Additionally, I read through the Bambi documentation to understand how the different models are implemented, and any additional functionality provided the library. After familarizing myself with the codebase at a high level, I decided to get a jumpstart on the project deliverables for week 1 through 4. Below is a table of the deliverables for the entire project timeline.\n\n\n\n\n\n\n\nWeek\nDeliverable\n\n\n\n\n1\nReview plot_cap() design and relevant Bambi codebase. Discuss grammar of graphics libraries with mentors. Identify any open GitHub issues regarding plot_cap() that would restrict development.\n\n\n2\nImplement predictions at the observational level for plot_cap(), and add tests, and documentation.\n\n\n3\nDesign of how plot_comparisons() and plot_slopes() can re-utilize the plot_cap() code and be incorporated into the Bambi codebase.\n\n\n4\nBegin plot_comparisons() implementation\n\n\n5\nDeliver working example of plot_comparisons() for a simple Bambi model, and open draft PR.\n\n\n6\nContinue plot_comparisons() implementation. Deliver example on a complex model.\n\n\n7\nWrite tests, examples, and documentation for plot_comparisons(), and open PR.\n\n\n8\nBegin plot_slopes() implementation.\n\n\n9\nDeliver working example of plot_slopes() for a simple Bambi model, and open draft PR.\n\n\n10\nContinue plot_slopes() implementation. Deliver example on a complex model.\n\n\n11\nWrite tests, examples, and documentation for plot_slopes(), and open PR.\n\n\n12\nReview plot_cap(), plot_comparisons(), and plot_slopes() code. Ensure documentation and examples are correct, and tests pass.\n\n\n13\nFinal submission. This week is a buffer period for code review, unexpected difficulties, and ensuring documentation and added examples are accurate and comprehensive."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-update-community-bonding.html#community-bonding-period",
    "href": "posts/2023-06-10-gsoc-update-community-bonding.html#community-bonding-period",
    "title": "Google Summer of Code - Community Bonding and Modularization",
    "section": "",
    "text": "The community bonding period is a time for students to get to know their mentors and organizations, and to change / finalize project details. I have been in contact with my mentors Tomás Capretto and Osvaldo Martin via Slack and GitHub. A goal of mine throughout the community bonding period was not only to get to know my mentors, but also to familiarize myself with the Bambi codebase before coding officially began. To achieve this, I read through the codebase attempting to understand how the different modules interact with one another. Additionally, I read through the Bambi documentation to understand how the different models are implemented, and any additional functionality provided the library. After familarizing myself with the codebase at a high level, I decided to get a jumpstart on the project deliverables for week 1 through 4. Below is a table of the deliverables for the entire project timeline.\n\n\n\n\n\n\n\nWeek\nDeliverable\n\n\n\n\n1\nReview plot_cap() design and relevant Bambi codebase. Discuss grammar of graphics libraries with mentors. Identify any open GitHub issues regarding plot_cap() that would restrict development.\n\n\n2\nImplement predictions at the observational level for plot_cap(), and add tests, and documentation.\n\n\n3\nDesign of how plot_comparisons() and plot_slopes() can re-utilize the plot_cap() code and be incorporated into the Bambi codebase.\n\n\n4\nBegin plot_comparisons() implementation\n\n\n5\nDeliver working example of plot_comparisons() for a simple Bambi model, and open draft PR.\n\n\n6\nContinue plot_comparisons() implementation. Deliver example on a complex model.\n\n\n7\nWrite tests, examples, and documentation for plot_comparisons(), and open PR.\n\n\n8\nBegin plot_slopes() implementation.\n\n\n9\nDeliver working example of plot_slopes() for a simple Bambi model, and open draft PR.\n\n\n10\nContinue plot_slopes() implementation. Deliver example on a complex model.\n\n\n11\nWrite tests, examples, and documentation for plot_slopes(), and open PR.\n\n\n12\nReview plot_cap(), plot_comparisons(), and plot_slopes() code. Ensure documentation and examples are correct, and tests pass.\n\n\n13\nFinal submission. This week is a buffer period for code review, unexpected difficulties, and ensuring documentation and added examples are accurate and comprehensive."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-update-community-bonding.html#plot_cap-using-posterior-predictive-samples",
    "href": "posts/2023-06-10-gsoc-update-community-bonding.html#plot_cap-using-posterior-predictive-samples",
    "title": "Google Summer of Code - Community Bonding and Modularization",
    "section": "plot_cap using posterior predictive samples",
    "text": "plot_cap using posterior predictive samples\nPredictions at the observation level for plot_cap were implemented in PR 668. By default, plot_cap uses the posterior distribution to visualize some mean outcome parameter of a a GLM. However, the posterior predictive distribution can also be plotted by specifying pps=True where pps stands for posterior predictive samples of the response variable. Upon completion of the PR above, an example showing the functionality of plot_cap was added to the Bambi documentation in PR 670."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-update-community-bonding.html#modularization-of-the-plots-sub-package",
    "href": "posts/2023-06-10-gsoc-update-community-bonding.html#modularization-of-the-plots-sub-package",
    "title": "Google Summer of Code - Community Bonding and Modularization",
    "section": "Modularization of the plots sub-package",
    "text": "Modularization of the plots sub-package\nCompleting the week one and two deliveribles early on in the program allowed me to spend more time on the design of the plots sub-package and implementation of plot_comparisons. For the design of the plots sub-package, I decided to create five different modules, each with a specific purpose. The modules are as follows:\n\ncreate_data.py - creates the data called by the functions in the effects.py and comparisons.py modules.\neffects.py - contains the functions (predictions, comparisons, slopes) that are used to create the dataframe that is passed to the plotting functions and or user when called standalone.\nutils.py - contains commonly used functions that are called by multiple modules in the plots sub-package.\nplotting.py - contains the plotting functions (plot_predictions, plot_comparisons, plot_slopes) that are used to generate the plots for the user.\nplot_types.py - determines the plot types (numeric or categorical) for the the plotting module.\n\nThe modularization of the plots sub-package will allow for easier maintenance and testing of the codebase and offers re-usability of code using OOP principles. The modularization of code happened during the initial development stages of the plot_comparisons function."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-update-community-bonding.html#comparisons-plot_comparisons",
    "href": "posts/2023-06-10-gsoc-update-community-bonding.html#comparisons-plot_comparisons",
    "title": "Google Summer of Code - Community Bonding and Modularization",
    "section": "Comparisons: plot_comparisons",
    "text": "Comparisons: plot_comparisons\nPredictive comparisons refers to the predictions made by a model for different covariate values while holding one or more variable constant. For example, we may be interested in the following quantity, “how does the probability of survival (outcome) change if a passenger travels in 1st class vs. 3rd class? given a certain age and sex?” Thus, the objective of the comparisons plotting functionality is to plot comparisons on the y-axis against values of one or more covariates on the x-axis\nCurrently, plot_comparisons only works when a user explicitly passes the contrast covariate and its value, and the conditional covariate and values they would like to condition on.\n\n\nCode\nimport numpy as np\nimport pandas as pd\n\nimport bambi as bmb\nfrom bambi.plots import plot_comparison, comparisons\n\n\n\n\nCode\ndat = pd.read_csv(\n    \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\", \n    index_col=0\n)\n\ndat[\"PClass\"] = dat[\"PClass\"].str.replace(\"[st, nd, rd]\", \"\", regex=True)\ndat[\"PClass\"] = dat[\"PClass\"].str.replace(\"*\", \"0\").astype(int)\ndat[\"PClass\"] = dat[\"PClass\"].replace(0, np.nan)\ndat[\"PClass\"] = pd.Categorical(dat[\"PClass\"], ordered=True)\ndat[\"SexCode\"] = pd.Categorical(dat[\"SexCode\"], ordered=True)\ndat = dat.dropna(axis=0, how=\"any\")\n\n\n\ntitanic_model = bmb.Model(\n    \"Survived ~ PClass * SexCode * Age\", \n    data=dat, \n    family=\"bernoulli\"\n)\ntitanic_idata = titanic_model.fit(\n    draws=1000, \n    target_accept=0.95, \n    random_seed=1234\n)\n\nModeling the probability that Survived==1\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [Intercept, PClass, SexCode, PClass:SexCode, Age, PClass:Age, SexCode:Age, PClass:SexCode:Age]\n\n\n\n\n\n\n\n    \n      \n      100.00% [8000/8000 00:15&lt;00:00 Sampling 4 chains, 0 divergences]\n    \n    \n\n\nSampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 16 seconds.\n\n\n\n# user can also pass a dict into conditional to define the \n# values used for the conditional variables\nfig, ax = plot_comparison(\n    model=titanic_model,\n    idata=titanic_idata,\n    contrast={\"PClass\": [1, 3]},\n    conditional={\"Age\": [50], \"SexCode\": [0, 1]}\n)\nfig.set_size_inches(7, 3)\n\n\n\n\n\n\n\n\nComparing an individual of Age= \\(50\\) and SexCode = \\(0\\), moving PClass from \\(1\\) to \\(3\\) decreases the probability of survival by about \\(0.2\\). For SexCode = \\(1\\), the probability of survival decrease by nearly \\(0.5\\)."
  },
  {
    "objectID": "posts/2023-06-10-gsoc-update-community-bonding.html#next-steps",
    "href": "posts/2023-06-10-gsoc-update-community-bonding.html#next-steps",
    "title": "Google Summer of Code - Community Bonding and Modularization",
    "section": "Next Steps",
    "text": "Next Steps\nThe next weeks will involve adding:\n\ndefault contrast and conditional values so the user does not have to explicitly pass them.\nunit-level comparisons which uses the observed data instead of creating a hypothetical dataset to compute comparison estimates.\nan average by argument to allow the user to marginalize over a subset of covariates."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Posts",
    "section": "",
    "text": "Stateful Joins in SQL\n\n\n\n\n\n\ndatabase-systems\n\n\n\n\n\n\n\n\n\nAug 22, 2024\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nData formats and encoding\n\n\n\n\n\n\ndatabase-systems\n\n\ncmu-db-notes\n\n\n\n\n\n\n\n\n\nAug 18, 2024\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Alternative Samplers in Bambi\n\n\n\n\n\n\nprobabilistic-programming\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nMar 29, 2024\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Interpret Usage in Bambi\n\n\n\n\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nDec 9, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nOutcome Constraints in Bayesian Optimization\n\n\n\n\n\n\nprobabilistic-programming\n\n\nbayesian-statistics\n\n\noptimization\n\n\n\n\n\n\n\n\n\nNov 28, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nSurvival Models in Bambi\n\n\n\n\n\n\nprobabilistic-programming\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nPredict New Groups with Hierarchical Models in Bambi\n\n\n\n\n\n\nprobabilistic-programming\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nOct 10, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nOrdinal Models in Bambi\n\n\n\n\n\n\nprobabilistic-programming\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nZero Inflated Models in Bambi\n\n\n\n\n\n\nprobabilistic-programming\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Summer of Code - Final Report\n\n\n\n\n\n\nopen-source\n\n\n\n\n\n\n\n\n\nAug 18, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Summer of Code - Average Predictive Slopes\n\n\n\n\n\n\nopen-source\n\n\npython\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nAug 1, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Summer of Code - Average Predictive Comparisons\n\n\n\n\n\n\nopen-source\n\n\npython\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nJun 30, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nGoogle Summer of Code - Community Bonding and Modularization\n\n\n\n\n\n\nopen-source\n\n\npython\n\n\nbayesian-statistics\n\n\n\n\n\n\n\n\n\nJun 10, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nTranslating a Model into a Log Joint Probability\n\n\n\n\n\n\nprobability\n\n\ninference\n\n\njax\n\n\n\n\n\n\n\n\n\nMar 26, 2023\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nInference - Hamiltonian Monte Carlo from Scratch\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\noptimization\n\n\npytorch\n\n\n\n\n\n\n\n\n\nOct 14, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nInference - Gibbs Sampling from Scratch\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\npytorch\n\n\n\n\n\n\n\n\n\nOct 12, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nInference - Metropolis Hastings from Scratch\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\npytorch\n\n\n\n\n\n\n\n\n\nOct 8, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nInference - Monte Carlo Approximation\n\n\n\n\n\n\nprobability\n\n\nsampling\n\n\ninference\n\n\npytorch\n\n\n\n\n\n\n\n\n\nOct 7, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nA Fragment of the Sphinx\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\nJul 25, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Modeling and Computation in Pyro - Chapter 5\n\n\n\n\n\n\nbayesian-statistics\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 23, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Modeling and Computation in Pyro - Chapter 4\n\n\n\n\n\n\nbayesian-statistics\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 22, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Modeling and Computation in Pyro - Chapter 3\n\n\n\n\n\n\nbayesian-statistics\n\n\npython\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nVariational Inference - ELBO\n\n\n\n\n\n\nprobability\n\n\noptimization\n\n\n\n\n\n\n\n\n\nJun 23, 2022\n\n\nGabriel Stechschulte\n\n\n\n\n\n\n\n\n\n\n\n\nNo Code, Dependency, and Building Technology\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2021\n\n\nGabriel Stechschulte\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-07-09-sql-lateral-join.html",
    "href": "posts/2024-07-09-sql-lateral-join.html",
    "title": "Stateful Joins in SQL",
    "section": "",
    "text": "In some scenarios, one needs to enrich an event stream with data from another source that holds “state”. This state provides additional context to the event stream.\nFor example, in manufacturing, a machine may use a set of machine process parameters (acceleration, force, etc.) when producing an item. The process parameters represent the “state” of the machine at production time \\(t\\). However, the software services that publishes messages on what is being produced and the machine process parameters currently used are separate. Furthermore, to avoid the duplication of data, the service that publishes process parameters only publishes a message when there is a change in state, e.g when an operator changes one of process parameters."
  },
  {
    "objectID": "posts/2024-07-09-sql-lateral-join.html#introduction",
    "href": "posts/2024-07-09-sql-lateral-join.html#introduction",
    "title": "Stateful Joins in SQL",
    "section": "",
    "text": "In some scenarios, one needs to enrich an event stream with data from another source that holds “state”. This state provides additional context to the event stream.\nFor example, in manufacturing, a machine may use a set of machine process parameters (acceleration, force, etc.) when producing an item. The process parameters represent the “state” of the machine at production time \\(t\\). However, the software services that publishes messages on what is being produced and the machine process parameters currently used are separate. Furthermore, to avoid the duplication of data, the service that publishes process parameters only publishes a message when there is a change in state, e.g when an operator changes one of process parameters."
  },
  {
    "objectID": "posts/2024-07-09-sql-lateral-join.html#data-simulation",
    "href": "posts/2024-07-09-sql-lateral-join.html#data-simulation",
    "title": "Stateful Joins in SQL",
    "section": "Data simulation",
    "text": "Data simulation\nLets simulate some data with TimescaleDB.\nCREATE TABLE production (\n    time timestamptz NOT NULL,\n    product_id INT NOT NULL\n);\n\nINSERT INTO production\nSELECT *,\n   1 as product_id\nFROM generate_series('2024-01-01 05:00:00', '2024-01-01 05:05:00', INTERVAL '1m') AS time\nUNION ALL\nSELECT *,\n    2 as product_id\nFROM generate_series('2024-01-01 05:10:00', '2024-01-01 05:13:00', INTERVAL '1m') AS time\n\nSELECT * FROM production;\n\n\n\ntime\nproduct_id\n\n\n\n\n2024-01-01 05:00:00+00\n1\n\n\n2024-01-01 05:01:00+00\n1\n\n\n2024-01-01 05:02:00+00\n1\n\n\n2024-01-01 05:03:00+00\n1\n\n\n2024-01-01 05:04:00+00\n1\n\n\n2024-01-01 05:05:00+00\n1\n\n\n2024-01-01 05:10:00+00\n2\n\n\n2024-01-01 05:11:00+00\n2\n\n\n2024-01-01 05:12:00+00\n2\n\n\n2024-01-01 05:13:00+00\n2\n\n\n\nCREATE TABLE machine (\n    time timestamptz NOT NULL,\n    speed NUMERIC NOT NULL\n);\n\nINSERT INTO machine (time, speed)\nVALUES ('2024-01-01 02:00:00'::timestamptz, 40.0),\n       ('2024-01-01 05:07:00'::timestamptz, 60.0);\n\nSELECT * FROM machine;\n\n\n\ntime\nspeed\n\n\n\n\n2024-01-01 02:00:00+00\n40.0\n\n\n2024-01-01 05:07:00+00\n60.0"
  },
  {
    "objectID": "posts/2024-07-09-sql-lateral-join.html#postgres-stateful-join",
    "href": "posts/2024-07-09-sql-lateral-join.html#postgres-stateful-join",
    "title": "Stateful Joins in SQL",
    "section": "Postgres stateful join",
    "text": "Postgres stateful join\nWe would like to enrich the production data with the process parameters from machine. Thus, we need to join the most recent process parameter with a production event where a production event most occur greater than or equal to the change in machine state.\nThis enrichment can be achieved with a stateful join using PostgreSQL’s LATERAL JOIN expression. The LATERAL keyword allows a subquery or derived table to reference columns from tables listed before it in the FROM clause. A LATERAL join is like a for loop: for each row returned by the tables listed before LATERAL in the FROM clause, PostgreSQL will evaluate the LATERAL subquery using the current row’s values. The resulting rows from the LATERAL subquery are joined to the current row, typically using a JOIN condition of ON TRUE since the real join conditions are inside the LATERAL subquery. This process is then repeated for each row or set of rows from the tables preceding LATERAL.\nSELECT *\nFROM production prod\nLEFT JOIN LATERAL (\n    SELECT time as change_time,\n           speed\n    FROM machine\n    WHERE time &lt;= prod.time\n    ORDER BY time DESC\n    LIMIT 1\n    ) ON TRUE;\n\n\n\n\n\n\n\n\n\ntime\nproduct_id\nchange_time\nspeed\n\n\n\n\n2024-01-01 05:00:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:01:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:02:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:03:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:04:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:05:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:10:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n2024-01-01 05:11:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n2024-01-01 05:12:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n2024-01-01 05:13:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n\nIn our hypothetical manufacturing example, the machine process parameters changed when product 2 began producing. Before this enrichment process, it wouldn’t have been known why the time to produce product 2 was faster. However, the LATERAL JOIN allows us to see that the speed increased from 40 to 60."
  },
  {
    "objectID": "posts/2024-08-22-sql-stateful-joins.html",
    "href": "posts/2024-08-22-sql-stateful-joins.html",
    "title": "Stateful Joins in SQL",
    "section": "",
    "text": "In some scenarios, one needs to enrich an event stream with data from another source that holds “state”. This state provides additional context to the event stream.\nFor example, in manufacturing, a machine may use a set of machine process parameters (acceleration, force, etc.) when producing an item. The process parameters represent the “state” of the machine at production time \\(t\\). However, the software services that publishes messages on what is being produced and the machine process parameters currently used are separate. Furthermore, to avoid the duplication of data, the service that publishes process parameters only publishes a message when there is a change in state, e.g when an operator changes one of process parameters."
  },
  {
    "objectID": "posts/2024-08-22-sql-stateful-joins.html#introduction",
    "href": "posts/2024-08-22-sql-stateful-joins.html#introduction",
    "title": "Stateful Joins in SQL",
    "section": "",
    "text": "In some scenarios, one needs to enrich an event stream with data from another source that holds “state”. This state provides additional context to the event stream.\nFor example, in manufacturing, a machine may use a set of machine process parameters (acceleration, force, etc.) when producing an item. The process parameters represent the “state” of the machine at production time \\(t\\). However, the software services that publishes messages on what is being produced and the machine process parameters currently used are separate. Furthermore, to avoid the duplication of data, the service that publishes process parameters only publishes a message when there is a change in state, e.g when an operator changes one of process parameters."
  },
  {
    "objectID": "posts/2024-08-22-sql-stateful-joins.html#data-simulation",
    "href": "posts/2024-08-22-sql-stateful-joins.html#data-simulation",
    "title": "Stateful Joins in SQL",
    "section": "Data simulation",
    "text": "Data simulation\nLets simulate some data with TimescaleDB.\nCREATE TABLE production (\n    time timestamptz NOT NULL,\n    product_id INT NOT NULL\n);\n\nINSERT INTO production\nSELECT *,\n   1 as product_id\nFROM generate_series('2024-01-01 05:00:00', '2024-01-01 05:05:00', INTERVAL '1m') AS time\nUNION ALL\nSELECT *,\n    2 as product_id\nFROM generate_series('2024-01-01 05:10:00', '2024-01-01 05:13:00', INTERVAL '1m') AS time\n\nSELECT * FROM production;\n\n\n\ntime\nproduct_id\n\n\n\n\n2024-01-01 05:00:00+00\n1\n\n\n2024-01-01 05:01:00+00\n1\n\n\n2024-01-01 05:02:00+00\n1\n\n\n2024-01-01 05:03:00+00\n1\n\n\n2024-01-01 05:04:00+00\n1\n\n\n2024-01-01 05:05:00+00\n1\n\n\n2024-01-01 05:10:00+00\n2\n\n\n2024-01-01 05:11:00+00\n2\n\n\n2024-01-01 05:12:00+00\n2\n\n\n2024-01-01 05:13:00+00\n2\n\n\n\nCREATE TABLE machine (\n    time timestamptz NOT NULL,\n    speed NUMERIC NOT NULL\n);\n\nINSERT INTO machine (time, speed)\nVALUES ('2024-01-01 02:00:00'::timestamptz, 40.0),\n       ('2024-01-01 05:07:00'::timestamptz, 60.0);\n\nSELECT * FROM machine;\n\n\n\ntime\nspeed\n\n\n\n\n2024-01-01 02:00:00+00\n40.0\n\n\n2024-01-01 05:07:00+00\n60.0"
  },
  {
    "objectID": "posts/2024-08-22-sql-stateful-joins.html#postgres-stateful-join",
    "href": "posts/2024-08-22-sql-stateful-joins.html#postgres-stateful-join",
    "title": "Stateful Joins in SQL",
    "section": "Postgres stateful join",
    "text": "Postgres stateful join\nWe would like to enrich the production data with the process parameters from machine. Thus, we need to join the most recent process parameter with a production event where a production event most occur greater than or equal to the change in machine state.\nThis enrichment can be achieved with a stateful join using PostgreSQL’s LATERAL JOIN expression. The LATERAL keyword allows a subquery or derived table to reference columns from tables listed before it in the FROM clause. A LATERAL join is like a for loop: for each row returned by the tables listed before LATERAL in the FROM clause, PostgreSQL will evaluate the LATERAL subquery using the current row’s values. The resulting rows from the LATERAL subquery are joined to the current row, typically using a JOIN condition of ON TRUE since the real join conditions are inside the LATERAL subquery. This process is then repeated for each row or set of rows from the tables preceding LATERAL.\nSELECT *\nFROM production prod\nLEFT JOIN LATERAL (\n    SELECT time as change_time,\n           speed\n    FROM machine\n    WHERE time &lt;= prod.time\n    ORDER BY time DESC\n    LIMIT 1\n    ) ON TRUE;\n\n\n\n\n\n\n\n\n\ntime\nproduct_id\nchange_time\nspeed\n\n\n\n\n2024-01-01 05:00:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:01:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:02:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:03:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:04:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:05:00.000000 +00:00\n1\n2024-01-01 02:00:00.000000 +00:00\n40\n\n\n2024-01-01 05:10:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n2024-01-01 05:11:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n2024-01-01 05:12:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n2024-01-01 05:13:00.000000 +00:00\n2\n2024-01-01 05:07:00.000000 +00:00\n60\n\n\n\nIn our hypothetical manufacturing example, the machine process parameters changed when product 2 began producing. Before this enrichment process, it wouldn’t have been known why the time to produce product 2 was faster. However, the LATERAL JOIN allows us to see that the speed increased from 40 to 60."
  }
]